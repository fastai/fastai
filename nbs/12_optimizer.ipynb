{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|skip\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from fastai.torch_basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "> Define the general fastai optimizer and the variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `_BaseOptimizer` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class _BaseOptimizer():\n",
    "    \"Common functionality between `Optimizer` and `OptimWrapper`\"\n",
    "    def all_params(self,\n",
    "        n:(slice, int)=slice(None), # Extended slicing over the optimizer `param_lists`\n",
    "        with_grad:bool=False # Get all param tuples. If `True` select only those with a gradient\n",
    "    ):\n",
    "        res = L((p,pg,self.state[p],hyper) for pg,hyper in zip(self.param_lists[n],self.hypers[n]) for p in pg)\n",
    "        return L(o for o in res if hasattr(o[0], 'grad') and o[0].grad is not None) if with_grad else res\n",
    "\n",
    "    def _set_require_grad(self,\n",
    "        rg:bool, # Requires grad: if `True` sets gradient for parameters, else uses state `state[\"force_train\"]`\n",
    "        p:Tensor, # Parameters to set gradient\n",
    "        pg, # Param groups (unused but needed because unpack *o)\n",
    "        state: dict,\n",
    "        h # Hyperparameter (unused but needed because unpack *o)\n",
    "    ):\n",
    "        p.requires_grad_(rg or state.get('force_train', False))\n",
    "    def freeze_to(self,\n",
    "        n:int # Freeze up to `n` layers\n",
    "    ):\n",
    "        self.frozen_idx = n if n >= 0 else len(self.param_lists) + n\n",
    "        if self.frozen_idx >= len(self.param_lists):\n",
    "            warn(f\"Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.\")\n",
    "        for o in self.all_params(slice(n, None)): self._set_require_grad(True,  *o)\n",
    "        for o in self.all_params(slice(None, n)): self._set_require_grad(False, *o)\n",
    "\n",
    "    def freeze(self):\n",
    "        assert(len(self.param_lists)>1)\n",
    "        self.freeze_to(-1)\n",
    "\n",
    "    def set_freeze(self,\n",
    "        n:int,\n",
    "        rg:bool, # Whether grad is required\n",
    "        ignore_force_train=False # Overwrites \"force_train\" or batch norm always trains even if frozen\n",
    "    ):\n",
    "        for p in self.param_lists[n]: p.requires_grad_(rg or (state.get('force_train', False) and not ignore_force_train))\n",
    "\n",
    "    def set_hypers(self, **kwargs): L(kwargs.items()).starmap(self.set_hyper)\n",
    "    def _set_hyper(self,\n",
    "        k, # Hyperparameter key\n",
    "        v # Hyperparameter value\n",
    "    ):\n",
    "        for v_,h in zip(v, self.hypers): h[k] = v_\n",
    "    def set_hyper(self,\n",
    "        k, # Hyperparameter key or slice of keys\n",
    "        v # Hyperparameter value or slice of values\n",
    "    ):\n",
    "        if isinstance(v, slice):\n",
    "            if v.start: v = even_mults(v.start, v.stop, len(self.param_lists))\n",
    "            else: v = [v.stop/10]*(len(self.param_lists)-1) + [v.stop]\n",
    "        v = L(v, use_list=None)\n",
    "        if len(v)==1: v = v*len(self.param_lists)\n",
    "        assert len(v) == len(self.hypers), f\"Trying to set {len(v)} values for {k} but there are {len(self.param_lists)} parameter groups.\"\n",
    "        self._set_hyper(k, v)\n",
    "\n",
    "    def unfreeze(self): self.freeze_to(0)\n",
    "    @property\n",
    "    def param_groups(self): return [{**{'params': pg}, **hp} for pg,hp in zip(self.param_lists, self.hypers)]\n",
    "    @param_groups.setter\n",
    "    def param_groups(self,\n",
    "        v:dict # List of dicts to set `params` and other hyper parameters\n",
    "    ):\n",
    "        for pg,v_ in zip(self.param_lists,v): pg = v_['params']\n",
    "        for hyper,v_ in zip(self.hypers,v):\n",
    "            for k,t in v_.items():\n",
    "                if k != 'params': hyper[k] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_docs(_BaseOptimizer, \n",
    "         all_params=\"List of param_groups, parameters, and hypers\",\n",
    "         freeze_to=\"Freeze parameter groups up to `n`\",\n",
    "         freeze=\"Freeze up to last parameter group\",\n",
    "         set_freeze=\"Set `rg` for parameter group `n` only\",\n",
    "         unfreeze=\"Unfreeze the entire model\",\n",
    "         set_hypers=\"`set_hyper` for all `kwargs`\",\n",
    "         set_hyper=\"Set the value(s) in `v` for hyper-parameter `k`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _update(\n",
    "    state:dict,\n",
    "    new=None # New values to update `state` dict\n",
    "):\n",
    "    if new is None: return state\n",
    "    if isinstance(new, dict): state.update(new)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Optimizer` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Optimizer(_BaseOptimizer):\n",
    "    \"Base optimizer class for the fastai library, updating `params` with `cbs`\"\n",
    "    _keep_on_clear = ['force_train', 'do_wd']\n",
    "    def __init__(self,\n",
    "        params:Tensor, # Parameters and hyper parameters\n",
    "        cbs:list, # `Optimizer` callbacks\n",
    "        train_bn:bool=True, # Batch normalization is always trained\n",
    "        **defaults # Default values to set on hyper parameters\n",
    "    ):\n",
    "        params = L(params)\n",
    "        self.cbs,self.state,self.train_bn = L(cbs),defaultdict(dict),train_bn\n",
    "        defaults = merge(*self.cbs.attrgot('defaults'), defaults)\n",
    "        self.param_lists = L(L(p) for p in params) if isinstance(params[0], (L,list)) else L([params])\n",
    "        self.hypers = L({} for _ in range_of(self.param_lists))\n",
    "        self.set_hypers(**defaults)\n",
    "        self.frozen_idx = 0\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p,*_ in self.all_params(with_grad=True):\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for p,pg,state,hyper in self.all_params(with_grad=True):\n",
    "            for cb in self.cbs: state = _update(state, cb(p, **{**state, **hyper}))\n",
    "            self.state[p] = state\n",
    "\n",
    "    def clear_state(self):\n",
    "        for p,pg,state,hyper in self.all_params():\n",
    "            self.state[p] = {k: state[k] for k in self._keep_on_clear if k in state}\n",
    "\n",
    "    def state_dict(self):\n",
    "        state = [self.state[p] for p,*_ in self.all_params()]\n",
    "        return {'state': state, 'hypers': self.hypers}\n",
    "\n",
    "    def load_state_dict(self,\n",
    "        sd:dict # State dict with `hypers` and `state` to load on the optimizer\n",
    "    ):\n",
    "        assert len(sd[\"hypers\"]) == len(self.param_lists)\n",
    "        assert len(sd[\"state\"])  == sum([len(pg) for pg in self.param_lists])\n",
    "        self.hypers = sd['hypers']\n",
    "        self.state = {p: s for p,s in zip(self.all_params().itemgot(0), sd['state'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_docs(Optimizer, \n",
    "         zero_grad=\"Standard PyTorch API: Zero all the grad attributes of the parameters\",\n",
    "         step=\"Standard PyTorch API: Update the stats and execute the steppers in on all parameters that have a grad\",\n",
    "         state_dict=\"Return the state of the optimizer in a dictionary\",\n",
    "         load_state_dict=\"Load the content of `sd`\",\n",
    "         clear_state=\"Reset the state of the optimizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing an Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`params` will be used to create the `param_groups` of the optimizer. If it's a collection (or a generator) of parameters, it will be a `L` containing one `L` with all the parameters. To define multiple parameter groups `params` should be passed as a collection (or a generator) of `L`s.\n",
    "\n",
    "> Note: In PyTorch, <code>model.parameters()</code> returns a generator with all the parameters, that you can directly pass to <code>Optimizer</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer([1,2,3], noop)\n",
    "test_eq(opt.param_lists, [[1,2,3]])\n",
    "opt = Optimizer(range(3), noop)\n",
    "test_eq(opt.param_lists, [[0,1,2]])\n",
    "opt = Optimizer([[1,2],[3]], noop)\n",
    "test_eq(opt.param_lists, [[1,2],[3]])\n",
    "opt = Optimizer(([o,o+1] for o in range(0,4,2)), noop)\n",
    "test_eq(opt.param_lists, [[0,1],[2,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cbs` is a list of functions that will be composed when applying the step. For instance, you can compose a function making the SGD step, with another one applying weight decay. Additionally, each `cb` can have a `defaults` attribute that contains hyper-parameters and their default value. Those are all gathered at initialization, and new values can be passed to override those defaults with the `defaults` kwargs. The steppers will be called by `Optimizer.step` (which is the standard PyTorch name), and gradients can be cleared with `Optimizer.zero_grad` (also a standard PyTorch name).\n",
    "\n",
    "Once the defaults have all been pulled off, they are copied as many times as there are `param_groups` and stored in `hypers`. To apply different hyper-parameters to different groups (differential learning rates, or no weight decay for certain layers for instance), you will need to adjust those values after the init. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tst_arg(p, lr=0, **kwargs): return p\n",
    "tst_arg.defaults = dict(lr=1e-2)\n",
    "\n",
    "def tst_arg2(p, lr2=0, **kwargs): return p\n",
    "tst_arg2.defaults = dict(lr2=1e-3)\n",
    "\n",
    "def tst_arg3(p, mom=0, **kwargs): return p\n",
    "tst_arg3.defaults = dict(mom=0.9)\n",
    "\n",
    "def tst_arg4(p, **kwargs): return p\n",
    "\n",
    "opt = Optimizer([1,2,3], [tst_arg,tst_arg2, tst_arg3])\n",
    "test_eq(opt.hypers, [{'lr2': 1e-3, 'mom': 0.9, 'lr': 1e-2}])\n",
    "opt = Optimizer([1,2,3], tst_arg, lr=0.1)\n",
    "test_eq(opt.hypers, [{'lr': 0.1}])\n",
    "opt = Optimizer([[1,2],[3]], tst_arg)\n",
    "test_eq(opt.hypers, [{'lr': 1e-2}, {'lr': 1e-2}])\n",
    "opt = Optimizer([[1,2],[3]], tst_arg, lr=0.1)\n",
    "test_eq(opt.hypers, [{'lr': 0.1}, {'lr': 0.1}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each hyper-parameter, you can pass a slice or a collection to set them, if there are multiple parameter groups. A slice will be converted to a log-uniform collection from its beginning to its end, or if it only has an end `e`, to a collection of as many values as there are parameter groups that are `...,e/10,e/10,e`.\n",
    "\n",
    "Setting an hyper-parameter with a collection that has a different number of elements than the optimizer has parameter groups will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer([[1,2],[3]], tst_arg, lr=[0.1,0.2])\n",
    "test_eq(opt.hypers, [{'lr': 0.1}, {'lr': 0.2}])\n",
    "opt = Optimizer([[1,2],[3],[4]], tst_arg, lr=slice(1e-2))\n",
    "test_eq(opt.hypers, [{'lr': 1e-3}, {'lr': 1e-3}, {'lr': 1e-2}])\n",
    "opt = Optimizer([[1,2],[3],[4]], tst_arg, lr=slice(1e-4,1e-2))\n",
    "test_eq(opt.hypers, [{'lr': 1e-4}, {'lr': 1e-3}, {'lr': 1e-2}])\n",
    "test_eq(opt.param_groups, [{'params': [1,2], 'lr': 1e-4}, {'params': [3], 'lr': 1e-3}, {'params': [4], 'lr': 1e-2}])\n",
    "test_fail(lambda: Optimizer([[1,2],[3],[4]], tst_arg, lr=np.array([0.1,0.2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic steppers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to give examples of optimizer steps, we will need some steppers, like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def sgd_step(p, lr, **kwargs):\n",
    "    p.data.add_(p.grad.data, alpha=-lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tst_param(val, grad=None):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    res = tensor([val]).float()\n",
    "    res.grad = tensor([val/10 if grad is None else grad]).float()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param(1., 0.1)\n",
    "sgd_step(p, 1.)\n",
    "test_eq(p, tensor([0.9]))\n",
    "test_eq(p.grad, tensor([0.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def weight_decay(p, lr, wd, do_wd=True, **kwargs):\n",
    "    \"Weight decay as decaying `p` with `lr*wd`\"\n",
    "    if do_wd and wd!=0: p.data.mul_(1 - lr*wd)\n",
    "\n",
    "weight_decay.defaults = dict(wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param(1., 0.1)\n",
    "weight_decay(p, 1., 0.1)\n",
    "test_eq(p, tensor([0.9]))\n",
    "test_eq(p.grad, tensor([0.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def l2_reg(p, lr, wd, do_wd=True, **kwargs):\n",
    "    \"L2 regularization as adding `wd*p` to `p.grad`\"\n",
    "    if do_wd and wd!=0: p.grad.data.add_(p.data, alpha=wd)\n",
    "\n",
    "l2_reg.defaults = dict(wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param(1., 0.1)\n",
    "l2_reg(p, 1., 0.1)\n",
    "test_eq(p, tensor([1.]))\n",
    "test_eq(p.grad, tensor([0.2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: Weight decay and L2 regularization is the same thing for basic SGD, but for more complex optimizers, they are very different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Optimizer.step\" class=\"doc_header\"><code>Optimizer.step</code><a href=\"__main__.py#L24\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Optimizer.step</code>()\n",
       "\n",
       "Standard PyTorch API: Update the stats and execute the steppers in on all parameters that have a grad"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Optimizer.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will loop over all param groups, then all parameters for which `grad` is not None and call each function in `stepper`, passing it the parameter `p` with the hyper-parameters in the corresponding dict in `hypers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test basic step\n",
    "r = L.range(4)\n",
    "def tst_params(): return r.map(tst_param)\n",
    "\n",
    "params = tst_params()\n",
    "opt = Optimizer(params, sgd_step, lr=0.1)\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], r.map(mul(0.99)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test two steps\n",
    "params = tst_params()\n",
    "opt = Optimizer(params, [weight_decay, sgd_step], lr=0.1, wd=0.1)\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], r.map(mul(0.98)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test None gradients are ignored\n",
    "params = tst_params()\n",
    "opt = Optimizer(params, sgd_step, lr=0.1)\n",
    "params[-1].grad = None\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [0., 0.99, 1.98, 3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test discriminative lrs\n",
    "params = tst_params()\n",
    "opt = Optimizer([params[:2], params[2:]], sgd_step, lr=0.1)\n",
    "opt.hypers[0]['lr'] = 0.01\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [0., 0.999, 1.98, 2.97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Optimizer.zero_grad\" class=\"doc_header\"><code>Optimizer.zero_grad</code><a href=\"__main__.py#L19\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Optimizer.zero_grad</code>()\n",
       "\n",
       "Standard PyTorch API: Zero all the grad attributes of the parameters"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Optimizer.zero_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tst_params()\n",
    "opt = Optimizer(params, [weight_decay, sgd_step], lr=0.1, wd=0.1)\n",
    "opt.zero_grad()\n",
    "[test_eq(p.grad, tensor([0.])) for p in params];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the `Optimizer` `cbs` can be functions updating the state associated with a parameter. That state can then be used by any stepper. The best example is a momentum calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tst_stat(p, **kwargs): \n",
    "    s = kwargs.get('sum', torch.zeros_like(p)) + p.data\n",
    "    return {'sum': s}\n",
    "tst_stat.defaults = {'mom': 0.9}\n",
    "\n",
    "#Test Optimizer init\n",
    "opt = Optimizer([1,2,3], tst_stat)\n",
    "test_eq(opt.hypers, [{'mom': 0.9}])\n",
    "opt = Optimizer([1,2,3], tst_stat, mom=0.99)\n",
    "test_eq(opt.hypers, [{'mom': 0.99}])\n",
    "\n",
    "#Test stat\n",
    "x = torch.randn(4,5)\n",
    "state = tst_stat(x)\n",
    "assert 'sum' in state\n",
    "test_eq(x, state['sum'])\n",
    "state = tst_stat(x, **state)\n",
    "test_eq(state['sum'], 2*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def average_grad(p, mom, dampening=False, grad_avg=None, **kwargs):\n",
    "    \"Keeps track of the avg grads of `p` in `state` with `mom`.\"\n",
    "    if grad_avg is None: grad_avg = torch.zeros_like(p.grad.data)\n",
    "    damp = 1-mom if dampening else 1.\n",
    "    grad_avg.mul_(mom).add_(p.grad.data, alpha=damp)\n",
    "    return {'grad_avg': grad_avg}\n",
    "\n",
    "average_grad.defaults = dict(mom=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dampening=False` gives the classical formula for momentum in SGD: \n",
    "```\n",
    "new_val = old_val * mom + grad\n",
    "```\n",
    "whereas `dampening=True` makes it an exponential moving average:\n",
    "```\n",
    "new_val = old_val * mom + grad * (1-mom)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param([1,2,3], [4,5,6])\n",
    "state = {}\n",
    "state = average_grad(p, mom=0.9, **state)\n",
    "test_eq(state['grad_avg'], p.grad)\n",
    "state = average_grad(p, mom=0.9, **state)\n",
    "test_eq(state['grad_avg'], p.grad * 1.9)\n",
    "\n",
    "#Test dampening\n",
    "state = {}\n",
    "state = average_grad(p,  mom=0.9, dampening=True, **state)\n",
    "test_eq(state['grad_avg'], 0.1*p.grad)\n",
    "state = average_grad(p, mom=0.9, dampening=True, **state)\n",
    "test_close(state['grad_avg'], (0.1*0.9+0.1)*p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def average_sqr_grad(p, sqr_mom, dampening=True, sqr_avg=None, **kwargs):\n",
    "    if sqr_avg is None: sqr_avg = torch.zeros_like(p.grad.data)\n",
    "    damp = 1-sqr_mom if dampening else 1.\n",
    "    sqr_avg.mul_(sqr_mom).addcmul_(p.grad.data, p.grad.data, value=damp)\n",
    "    return {'sqr_avg': sqr_avg}\n",
    "\n",
    "average_sqr_grad.defaults = dict(sqr_mom=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dampening=False` gives the classical formula for momentum in SGD: \n",
    "```\n",
    "new_val = old_val * mom + grad**2\n",
    "```\n",
    "whereas `dampening=True` makes it an exponential moving average:\n",
    "```\n",
    "new_val = old_val * mom + (grad**2) * (1-mom)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param([1,2,3], [4,5,6])\n",
    "state = {}\n",
    "state = average_sqr_grad(p, sqr_mom=0.99, dampening=False, **state)\n",
    "test_eq(state['sqr_avg'], p.grad.pow(2))\n",
    "state = average_sqr_grad(p, sqr_mom=0.99, dampening=False, **state)\n",
    "test_eq(state['sqr_avg'], p.grad.pow(2) * 1.99)\n",
    "\n",
    "#Test dampening\n",
    "state = {}\n",
    "state = average_sqr_grad(p, sqr_mom=0.99, **state)\n",
    "test_close(state['sqr_avg'], 0.01*p.grad.pow(2))\n",
    "state = average_sqr_grad(p, sqr_mom=0.99, **state)\n",
    "test_close(state['sqr_avg'], (0.01*0.99+0.01)*p.grad.pow(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Optimizer.freeze\" class=\"doc_header\"><code>Optimizer.freeze</code><a href=\"__main__.py#L28\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Optimizer.freeze</code>()\n",
       "\n",
       "Freeze up to last parameter group"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Optimizer.freeze, name=\"Optimizer.freeze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Optimizer.freeze_to\" class=\"doc_header\"><code>Optimizer.freeze_to</code><a href=\"__main__.py#L19\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Optimizer.freeze_to</code>(**`n`**:`int`)\n",
       "\n",
       "Freeze parameter groups up to `n`\n",
       "\n",
       "||Type|Default|Details|\n",
       "|---|---|---|---|\n",
       "|**`n`**|`int`||Freeze up to `n` layers|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Optimizer.freeze_to, name=\"Optimizer.freeze_to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Optimizer.unfreeze\" class=\"doc_header\"><code>Optimizer.unfreeze</code><a href=\"__main__.py#L57\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Optimizer.unfreeze</code>()\n",
       "\n",
       "Unfreeze the entire model"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Optimizer.unfreeze, name=\"Optimizer.unfreeze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freezing the first layer\n",
    "params = [tst_params(), tst_params(), tst_params()]\n",
    "opt = Optimizer(params, sgd_step, lr=0.1)\n",
    "opt.freeze_to(1)\n",
    "req_grad = Self.requires_grad()\n",
    "test_eq(L(params[0]).map(req_grad), [False]*4)\n",
    "for i in {1,2}: test_eq(L(params[i]).map(req_grad), [True]*4)\n",
    "    \n",
    "#Unfreezing\n",
    "opt.unfreeze()\n",
    "for i in range(2): test_eq(L(params[i]).map(req_grad), [True]*4)\n",
    "\n",
    "#TODO: test warning\n",
    "# opt.freeze_to(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters such as batchnorm weights/bias can be marked to always be in training mode, just put `force_train=true` in their state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [tst_params(), tst_params(), tst_params()]\n",
    "opt = Optimizer(params, sgd_step, lr=0.1)\n",
    "for p in L(params[1])[[1,3]]: opt.state[p] = {'force_train': True}\n",
    "opt.freeze()\n",
    "test_eq(L(params[0]).map(req_grad), [False]*4)\n",
    "test_eq(L(params[1]).map(req_grad), [False, True, False, True])\n",
    "test_eq(L(params[2]).map(req_grad), [True]*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Optimizer.state_dict\" class=\"doc_header\"><code>Optimizer.state_dict</code><a href=\"__main__.py#L33\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Optimizer.state_dict</code>()\n",
       "\n",
       "Return the state of the optimizer in a dictionary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Optimizer.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Optimizer.load_state_dict\" class=\"doc_header\"><code>Optimizer.load_state_dict</code><a href=\"__main__.py#L37\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Optimizer.load_state_dict</code>(**`sd`**:`dict`)\n",
       "\n",
       "Load the content of `sd`\n",
       "\n",
       "||Type|Default|Details|\n",
       "|---|---|---|---|\n",
       "|**`sd`**|`dict`||State dict with `hypers` and `state` to load on the optimizer|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Optimizer.load_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param([1,2,3], [4,5,6])\n",
    "opt = Optimizer(p, average_grad)\n",
    "opt.step()\n",
    "test_eq(opt.state[p]['grad_avg'], tensor([[4., 5., 6.]]))\n",
    "\n",
    "sd = opt.state_dict()\n",
    "p1 = tst_param([10,20,30], [40,50,60])\n",
    "opt = Optimizer(p1, average_grad, mom=0.99)\n",
    "test_eq(opt.hypers[0]['mom'], 0.99)\n",
    "test_eq(opt.state, {})\n",
    "\n",
    "opt.load_state_dict(sd)\n",
    "test_eq(opt.hypers[0]['mom'], 0.9)\n",
    "test_eq(opt.state[p1]['grad_avg'], tensor([[4., 5., 6.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Optimizer.clear_state\" class=\"doc_header\"><code>Optimizer.clear_state</code><a href=\"__main__.py#L29\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Optimizer.clear_state</code>()\n",
       "\n",
       "Reset the state of the optimizer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Optimizer.clear_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param([1,2,3], [4,5,6])\n",
    "opt = Optimizer(p, average_grad)\n",
    "opt.state[p] = {'force_train': True}\n",
    "opt.step()\n",
    "test_eq(opt.state[p]['grad_avg'], tensor([[4., 5., 6.]]))\n",
    "\n",
    "opt.clear_state()\n",
    "test_eq(opt.state[p], {'force_train': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def momentum_step(p, lr, grad_avg, **kwargs):\n",
    "    \"Step for SGD with momentum with `lr`\"\n",
    "    p.data.add_(grad_avg, alpha=-lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def SGD(params, lr, mom=0., wd=0., decouple_wd=True):\n",
    "    \"A `Optimizer` for SGD with `lr` and `mom` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    if mom != 0: cbs.append(average_grad)\n",
    "    cbs.append(sgd_step if mom==0 else momentum_step)\n",
    "    return Optimizer(params, cbs, lr=lr, mom=mom, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vanilla SGD\n",
    "params = tst_params()\n",
    "opt = SGD(params, lr=0.1)\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [i*0.99 for i in range(4)])\n",
    "opt.step()\n",
    "[p.item() for p in params]\n",
    "test_close([p.item() for p in params], [i*0.98 for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD with momentum\n",
    "params = tst_params()\n",
    "opt = SGD(params, lr=0.1, mom=0.9)\n",
    "assert isinstance(opt, Optimizer)\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [i*0.99 for i in range(4)])\n",
    "opt.step()\n",
    "[p.item() for p in params]\n",
    "test_close([p.item() for p in params], [i*(1 - 0.1 * (0.1 + 0.1*1.9)) for i in range(4)])\n",
    "for i,p in enumerate(params): test_close(opt.state[p]['grad_avg'].item(), i*0.19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test weight decay, notice how we can see that L2 regularization is different from weight decay even for simple SGD with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tst_params()\n",
    "#Weight decay\n",
    "opt = SGD(params, lr=0.1, mom=0.9, wd=0.1)\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [i*0.98 for i in range(4)])\n",
    "#L2 reg\n",
    "opt = SGD(params, lr=0.1, mom=0.9, wd=0.1, decouple_wd=False)\n",
    "opt.step()\n",
    "#TODO: fix cause this formula was wrong\n",
    "#test_close([p.item() for p in params], [i*0.97 for i in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def rms_prop_step(p, lr, sqr_avg, eps, grad_avg=None, **kwargs):\n",
    "    \"Step for SGD with momentum with `lr`\"\n",
    "    denom = sqr_avg.sqrt().add_(eps)\n",
    "    p.data.addcdiv_((grad_avg if grad_avg is not None else p.grad), denom, value=-lr)\n",
    "\n",
    "rms_prop_step.defaults = dict(eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RMSProp(params, lr, sqr_mom=0.99, mom=0., wd=0., decouple_wd=True):\n",
    "    \"A `Optimizer` for RMSProp with `lr`, `sqr_mom`, `mom` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += ([average_sqr_grad] if mom==0. else [average_grad, average_sqr_grad])\n",
    "    cbs.append(rms_prop_step)\n",
    "    return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp was introduced by Geoffrey Hinton in his [course](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf). What is named `sqr_mom` here is the `alpha` in the course. Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without momentum\n",
    "params = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt = RMSProp(params, lr=0.1)\n",
    "opt.step()\n",
    "test_close(params[0], tensor([0.,1.,2.]))\n",
    "opt.step()\n",
    "step = - 0.1 * 0.1 / (math.sqrt((0.01*0.99+0.01) * 0.1**2) + 1e-8)\n",
    "test_close(params[0], tensor([step, 1+step, 2+step]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With momentum\n",
    "params = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt = RMSProp(params, lr=0.1, mom=0.9)\n",
    "opt.step()\n",
    "test_close(params[0], tensor([0.,1.,2.]))\n",
    "opt.step()\n",
    "step = - 0.1 * (0.1 + 0.9*0.1) / (math.sqrt((0.01*0.99+0.01) * 0.1**2) + 1e-8)\n",
    "test_close(params[0], tensor([step, 1+step, 2+step]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def step_stat(p, step=0, **kwargs):\n",
    "    \"Register the number of steps done in `state` for `p`\"\n",
    "    step += 1\n",
    "    return {'step' : step}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param(1,0.1)\n",
    "state = {}\n",
    "state = step_stat(p, **state)\n",
    "test_eq(state['step'], 1)\n",
    "for _ in range(5): state = step_stat(p, **state)\n",
    "test_eq(state['step'], 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def debias(mom, damp, step): return damp * (1 - mom**step) / (1-mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def adam_step(p, lr, mom, step, sqr_mom, grad_avg, sqr_avg, eps, **kwargs):\n",
    "    \"Step for Adam with `lr` on `p`\"\n",
    "    debias1 = debias(mom,     1-mom,     step)\n",
    "    debias2 = debias(sqr_mom, 1-sqr_mom, step)\n",
    "    p.data.addcdiv_(grad_avg, (sqr_avg/debias2).sqrt() + eps, value = -lr / debias1)\n",
    "    return p\n",
    "\n",
    "adam_step._defaults = dict(eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0.01, decouple_wd=True):\n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step]\n",
    "    return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam was introduced by Diederik P. Kingma and Jimmy Ba in [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980). For consistency across optimizers, we renamed `beta1` and `beta2` in the paper to `mom` and  `sqr_mom`. Note that our defaults also differ from the paper (0.99 for `sqr_mom` or `beta2`, 1e-5 for `eps`). Those values seem to be better from our experiments in a wide range of situations.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients).\n",
    "\n",
    "> Note: Don't forget that `eps` is an hyper-parameter you can change. Some models won't train without a very high `eps` like 0.1 (intuitively, the higher `eps` is, the closer we are to normal SGD). The usual default of 1e-8 is often too extreme in the sense we don't manage to get as good results as with SGD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt = Adam(params, lr=0.1, wd=0)\n",
    "opt.step()\n",
    "step = -0.1 * 0.1 / (math.sqrt(0.1**2) + 1e-8)\n",
    "test_close(params[0], tensor([1+step, 2+step, 3+step]))\n",
    "opt.step()\n",
    "test_close(params[0], tensor([1+2*step, 2+2*step, 3+2*step]), eps=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAdam (for rectified Adam) was introduced by Zhang et al. in [On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/abs/1907.08610) to slightly modify the Adam optimizer to be more stable at the beginning of training (and thus not require a long warmup). They use an estimate of the variance of the moving average of the squared gradients (the term in the denominator of traditional Adam) and rescale this moving average by this term before performing the update.\n",
    "\n",
    "This version also incorporates [SAdam](https://arxiv.org/abs/1908.00700); set `beta` to enable this (definition same as in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def radam_step(p, lr, mom, step, sqr_mom, grad_avg, sqr_avg, eps, beta, **kwargs):\n",
    "    \"Step for RAdam with `lr` on `p`\"\n",
    "    debias1 = debias(mom,     1-mom,     step)\n",
    "    debias2 = debias(sqr_mom, 1-sqr_mom, step)\n",
    "    r_inf = 2/(1-sqr_mom) - 1\n",
    "    r = r_inf - 2*step*sqr_mom**step/(1-sqr_mom**step)\n",
    "    if r > 5:\n",
    "        v = math.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "        denom = (sqr_avg/debias2).sqrt()\n",
    "        if eps: denom += eps\n",
    "        if beta: denom = F.softplus(denom, beta)\n",
    "        p.data.addcdiv_(grad_avg, denom, value = -lr*v / debias1)\n",
    "    else: p.data.add_(grad_avg, alpha=-lr / debias1)\n",
    "    return p\n",
    "\n",
    "radam_step._defaults = dict(eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RAdam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., beta=0., decouple_wd=True):\n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, radam_step]\n",
    "    return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, beta=beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the effective correction reported to the adam step for 500 iterations in RAdam. We can see how it goes from 0 to 1, mimicking the effect of a warm-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta = 0.99\n",
    "r_inf = 2/(1-beta) - 1\n",
    "rs = np.array([r_inf - 2*s*beta**s/(1-beta**s) for s in range(5,500)])\n",
    "v = np.sqrt(((rs-4) * (rs-2) * r_inf)/((r_inf-4)*(r_inf-2)*rs))\n",
    "plt.plot(v);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt = RAdam(params, lr=0.1)\n",
    "#The r factor is lower than 5 during the first 5 steps so updates use the average of gradients (all the same)\n",
    "r_inf = 2/(1-0.99) - 1\n",
    "for i in range(5): \n",
    "    r = r_inf - 2*(i+1)*0.99**(i+1)/(1-0.99**(i+1))\n",
    "    assert r <= 5\n",
    "    opt.step()\n",
    "p = tensor([0.95, 1.9, 2.85])\n",
    "test_close(params[0], p)\n",
    "\n",
    "#The r factor is greater than 5 for the sixth step so we update with RAdam\n",
    "r = r_inf - 2*6*0.99**6/(1-0.99**6)\n",
    "assert r > 5\n",
    "opt.step()\n",
    "v = math.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "step = -0.1*0.1*v/(math.sqrt(0.1**2) + 1e-8)\n",
    "test_close(params[0], p+step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QHAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QHAdam (for Quasi-Hyperbolic Adam) was introduced by Ma & Yarats in [Quasi-Hyperbolic Momentum and Adam for Deep Learning](https://arxiv.org/pdf/1810.06801.pdf) as a *\"computationally cheap, intuitive to interpret, and simple to implement\"* optimizer. Additional code can be found in their [qhoptim repo](https://github.com/facebookresearch/qhoptim). QHAdam is based on QH-Momentum, which introduces the immediate discount factor `nu`, encapsulating plain SGD (`nu = 0`) and momentum (`nu = 1`). QH-Momentum is defined below, where g_t+1 is the update of the moment. An interpretation of QHM is as a nu-weighted average of the momentum update step and the plain SGD update step.\n",
    "\n",
    "> θ_t+1 ← θ_t − lr * [(1 − nu) · ∇L_t(θ_t) + nu · g_t+1]\n",
    "\n",
    "QHAdam takes the concept behind QHM above and applies it to Adam, replacing both of Adam’s moment estimators with quasi-hyperbolic terms. \n",
    "\n",
    "The paper's suggested default parameters are `mom = 0.999`, `sqr_mom = 0.999`, `nu_1 = 0.7` and `and nu_2 = 1.0`. When training is not stable, it is possible that setting `nu_2 < 1` can improve stability by imposing a tighter step size bound. Note that QHAdam recovers Adam when `nu_1 = nu_2 = 1.0`. QHAdam recovers RMSProp (Hinton et al., 2012) when `nu_1 = 0` and `nu_2 = 1`, and NAdam (Dozat, 2016) when `nu_1 = mom` and `nu_2 = 1`.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def qhadam_step(p, lr, mom, sqr_mom, sqr_avg, nu_1, nu_2, step, grad_avg, eps, **kwargs):\n",
    "    debias1 = debias(mom,     1-mom,     step)\n",
    "    debias2 = debias(sqr_mom, 1-sqr_mom, step)\n",
    "    p.data.addcdiv_(((1-nu_1) * p.grad.data) + (nu_1 * (grad_avg / debias1)),\n",
    "                    (((1 - nu_2) * (p.grad.data)**2) + (nu_2 * (sqr_avg / debias2))).sqrt() + eps,\n",
    "                    value = -lr)\n",
    "    return p\n",
    "\n",
    "qhadam_step._defaults = dict(eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def QHAdam(params, lr, mom=0.999, sqr_mom=0.999, nu_1=0.7, nu_2 = 1.0, eps=1e-8, wd=0., decouple_wd=True):\n",
    "    \"An `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `nus`, eps` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += [partial(average_grad, dampening=True), partial(average_sqr_grad, dampening=True), step_stat, qhadam_step]\n",
    "    return Optimizer(params, cbs, lr=lr, nu_1=nu_1, nu_2=nu_2 ,\n",
    "                     mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt = QHAdam(params, lr=0.1)\n",
    "opt.step()\n",
    "step = -0.1 * (((1-0.7) * 0.1) + (0.7 * 0.1)) / (\n",
    "     math.sqrt(((1-1.0) * 0.1**2) + (1.0 * 0.1**2)) + 1e-8) \n",
    "test_close(params[0], tensor([1+step, 2+step, 3+step]))\n",
    "opt.step()\n",
    "test_close(params[0], tensor([1+2*step, 2+2*step, 3+2*step]), eps=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LARS/LARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def larc_layer_lr(p, lr, trust_coeff, wd, eps, clip=True, **kwargs):\n",
    "    \"Computes the local lr before weight decay is applied\"\n",
    "    p_norm,g_norm = torch.norm(p.data),torch.norm(p.grad.data)\n",
    "    local_lr = lr*trust_coeff * (p_norm) / (g_norm + p_norm * wd + eps)\n",
    "    return {'local_lr': min(lr, local_lr) if clip else local_lr}\n",
    "\n",
    "larc_layer_lr.defaults = dict(trust_coeff=0.02, wd=0., eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def larc_step(p, local_lr, grad_avg=None, **kwargs):\n",
    "    \"Step for LARC `local_lr` on `p`\"\n",
    "    p.data.add_(p.grad.data if grad_avg is None else grad_avg, alpha = -local_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Larc(params, lr, mom=0.9, clip=True, trust_coeff=0.02, eps=1e-8, wd=0., decouple_wd=True):\n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    if mom!=0.: cbs.append(average_grad)\n",
    "    cbs += [partial(larc_layer_lr, clip=clip), larc_step]\n",
    "    return Optimizer(params, cbs, lr=lr, mom=mom, trust_coeff=trust_coeff, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LARS optimizer was first introduced in [Large Batch Training of Convolutional Networks](https://arxiv.org/abs/1708.03888) then refined in its LARC variant (original LARS is with `clip=False`). A learning rate is computed for each individual layer with a certain `trust_coefficient`, then clipped to be always less than `lr`.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt = Larc(params, lr=0.1)\n",
    "opt.step()\n",
    "#First param local lr is 0.02 < lr so it's not clipped\n",
    "test_close(opt.state[params[0]]['local_lr'], 0.02)\n",
    "#Second param local lr is 0.2 > lr so it's clipped\n",
    "test_eq(opt.state[params[1]]['local_lr'], 0.1)\n",
    "test_close(params[0], tensor([0.998,1.996,2.994]))\n",
    "test_close(params[1], tensor([0.999,1.998,2.997]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt = Larc(params, lr=0.1, clip=False)\n",
    "opt.step()\n",
    "#No clipping\n",
    "test_close(opt.state[params[0]]['local_lr'], 0.02)\n",
    "test_close(opt.state[params[1]]['local_lr'], 0.2)\n",
    "test_close(params[0], tensor([0.998,1.996,2.994]))\n",
    "test_close(params[1], tensor([0.998,1.996,2.994]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def lamb_step(p, lr, mom, step, sqr_mom, grad_avg, sqr_avg, eps, **kwargs):\n",
    "    \"Step for LAMB with `lr` on `p`\"\n",
    "    debias1 = debias(mom,     1-mom,     step)\n",
    "    debias2 = debias(sqr_mom, 1-sqr_mom, step)\n",
    "    r1 = p.data.pow(2).mean().sqrt()\n",
    "    step = (grad_avg/debias1) / ((sqr_avg/debias2).sqrt()+eps)\n",
    "    r2 = step.pow(2).mean().sqrt()\n",
    "    q = 1 if r1 == 0 or r2 == 0 else min(r1/r2,10)\n",
    "    p.data.add_(step, alpha = -lr * q)\n",
    "\n",
    "lamb_step._defaults = dict(eps=1e-6, wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Lamb(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., decouple_wd=True):\n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, lamb_step]\n",
    "    return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAMB was introduced in [Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://arxiv.org/abs/1904.00962). Intuitively, it's LARC applied to Adam. As in `Adam`, we renamed `beta1` and `beta2` in the paper to `mom` and  `sqr_mom`. Note that our defaults also differ from the paper (0.99 for `sqr_mom` or `beta2`, 1e-5 for `eps`). Those values seem to be better from our experiments in a wide range of situations.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt = Lamb(params, lr=0.1)\n",
    "opt.step()\n",
    "test_close(params[0], tensor([0.7840,1.7840,2.7840]), eps=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookahead -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookahead was introduced by Zhang et al. in [Lookahead Optimizer: k steps forward, 1 step back](https://arxiv.org/abs/1907.08610). It can be run on top of any optimizer and consists in having the final weights of the model be a moving average. In practice, we update our model using the internal optimizer but keep a copy of old weights that and every `k` steps, we change the weights by a moving average of the *fast weights* (the ones updated by the inner optimizer) with the *slow weights* (the copy of old weights). Those *slow weights* act like a stability mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Lookahead(Optimizer, GetAttr):\n",
    "    \"Wrap `opt` in a lookahead optimizer\"\n",
    "    _default='opt'\n",
    "    def __init__(self, opt, k=6, alpha=0.5):\n",
    "        store_attr('opt,k,alpha')\n",
    "        self._init_state()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        if self.slow_weights is None: self._copy_weights()\n",
    "        self.opt.step()\n",
    "        self.count += 1\n",
    "        if self.count%self.k != 0: return\n",
    "        for slow_pg,fast_pg in zip(self.slow_weights,self.param_lists):\n",
    "            for slow_p,fast_p in zip(slow_pg,fast_pg):\n",
    "                slow_p.data.add_(fast_p.data-slow_p.data, alpha=self.alpha)\n",
    "                fast_p.data.copy_(slow_p.data)\n",
    "\n",
    "    def clear_state(self):\n",
    "        self.opt.clear_state()\n",
    "        self._init_state()\n",
    "\n",
    "    def state_dict(self):\n",
    "        state = self.opt.state_dict()\n",
    "        state.update({'count': self.count, 'slow_weights': self.slow_weights})\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, sd):\n",
    "        self.count = sd.pop('count')\n",
    "        self.slow_weights = sd.pop('slow_weights')\n",
    "        self.opt.load_state_dict(sd)\n",
    "\n",
    "    def _init_state(self): self.count,self.slow_weights = 0,None\n",
    "    def _copy_weights(self): self.slow_weights = L(L(p.clone().detach() for p in pg) for pg in self.param_lists)\n",
    "\n",
    "    @property\n",
    "    def param_lists(self): return self.opt.param_lists\n",
    "    @param_lists.setter\n",
    "    def param_lists(self, v): self.opt.param_lists = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "p,g = params[0].data.clone(),tensor([0.1,0.2,0.3])\n",
    "opt = Lookahead(SGD(params, lr=0.1))\n",
    "for k in range(5): opt.step()\n",
    "#first 5 steps are normal SGD steps\n",
    "test_close(params[0], p - 0.5*g)\n",
    "#Since k=6, sixth step is a moving average of the 6 SGD steps with the initial weight\n",
    "opt.step()\n",
    "test_close(params[0], p * 0.5 + (p-0.6*g) * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@delegates(RAdam)\n",
    "def ranger(p, lr, mom=0.95, wd=0.01, eps=1e-6, **kwargs):\n",
    "    \"Convenience method for `Lookahead` with `RAdam`\"\n",
    "    return Lookahead(RAdam(p, lr=lr, mom=mom, wd=wd, eps=eps, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OptimWrapper -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OptimWrapper` provides simple functionality to use existing optimizers constructed with [`torch.optim.Optimizer`](https://pytorch.org/docs/stable/_modules/torch/optim/optimizer.html#Optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def detuplify_pg(d):\n",
    "    res = {}\n",
    "    for k,v in d.items():\n",
    "        if k == 'params': continue\n",
    "        if is_listy(v): res.update(**{f'{k}__{i}': v_ for i,v_ in enumerate(v)})\n",
    "        else: res[k] = v\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = {'lr': 1e-2, 'mom': 0.9, 'params':[0,1,2]}\n",
    "test_eq(detuplify_pg(tst), {'lr': 1e-2, 'mom': 0.9})\n",
    "tst = {'lr': 1e-2, 'betas': (0.9,0.999), 'params':[0,1,2]}\n",
    "test_eq(detuplify_pg(tst), {'lr': 1e-2, 'betas__0': 0.9, 'betas__1': 0.999})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def set_item_pg(pg, k, v):\n",
    "    if '__' not in k: pg[k] = v\n",
    "    else:\n",
    "        name,idx = k.split('__')\n",
    "        pg[name] = tuple(v if i==int(idx) else pg[name][i] for i in range_of(pg[name]))\n",
    "    return pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = {'lr': 1e-2, 'mom': 0.9, 'params':[0,1,2]}\n",
    "test_eq(set_item_pg(tst, 'lr', 1e-3), {'lr': 1e-3, 'mom': 0.9, 'params':[0,1,2]})\n",
    "tst = {'lr': 1e-2, 'betas': (0.9,0.999), 'params':[0,1,2]}\n",
    "test_eq(set_item_pg(tst, 'betas__0', 0.95), {'lr': 1e-2, 'betas': (0.95,0.999), 'params':[0,1,2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "pytorch_hp_map = {'momentum': 'mom', 'weight_decay': 'wd', 'alpha': 'sqr_mom', 'betas__0': 'mom', 'betas__1': 'sqr_mom'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _convert_params(o:list) -> list:\n",
    "    splitter = []\n",
    "    for group in o:\n",
    "        if isinstance(group, dict): splitter.append(group)\n",
    "        else: splitter.append({'params':group})\n",
    "    return splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class OptimWrapper(_BaseOptimizer, GetAttr):\n",
    "    \"A wrapper class for existing PyTorch optimizers\"\n",
    "    _xtra=['zero_grad', 'step', 'state_dict', 'load_state_dict']\n",
    "    _default='opt'\n",
    "    def __init__(self, \n",
    "         params:list|dict=None, # Model parameters to pass to `opt`. If using an already built `opt`\n",
    "         opt:callable|torch.optim.Optimizer=None, # A torch optimizer constructor, or an already built optimizer \n",
    "         hp_map:dict=None, # A dictionary converting the keys of a built `opt` to the keys of fastai's Optimizer\n",
    "         convert_groups=True, # Whether to convert parameter groups\n",
    "         **kwargs\n",
    "    ):\n",
    "        if params is None and opt is None: raise ValueError(\"Both `params` and `opt` cannot be None.\")\n",
    "        if callable(opt):\n",
    "            self.opt = opt(_convert_params(params), **kwargs) if convert_groups else opt(params, **kwargs)\n",
    "        else:\n",
    "            if params is not None: raise ValueError(\"Tried using both `params` and a built optimizer. Just pass in `opt`.\")\n",
    "            self.opt = opt\n",
    "        if hp_map is None: hp_map = pytorch_hp_map\n",
    "        self.fwd_map = {k: hp_map[k] if k in hp_map else k for k in detuplify_pg(self.opt.param_groups[0]).keys()}\n",
    "        self.bwd_map = {v:k for k,v in self.fwd_map.items()}\n",
    "        self.state = defaultdict(dict, {})\n",
    "        self.frozen_idx = 0\n",
    "\n",
    "    @property\n",
    "    def hypers(self):\n",
    "        return [{self.fwd_map[k]:v for k,v in detuplify_pg(pg).items() if k != 'params'} for pg in self.opt.param_groups]\n",
    "\n",
    "    def _set_hyper(self, k, v):\n",
    "        for pg,v_ in zip(self.opt.param_groups,v): pg = set_item_pg(pg, self.bwd_map[k], v_)\n",
    "\n",
    "    def clear_state(self): self.opt.state = defaultdict(dict, {})\n",
    "\n",
    "    @property\n",
    "    def param_lists(self): return [pg['params'] for pg in self.opt.param_groups]\n",
    "    @param_lists.setter\n",
    "    def param_lists(self, v):\n",
    "        for pg,v_ in zip(self.opt.param_groups,v): pg['params'] = v_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD([tensor([1,2,3])], lr=1e-3, mom=0.9, wd=1e-2)\n",
    "tst_sgd = OptimWrapper([tensor([1,2,3])], torch.optim.SGD, lr=1e-3, momentum=0.9, weight_decay=1e-2)\n",
    "#Access to param_groups\n",
    "test_eq(tst_sgd.param_lists, sgd.param_lists)\n",
    "#Set param_groups\n",
    "tst_sgd.param_lists = [[tensor([4,5,6])]]\n",
    "test_eq(tst_sgd.opt.param_groups[0]['params'], [tensor(4,5,6)])\n",
    "#Access to hypers\n",
    "_xtra_hypers = dict(dampening=0., nesterov=False, maximize=False)\n",
    "test_eq(tst_sgd.hypers, [{**sgd.hypers[0], **_xtra_hypers}])\n",
    "#Set hypers\n",
    "tst_sgd.set_hyper('mom', 0.95)\n",
    "test_eq(tst_sgd.opt.param_groups[0]['momentum'], 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_sgd = OptimWrapper([{'params': [tensor([1,2,3])], 'lr': 1e-3}, \n",
    "                                        {'params': [tensor([4,5,6])], 'lr': 1e-2}], torch.optim.SGD, momentum=0.9, weight_decay=1e-2)\n",
    "sgd = SGD([[tensor([1,2,3])], [tensor([4,5,6])]], lr=[1e-3, 1e-2], mom=0.9, wd=1e-2)\n",
    "#Access to param_groups\n",
    "test_eq(tst_sgd.param_lists, sgd.param_lists)\n",
    "#Set param_groups\n",
    "tst_sgd.param_lists = [[tensor([4,5,6])], [tensor([1,2,3])]]\n",
    "test_eq(tst_sgd.opt.param_groups[0]['params'], [tensor(4,5,6)])\n",
    "test_eq(tst_sgd.opt.param_groups[1]['params'], [tensor(1,2,3)])\n",
    "#Access to hypers\n",
    "test_eq(tst_sgd.hypers, [{**sgd.hypers[i], **_xtra_hypers} for i in range(2)])\n",
    "#Set hypers\n",
    "tst_sgd.set_hyper('mom', 0.95)\n",
    "test_eq([pg['momentum'] for pg in tst_sgd.opt.param_groups], [0.95,0.95])\n",
    "tst_sgd.set_hyper('lr', [1e-4,1e-3])\n",
    "test_eq([pg['lr'] for pg in tst_sgd.opt.param_groups], [1e-4,1e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we can use an already made optimizer\n",
    "tst_sgd = torch.optim.SGD([{'params': [tensor([1,2,3])], 'lr': 1e-3}, \n",
    "                                        {'params': [tensor([4,5,6])], 'lr': 1e-2}])\n",
    "tst_sgd = OptimWrapper(opt = tst_sgd)\n",
    "sgd = SGD([[tensor([1,2,3])], [tensor([4,5,6])]], lr=[1e-3, 1e-2])\n",
    "#Access to param_groups\n",
    "test_eq(tst_sgd.param_lists, sgd.param_lists)\n",
    "#Set param_groups\n",
    "tst_sgd.param_lists = [[tensor([4,5,6])], [tensor([1,2,3])]]\n",
    "test_eq(tst_sgd.opt.param_groups[0]['params'], [tensor(4,5,6)])\n",
    "test_eq(tst_sgd.opt.param_groups[1]['params'], [tensor(1,2,3)])\n",
    "#Access to hypers\n",
    "test_eq(tst_sgd.hypers, [{**sgd.hypers[i], **_xtra_hypers} for i in range(2)])\n",
    "#Set hypers\n",
    "tst_sgd.set_hyper('mom', 0.95)\n",
    "test_eq([pg['momentum'] for pg in tst_sgd.opt.param_groups], [0.95,0.95])\n",
    "tst_sgd.set_hyper('lr', [1e-4,1e-3])\n",
    "test_eq([pg['lr'] for pg in tst_sgd.opt.param_groups], [1e-4,1e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#check it works with tuply hp names like in Adam\n",
    "tst_adam = OptimWrapper([tensor([1,2,3])], torch.optim.Adam, lr=1e-2, betas=(0.9, 0.99))\n",
    "test_eq(tst_adam.hypers, [{\n",
    "    'lr': 0.01, 'mom': 0.9, 'sqr_mom': 0.99, 'eps': 1e-08, 'wd': 0, 'amsgrad': False, 'maximize':False}])\n",
    "tst_adam.set_hyper('mom', 0.95)\n",
    "test_eq(tst_adam.opt.param_groups[0]['betas'], (0.95, 0.99))\n",
    "tst_adam.set_hyper('sqr_mom', 0.9)\n",
    "test_eq(tst_adam.opt.param_groups[0]['betas'], (0.95, 0.9))\n",
    "\n",
    "tst_adam = torch.optim.Adam([tensor([1,2,3])], lr=1e-2, betas=(0.9, 0.99))\n",
    "tst_adam = OptimWrapper(opt=tst_adam)\n",
    "test_eq(tst_adam.hypers, [{\n",
    "    'lr': 0.01, 'mom': 0.9, 'sqr_mom': 0.99, 'eps': 1e-08, 'wd': 0, 'amsgrad': False, 'maximize':False}])\n",
    "tst_adam.set_hyper('mom', 0.95)\n",
    "test_eq(tst_adam.opt.param_groups[0]['betas'], (0.95, 0.99))\n",
    "tst_adam.set_hyper('sqr_mom', 0.9)\n",
    "test_eq(tst_adam.opt.param_groups[0]['betas'], (0.95, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mock_train(m, x, y, opt):\n",
    "    m.train()\n",
    "    for i in range(0, 100, 25):\n",
    "        z = m(x[i:i+25])\n",
    "        loss = F.mse_loss(z, y[i:i+25])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(4,5)\n",
    "x = torch.randn(100, 3, 4)\n",
    "y = torch.randn(100, 3, 5)\n",
    "try:\n",
    "    torch.save(m.state_dict(), 'tmp.pth')\n",
    "    wgt,bias = m.weight.data.clone(),m.bias.data.clone()\n",
    "\n",
    "    m.load_state_dict(torch.load('tmp.pth'))\n",
    "    opt1 = OptimWrapper(m.parameters(), torch.optim.AdamW, betas=(0.9, 0.99), eps=1e-5, weight_decay=1e-2)\n",
    "    _mock_train(m, x.clone(), y.clone(), opt1)\n",
    "    wgt1,bias1 = m.weight.data.clone(),m.bias.data.clone()\n",
    "\n",
    "    m.load_state_dict(torch.load('tmp.pth'))\n",
    "    opt2 = Adam(m.parameters(), 1e-3, wd=1e-2)\n",
    "    _mock_train(m, x.clone(), y.clone(), opt2)\n",
    "    wgt2,bias2 = m.weight.data.clone(),m.bias.data.clone()\n",
    "    \n",
    "    test_close(wgt1,wgt2,eps=1e-3)\n",
    "    test_close(bias1,bias2,eps=1e-3)\n",
    "finally: os.remove('tmp.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "m = nn.Linear(4,5)\n",
    "x = torch.randn(100, 3, 4)\n",
    "y = torch.randn(100, 3, 5)\n",
    "try:\n",
    "    torch.save(m.state_dict(), 'tmp.pth')\n",
    "    wgt,bias = m.weight.data.clone(),m.bias.data.clone()\n",
    "\n",
    "    m.load_state_dict(torch.load('tmp.pth'))\n",
    "    opt1 = torch.optim.AdamW(m.parameters(), betas=(0.9, 0.99), eps=1e-5, weight_decay=1e-2)\n",
    "    opt1 = OptimWrapper(opt=opt1)\n",
    "    _mock_train(m, x.clone(), y.clone(), opt1)\n",
    "    wgt1,bias1 = m.weight.data.clone(),m.bias.data.clone()\n",
    "\n",
    "    m.load_state_dict(torch.load('tmp.pth'))\n",
    "    opt2 = Adam(m.parameters(), 1e-3, wd=1e-2)\n",
    "    _mock_train(m, x.clone(), y.clone(), opt2)\n",
    "    wgt2,bias2 = m.weight.data.clone(),m.bias.data.clone()\n",
    "    \n",
    "    test_close(wgt1,wgt2,eps=1e-3)\n",
    "    test_close(bias1,bias2,eps=1e-3)\n",
    "finally: os.remove('tmp.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(4,5)\n",
    "x = torch.randn(100, 3, 4)\n",
    "y = torch.randn(100, 3, 5)\n",
    "try:\n",
    "    torch.save(m.state_dict(), 'tmp.pth')\n",
    "    wgt,bias = m.weight.data.clone(),m.bias.data.clone()\n",
    "\n",
    "    m.load_state_dict(torch.load('tmp.pth'))\n",
    "    opt1 = OptimWrapper(m.parameters(), torch.optim.Adam, betas=(0.9, 0.99), eps=1e-5, weight_decay=1e-2)\n",
    "    _mock_train(m, x.clone(), y.clone(), opt1)\n",
    "    wgt1,bias1 = m.weight.data.clone(),m.bias.data.clone()\n",
    "\n",
    "    m.load_state_dict(torch.load('tmp.pth'))\n",
    "    opt2 = Adam(m.parameters(), 1e-3, wd=1e-2, decouple_wd=False)\n",
    "    _mock_train(m, x.clone(), y.clone(), opt2)\n",
    "    wgt2,bias2 = m.weight.data.clone(),m.bias.data.clone()\n",
    "    \n",
    "    test_close(wgt1,wgt2,eps=1e-3)\n",
    "    test_close(bias1,bias2,eps=1e-3)\n",
    "finally: os.remove('tmp.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "m = nn.Linear(4,5)\n",
    "x = torch.randn(100, 3, 4)\n",
    "y = torch.randn(100, 3, 5)\n",
    "try:\n",
    "    torch.save(m.state_dict(), 'tmp.pth')\n",
    "    wgt,bias = m.weight.data.clone(),m.bias.data.clone()\n",
    "\n",
    "    m.load_state_dict(torch.load('tmp.pth'))\n",
    "    opt1 = torch.optim.Adam(m.parameters(), betas=(0.9, 0.99), eps=1e-5, weight_decay=1e-2)\n",
    "    opt1 = OptimWrapper(opt=opt1)\n",
    "    _mock_train(m, x.clone(), y.clone(), opt1)\n",
    "    wgt1,bias1 = m.weight.data.clone(),m.bias.data.clone()\n",
    "\n",
    "    m.load_state_dict(torch.load('tmp.pth'))\n",
    "    opt2 = Adam(m.parameters(), 1e-3, wd=1e-2, decouple_wd=False)\n",
    "    _mock_train(m, x.clone(), y.clone(), opt2)\n",
    "    wgt2,bias2 = m.weight.data.clone(),m.bias.data.clone()\n",
    "    \n",
    "    test_close(wgt1,wgt2,eps=1e-3)\n",
    "    test_close(bias1,bias2,eps=1e-3)\n",
    "finally: os.remove('tmp.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use an existing PyTorch optimizer, you can define an optimizer function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func = partial(OptimWrapper, opt=torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you already have an existing one, pass in only `opt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD([tensor([1,2,3])], lr=1e-2)\n",
    "opt_func = OptimWrapper(opt=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_torch_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 01a_losses.ipynb.\n",
      "Converted 02_data.load.ipynb.\n",
      "Converted 03_data.core.ipynb.\n",
      "Converted 04_data.external.ipynb.\n",
      "Converted 05_data.transforms.ipynb.\n",
      "Converted 06_data.block.ipynb.\n",
      "Converted 07_vision.core.ipynb.\n",
      "Converted 08_vision.data.ipynb.\n",
      "Converted 09_vision.augment.ipynb.\n",
      "Converted 09b_vision.utils.ipynb.\n",
      "Converted 09c_vision.widgets.ipynb.\n",
      "Converted 10_tutorial.pets.ipynb.\n",
      "Converted 10b_tutorial.albumentations.ipynb.\n",
      "Converted 11_vision.models.xresnet.ipynb.\n",
      "Converted 12_optimizer.ipynb.\n",
      "Converted 13_callback.core.ipynb.\n",
      "Converted 13a_learner.ipynb.\n",
      "Converted 13b_metrics.ipynb.\n",
      "Converted 14_callback.schedule.ipynb.\n",
      "Converted 14a_callback.data.ipynb.\n",
      "Converted 15_callback.hook.ipynb.\n",
      "Converted 15a_vision.models.unet.ipynb.\n",
      "Converted 16_callback.progress.ipynb.\n",
      "Converted 17_callback.tracker.ipynb.\n",
      "Converted 18_callback.fp16.ipynb.\n",
      "Converted 18a_callback.training.ipynb.\n",
      "Converted 18b_callback.preds.ipynb.\n",
      "Converted 19_callback.mixup.ipynb.\n",
      "Converted 20_interpret.ipynb.\n",
      "Converted 20a_distributed.ipynb.\n",
      "Converted 21_vision.learner.ipynb.\n",
      "Converted 22_tutorial.imagenette.ipynb.\n",
      "Converted 23_tutorial.vision.ipynb.\n",
      "Converted 24_tutorial.image_sequence.ipynb.\n",
      "Converted 24_tutorial.siamese.ipynb.\n",
      "Converted 24_vision.gan.ipynb.\n",
      "Converted 30_text.core.ipynb.\n",
      "Converted 31_text.data.ipynb.\n",
      "Converted 32_text.models.awdlstm.ipynb.\n",
      "Converted 33_text.models.core.ipynb.\n",
      "Converted 34_callback.rnn.ipynb.\n",
      "Converted 35_tutorial.wikitext.ipynb.\n",
      "Converted 37_text.learner.ipynb.\n",
      "Converted 38_tutorial.text.ipynb.\n",
      "Converted 39_tutorial.transformers.ipynb.\n",
      "Converted 40_tabular.core.ipynb.\n",
      "Converted 41_tabular.data.ipynb.\n",
      "Converted 42_tabular.model.ipynb.\n",
      "Converted 43_tabular.learner.ipynb.\n",
      "Converted 44_tutorial.tabular.ipynb.\n",
      "Converted 45_collab.ipynb.\n",
      "Converted 46_tutorial.collab.ipynb.\n",
      "Converted 50_tutorial.datablock.ipynb.\n",
      "Converted 60_medical.imaging.ipynb.\n",
      "Converted 61_tutorial.medical_imaging.ipynb.\n",
      "Converted 65_medical.text.ipynb.\n",
      "Converted 70_callback.wandb.ipynb.\n",
      "Converted 71_callback.tensorboard.ipynb.\n",
      "Converted 72_callback.neptune.ipynb.\n",
      "Converted 73_callback.captum.ipynb.\n",
      "Converted 74_huggingface.ipynb.\n",
      "Converted 97_test_utils.ipynb.\n",
      "Converted 99_pytorch_doc.ipynb.\n",
      "Converted dev-setup.ipynb.\n",
      "Converted app_examples.ipynb.\n",
      "Converted camvid.ipynb.\n",
      "Converted migrating_catalyst.ipynb.\n",
      "Converted migrating_ignite.ipynb.\n",
      "Converted migrating_lightning.ipynb.\n",
      "Converted migrating_pytorch.ipynb.\n",
      "Converted migrating_pytorch_verbose.ipynb.\n",
      "Converted ulmfit.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted quick_start.ipynb.\n",
      "Converted tutorial.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
