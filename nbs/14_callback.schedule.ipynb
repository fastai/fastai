{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp callback.schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparam schedule\n",
    "\n",
    "> Callback and helper functions to schedule any hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.test_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _Annealer:\n",
    "    def __init__(self, f, start, end): store_attr('f,start,end')\n",
    "    def __call__(self, pos): return self.f(self.start, self.end, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def annealer(f):\n",
    "    \"Decorator to make `f` return itself partially applied.\"\n",
    "    @functools.wraps(f)\n",
    "    def _inner(start, end): return _Annealer(f, start, end)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the decorator we will use for all of our scheduling functions, as it transforms a function taking `(start, end, pos)` to something taking `(start, end)` and return a function depending of `pos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO Jeremy, make this pickle\n",
    "#@annealer\n",
    "#def SchedLin(start, end, pos): return start + pos*(end-start)\n",
    "#@annealer\n",
    "#def SchedCos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\n",
    "#@annealer\n",
    "#def SchedNo (start, end, pos): return start\n",
    "#@annealer\n",
    "#def SchedExp(start, end, pos): return start * (end/start) ** pos\n",
    "#\n",
    "#SchedLin.__doc__ = \"Linear schedule function from `start` to `end`\"\n",
    "#SchedCos.__doc__ = \"Cosine schedule function from `start` to `end`\"\n",
    "#SchedNo .__doc__ = \"Constant schedule function with `start` value\"\n",
    "#SchedExp.__doc__ = \"Exponential schedule function from `start` to `end`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sched_lin(start, end, pos): return start + pos*(end-start)\n",
    "def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\n",
    "def sched_no (start, end, pos): return start\n",
    "def sched_exp(start, end, pos): return start * (end/start) ** pos\n",
    "\n",
    "def SchedLin(start, end): return _Annealer(sched_lin, start, end)\n",
    "def SchedCos(start, end): return _Annealer(sched_cos, start, end)\n",
    "def SchedNo (start, end): return _Annealer(sched_no,  start, end)\n",
    "def SchedExp(start, end): return _Annealer(sched_exp, start, end)\n",
    "\n",
    "SchedLin.__doc__ = \"Linear schedule function from `start` to `end`\"\n",
    "SchedCos.__doc__ = \"Cosine schedule function from `start` to `end`\"\n",
    "SchedNo .__doc__ = \"Constant schedule function with `start` value\"\n",
    "SchedExp.__doc__ = \"Exponential schedule function from `start` to `end`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "tst = pickle.dumps(SchedCos(0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annealings = \"NO LINEAR COS EXP\".split()\n",
    "p = torch.linspace(0.,1,100)\n",
    "fns = [SchedNo, SchedLin, SchedCos, SchedExp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SchedPoly(start, end, power):\n",
    "    \"Polynomial schedule (of `power`) function from `start` to `end`\"\n",
    "    def _inner(pos): return start + (end - start) * pos ** power\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn, t in zip(fns, annealings):\n",
    "    plt.plot(p, [fn(2, 1e-2)(o) for o in p], label=t)\n",
    "f = SchedPoly(2,1e-2,0.5)\n",
    "plt.plot(p, [f(o) for o in p], label=\"POLY(0.5)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SchedLin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = SchedLin(0, 2)\n",
    "test_eq(L(map(sched, [0., 0.25, 0.5, 0.75, 1.])), [0., 0.5, 1., 1.5, 2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SchedCos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = SchedCos(0, 2)\n",
    "test_close(L(map(sched, [0., 0.25, 0.5, 0.75, 1.])), [0., 0.29289, 1., 1.70711, 2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SchedNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = SchedNo(0, 2)\n",
    "test_close(L(map(sched, [0., 0.25, 0.5, 0.75, 1.])), [0., 0., 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SchedExp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = SchedExp(1, 2)\n",
    "test_close(L(map(sched, [0., 0.25, 0.5, 0.75, 1.])), [1., 1.18921, 1.41421, 1.68179, 2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SchedPoly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = SchedPoly(0, 2, 2)\n",
    "test_close(L(map(sched, [0., 0.25, 0.5, 0.75, 1.])), [0., 0.125, 0.5, 1.125, 2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.linspace(0.,1,100)\n",
    "\n",
    "pows = [0.5,1.,2.]\n",
    "for e in pows:\n",
    "    f = SchedPoly(2, 0, e)\n",
    "    plt.plot(p, [f(o) for o in p], label=f'power {e}')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def combine_scheds(pcts, scheds):\n",
    "    \"Combine `scheds` according to `pcts` in one function\"\n",
    "    assert sum(pcts) == 1.\n",
    "    pcts = tensor([0] + L(pcts))\n",
    "    assert torch.all(pcts >= 0)\n",
    "    pcts = torch.cumsum(pcts, 0)\n",
    "    def _inner(pos):\n",
    "        if int(pos) == 1: return scheds[-1](1.)\n",
    "        idx = (pos >= pcts).nonzero().max()\n",
    "        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])\n",
    "        return scheds[idx](actual_pos.item())\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pcts` must be a list of positive numbers that add up to 1 and is the same length as `scheds`. The generated function will use `scheds[0]` from 0 to `pcts[0]` then `scheds[1]` from `pcts[0]` to `pcts[0]+pcts[1]` and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.linspace(0.,1,100)\n",
    "f = combine_scheds([0.3,0.7], [SchedCos(0.3,0.6), SchedCos(0.6,0.2)])\n",
    "plt.plot(p, [f(o) for o in p]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.linspace(0.,1,100)\n",
    "f = combine_scheds([0.3,0.2,0.5], [SchedLin(0.,1.), SchedNo(1.,1.), SchedCos(1., 0.)])\n",
    "plt.plot(p, [f(o) for o in p]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_close([f(0.), f(0.15), f(0.3), f(0.4), f(0.5), f(0.7), f(1.)],\n",
    "           [0., 0.5, 1., 1., 1., 0.65451, 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def combined_cos(pct, start, middle, end):\n",
    "    \"Return a scheduler with cosine annealing from `start`→`middle` & `middle`→`end`\"\n",
    "    return combine_scheds([pct,1-pct], [SchedCos(start, middle), SchedCos(middle, end)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a useful helper function for the [1cycle policy](https://sgugger.github.io/the-1cycle-policy.html). `pct` is used for the `start` to `middle` part, `1-pct` for the `middle` to `end`. Handles floats or collection of floats. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = combined_cos(0.25,0.5,1.,0.)\n",
    "plt.plot(p, [f(o) for o in p]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_close([f(0.), f(0.1), f(0.25), f(0.5), f(1.)], [0.5, 0.67275, 1., 0.75, 0.])\n",
    "f = combined_cos(0.25, np.array([0.25,0.5]), np.array([0.5,1.]), np.array([0.,0.]))\n",
    "for a,b in zip([f(0.), f(0.1), f(0.25), f(0.5), f(1.)],\n",
    "               [[0.25,0.5], [0.33638,0.67275], [0.5,1.], [0.375,0.75], [0.,0.]]):\n",
    "    test_close(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ParamScheduler -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@docs\n",
    "class ParamScheduler(Callback):\n",
    "    \"Schedule hyper-parameters according to `scheds`\"\n",
    "    order,run_valid = 60,False\n",
    "\n",
    "    def __init__(self, scheds): self.scheds = scheds\n",
    "    def before_fit(self): self.hps = {p:[] for p in self.scheds.keys()}\n",
    "    def before_batch(self): self._update_val(self.pct_train)\n",
    "\n",
    "    def _update_val(self, pct):\n",
    "        for n,f in self.scheds.items(): self.opt.set_hyper(n, f(pct))\n",
    "\n",
    "    def after_batch(self):\n",
    "        for p in self.scheds.keys(): self.hps[p].append(self.opt.hypers[-1][p])\n",
    "\n",
    "    def after_fit(self):\n",
    "        if hasattr(self.learn, 'recorder') and hasattr(self, 'hps'): self.recorder.hps = self.hps\n",
    "\n",
    "    _docs = {\"before_fit\": \"Initialize container for hyper-parameters\",\n",
    "             \"before_batch\": \"Set the proper hyper-parameters in the optimizer\",\n",
    "             \"after_batch\": \"Record hyper-parameters of this batch\",\n",
    "             \"after_fit\": \"Save the hyper-parameters in the recorder if there is one\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scheds` is a dictionary with one key for each hyper-parameter you want to schedule, with either a scheduler or a list of schedulers as values (in the second case, the list must have the same length as the the number of parameters groups of the optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = synth_learner()\n",
    "sched = {'lr': SchedLin(1e-3, 1e-2)}\n",
    "learn.fit(1, cbs=ParamScheduler(sched))\n",
    "n = len(learn.dls.train)\n",
    "test_close(learn.recorder.hps['lr'], [1e-3 + (1e-2-1e-3) * i/n for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#test discriminative lrs\n",
    "def _splitter(m): return [[m.a], [m.b]]\n",
    "learn = synth_learner(splitter=_splitter)\n",
    "sched = {'lr': combined_cos(0.5, np.array([1e-4,1e-3]), np.array([1e-3,1e-2]), np.array([1e-5,1e-4]))}\n",
    "learn.fit(1, cbs=ParamScheduler(sched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ParamScheduler.before_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ParamScheduler.before_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ParamScheduler.after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ParamScheduler.after_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def fit_one_cycle(self:Learner, n_epoch, lr_max=None, div=25., div_final=1e5, pct_start=0.25, wd=None,\n",
    "                  moms=None, cbs=None, reset_opt=False):\n",
    "    \"Fit `self.model` for `n_epoch` using the 1cycle policy.\"\n",
    "    if self.opt is None: self.create_opt()\n",
    "    self.opt.set_hyper('lr', self.lr if lr_max is None else lr_max)\n",
    "    lr_max = np.array([h['lr'] for h in self.opt.hypers])\n",
    "    scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),\n",
    "              'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}\n",
    "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1cycle policy was introduced by Leslie N. Smith et al. in [Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120). It schedules the learning rate with a cosine annealing from `lr_max/div` to `lr_max` then `lr_max/div_final` (pass an array to `lr_max` if you want to use differential learning rates) and the momentum with cosine annealing according to the values in `moms`. The first phase takes `pct_start` of the training. You can optionally pass additional `cbs` and `reset_opt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integration test: training a few epochs should make the model better\n",
    "learn = synth_learner(lr=1e-2)\n",
    "xb,yb = learn.dls.one_batch()\n",
    "init_loss = learn.loss_func(learn.model(xb), yb)\n",
    "learn.fit_one_cycle(2)\n",
    "xb,yb = learn.dls.one_batch()\n",
    "final_loss = learn.loss_func(learn.model(xb), yb)\n",
    "assert final_loss < init_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scheduler test\n",
    "lrs,moms = learn.recorder.hps['lr'],learn.recorder.hps['mom']\n",
    "test_close(lrs,  [combined_cos(0.25,1e-2/25,1e-2,1e-7)(i/20) for i in range(20)])\n",
    "test_close(moms, [combined_cos(0.25,0.95,0.85,0.95)(i/20) for i in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def plot_sched(self:Recorder, keys=None, figsize=None):\n",
    "    keys = self.hps.keys() if keys is None else L(keys)\n",
    "    rows,cols = (len(keys)+1)//2, min(2, len(keys))\n",
    "    figsize = figsize or (6*cols,4*rows)\n",
    "    _, axs = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axs = axs.flatten() if len(keys) > 1 else L(axs)\n",
    "    for p,ax in zip(keys, axs):\n",
    "        ax.plot(self.hps[p])\n",
    "        ax.set_ylabel(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#test discriminative lrs\n",
    "def _splitter(m): return [[m.a], [m.b]]\n",
    "learn = synth_learner(splitter=_splitter)\n",
    "learn.fit_one_cycle(1, lr_max=slice(1e-3,1e-2))\n",
    "#n = len(learn.dls.train)\n",
    "#est_close(learn.recorder.hps['lr'], [1e-3 + (1e-2-1e-3) * i/n for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = synth_learner()\n",
    "learn.fit_one_cycle(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_sched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def fit_flat_cos(self:Learner, n_epoch, lr=None, div_final=1e5, pct_start=0.75, wd=None,\n",
    "                 cbs=None, reset_opt=False):\n",
    "    \"Fit `self.model` for `n_epoch` at flat `lr` before a cosine annealing.\"\n",
    "    if self.opt is None: self.create_opt()\n",
    "    self.opt.set_hyper('lr', self.lr if lr is None else lr)\n",
    "    lr = np.array([h['lr'] for h in self.opt.hypers])\n",
    "    scheds = {'lr': combined_cos(pct_start, lr, lr, lr/div_final)}\n",
    "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = synth_learner()\n",
    "learn.fit_flat_cos(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_sched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def fit_sgdr(self:Learner, n_cycles, cycle_len, lr_max=None, cycle_mult=2, cbs=None, reset_opt=False, wd=None):\n",
    "    \"Fit `self.model` for `n_cycles` of `cycle_len` using SGDR.\"\n",
    "    if self.opt is None: self.create_opt()\n",
    "    self.opt.set_hyper('lr', self.lr if lr_max is None else lr_max)\n",
    "    lr_max = np.array([h['lr'] for h in self.opt.hypers])\n",
    "    n_epoch = cycle_len * (cycle_mult**n_cycles-1)//(cycle_mult-1)\n",
    "    pcts = [cycle_len * cycle_mult**i / n_epoch for i in range(n_cycles)]\n",
    "    scheds = [SchedCos(lr_max, 0) for _ in range(n_cycles)]\n",
    "    scheds = {'lr': combine_scheds(pcts, scheds)}\n",
    "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This schedule was introduced by Ilya Loshchilov et al. in [SGDR: Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/abs/1608.03983). It consists of `n_cycles` that are cosine annealings from `lr_max` (defaults to the `Learner` lr) to 0, with a length of `cycle_len * cycle_mult**i` for the `i`-th cycle (first one is `cycle_len`-long, then we multiply the length by `cycle_mult` at each epoch). You can optionally pass additional `cbs` and `reset_opt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "learn = synth_learner()\n",
    "with learn.no_logging(): learn.fit_sgdr(3, 1)\n",
    "test_eq(learn.n_epoch, 7)\n",
    "iters = [k * len(learn.dls.train) for k in [0,1,3,7]]\n",
    "for i in range(3):\n",
    "    n = iters[i+1]-iters[i]\n",
    "    #The start of a cycle can be mixed with the 0 of the previous cycle with rounding errors, so we test at +1\n",
    "    test_close(learn.recorder.lrs[iters[i]+1:iters[i+1]], [SchedCos(learn.lr, 0)(k/n) for k in range(1,n)])\n",
    "\n",
    "learn.recorder.plot_sched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "@delegates(Learner.fit_one_cycle)\n",
    "def fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n",
    "              pct_start=0.3, div=5.0, **kwargs):\n",
    "    \"Fine tune with `freeze` for `freeze_epochs` then with `unfreeze` from `epochs` using discriminative LR\"\n",
    "    self.freeze()\n",
    "    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n",
    "    base_lr /= 2\n",
    "    self.unfreeze()\n",
    "    self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRFind -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@docs\n",
    "class LRFinder(ParamScheduler):\n",
    "    \"Training with exponentially growing learning rate\"\n",
    "    def __init__(self, start_lr=1e-7, end_lr=10, num_it=100, stop_div=True):\n",
    "        if is_listy(start_lr):\n",
    "            self.scheds = {'lr': [SchedExp(s, e) for (s,e) in zip(start_lr,end_lr)]}\n",
    "        else: self.scheds = {'lr': SchedExp(start_lr, end_lr)}\n",
    "        self.num_it,self.stop_div = num_it,stop_div\n",
    "\n",
    "    def before_fit(self):\n",
    "        super().before_fit()\n",
    "        self.learn.save('_tmp')\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def before_batch(self):\n",
    "        self._update_val(self.train_iter/self.num_it)\n",
    "\n",
    "    def after_batch(self):\n",
    "        super().after_batch()\n",
    "        if self.smooth_loss < self.best_loss: self.best_loss = self.smooth_loss\n",
    "        if self.smooth_loss > 4*self.best_loss and self.stop_div: raise CancelFitException()\n",
    "        if self.train_iter >= self.num_it: raise CancelFitException()\n",
    "\n",
    "    def before_validate(self): raise CancelValidException()\n",
    "\n",
    "    def after_fit(self):\n",
    "        self.learn.opt.zero_grad() #Need to zero the gradients of the model before detaching the optimizer for future fits\n",
    "        tmp_f = self.path/self.model_dir/'_tmp.pth'\n",
    "        if tmp_f.exists():\n",
    "            self.learn.load('_tmp', with_opt=True)\n",
    "            os.remove(tmp_f)\n",
    "\n",
    "    _docs = {\"before_fit\": \"Initialize container for hyper-parameters and save the model\",\n",
    "             \"before_batch\": \"Set the proper hyper-parameters in the optimizer\",\n",
    "             \"after_batch\": \"Record hyper-parameters of this batch and potentially stop training\",\n",
    "             \"after_fit\": \"Save the hyper-parameters in the recorder if there is one and load the original model\",\n",
    "             \"before_validate\": \"Skip the validation part of training\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "set_seed(99, True)\n",
    "path = untar_data(URLs.PETS)/'images'\n",
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, get_image_files(path), valid_pct=0.2,\n",
    "    label_func=lambda x: x[0].isupper(), item_tfms=Resize(224))\n",
    "\n",
    "learn = cnn_learner(dls, resnet18)\n",
    "learn.fit(1)\n",
    "learn.opt.state_dict()['state'][1]['grad_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.opt.state_dict()['state'][1]['grad_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.opt.state_dict()['state'][1]['grad_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    learn = synth_learner(path=Path(d))\n",
    "    init_a,init_b = learn.model.a,learn.model.b\n",
    "    with learn.no_logging(): learn.fit(20, cbs=LRFinder(num_it=100))\n",
    "    assert len(learn.recorder.lrs) <= 100\n",
    "    test_eq(len(learn.recorder.lrs), len(learn.recorder.losses))\n",
    "    #Check stop if diverge\n",
    "    if len(learn.recorder.lrs) < 100: assert learn.recorder.losses[-1] > 4 * min(learn.recorder.losses)\n",
    "    #Test schedule\n",
    "    test_eq(learn.recorder.lrs, [SchedExp(1e-7, 10)(i/100) for i in range_of(learn.recorder.lrs)])\n",
    "    #No validation data\n",
    "    test_eq([len(v) for v in learn.recorder.values], [1 for _ in range_of(learn.recorder.values)])\n",
    "    #Model loaded back properly\n",
    "    test_eq(learn.model.a, init_a)\n",
    "    test_eq(learn.model.b, init_b)\n",
    "    test_eq(learn.opt.state_dict()['state'], [{}, {}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(LRFinder.before_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(LRFinder.before_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(LRFinder.after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(LRFinder.before_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def plot_lr_find(self:Recorder, skip_end=5):\n",
    "    \"Plot the result of an LR Finder test (won't work if you didn't do `learn.lr_find()` before)\"\n",
    "    lrs    = self.lrs    if skip_end==0 else self.lrs   [:-skip_end]\n",
    "    losses = self.losses if skip_end==0 else self.losses[:-skip_end]\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.plot(lrs, losses)\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_xlabel(\"Learning Rate\")\n",
    "    ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "SuggestedLRs = collections.namedtuple('SuggestedLRs', ['lr_min', 'lr_steep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def lr_find(self:Learner, start_lr=1e-7, end_lr=10, num_it=100, stop_div=True, show_plot=True, suggestions=True):\n",
    "    \"Launch a mock training to find a good learning rate, return lr_min, lr_steep if `suggestions` is True\"\n",
    "    n_epoch = num_it//len(self.dls.train) + 1\n",
    "    cb=LRFinder(start_lr=start_lr, end_lr=end_lr, num_it=num_it, stop_div=stop_div)\n",
    "    with self.no_logging(): self.fit(n_epoch, cbs=cb)\n",
    "    if show_plot: self.recorder.plot_lr_find()\n",
    "    if suggestions:\n",
    "        lrs,losses = tensor(self.recorder.lrs[num_it//10:-5]),tensor(self.recorder.losses[num_it//10:-5])\n",
    "        if len(losses) == 0: return\n",
    "        lr_min = lrs[losses.argmin()].item()\n",
    "        grads = (losses[1:]-losses[:-1]) / (lrs[1:].log()-lrs[:-1].log())\n",
    "        lr_steep = lrs[grads.argmin()].item()\n",
    "        return SuggestedLRs(lr_min/10.,lr_steep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First introduced by Leslie N. Smith in [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/pdf/1506.01186.pdf), the LR Finder trains the model with exponentially growing learning rates from `start_lr` to `end_lr` for `num_it` and stops in case of divergence (unless `stop_div=False`) then plots the losses vs the learning rates with a log scale. \n",
    "\n",
    "A good value for the learning rates is then either:\n",
    "- one tenth of the minimum before the divergence\n",
    "- when the slope is the steepest\n",
    "\n",
    "Those two values are returned by default by the Learning Rate Finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    learn = synth_learner(path=Path(d))\n",
    "    weights_pre_lr_find = L(learn.model.parameters())\n",
    "    lr_min,lr_steep = learn.lr_find()\n",
    "    weights_post_lr_find = L(learn.model.parameters())\n",
    "test_eq(weights_pre_lr_find, weights_post_lr_find)\n",
    "print(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
