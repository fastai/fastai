{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.torch_basics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External data\n",
    "> Helper functions to download the fastai datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 3, 'b': 2}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge({'a':1, 'b': 2}, {'a':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Config:\n",
    "    config_path = Path(os.getenv('FASTAI_HOME', '~/.fastai')).expanduser()\n",
    "    config_file = config_path/'config.yml'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.config_path.mkdir(parents=True, exist_ok=True)\n",
    "        if not self.config_file.exists(): self.create_config()\n",
    "        self.d = self.load_config()\n",
    "\n",
    "    def __getitem__(self,k):\n",
    "        k = k.lower()\n",
    "        if k not in self.d: k = k+'_path'\n",
    "        return Path(self.d[k])\n",
    "\n",
    "    def __getattr__(self,k):\n",
    "        if k=='d': raise AttributeError\n",
    "        return self[k]\n",
    "\n",
    "    def __setitem__(self,k,v): self.d[k] = str(v)\n",
    "    def __contains__(self,k): return k in self.d\n",
    "\n",
    "    def load_config(self):\n",
    "        with open(self.config_file, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            if 'version' in config and config['version'] == 2: return config\n",
    "            elif 'version' in config: self.create_config(config)\n",
    "            else: self.create_config()\n",
    "        return self.load_config()\n",
    "\n",
    "    def create_config(self, cfg=None):\n",
    "        config = {'data_path':    str(self.config_path/'data'),\n",
    "                  'archive_path': str(self.config_path/'archive'),\n",
    "                  'storage_path': str(self.config_path/'data'),\n",
    "                  'model_path':   str(self.config_path/'models'),\n",
    "                  'version':      2}\n",
    "        if cfg is not None: \n",
    "            cfg['version'] = 2\n",
    "            config = merge(config, cfg)\n",
    "        self.save_file(config)\n",
    "\n",
    "    def save(self): self.save_file(self.d)\n",
    "    def save_file(self, config):\n",
    "        with self.config_file.open('w') as f: yaml.dump(config, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "config_path = config.config_path\n",
    "config_file,config_bak = config_path/'config.yml',config_path/'config.yml.bak'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is just to make the config file compatible with current fastai\n",
    "# TODO: make this a method that auto-runs as needed\n",
    "if 'data_archive_path' not in config:\n",
    "    config['data_archive_path'] = config.data_path\n",
    "    config.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_file.exists(): shutil.move(config_file, config_bak)\n",
    "#Test default config\n",
    "config = Config()\n",
    "assert config_file.exists()\n",
    "test_eq(config.archive, config_path/'archive')\n",
    "\n",
    "#Test change in config\n",
    "config['archive_path'] = '.'\n",
    "config.save()\n",
    "config = Config()\n",
    "test_eq(config.archive, Path('.'))\n",
    "\n",
    "if config_bak.exists(): shutil.move(config_bak, config_file)\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class URLs():\n",
    "    \"Global constants for dataset and model URLs.\"\n",
    "    LOCAL_PATH = Path.cwd()\n",
    "    URL = 'http://files.fast.ai/data/examples/'\n",
    "    MDL = 'http://files.fast.ai/models/'\n",
    "    S3 = 'https://s3.amazonaws.com/fast-ai-'\n",
    "\n",
    "    S3_IMAGE    = f'{S3}imageclas/'\n",
    "    S3_IMAGELOC = f'{S3}imagelocal/'\n",
    "    S3_NLP      = f'{S3}nlp/'\n",
    "    S3_COCO     = f'{S3}coco/'\n",
    "    S3_MODEL    = f'{S3}modelzoo/'\n",
    "\n",
    "    # main datasets\n",
    "    ADULT_SAMPLE        = f'{URL}adult_sample.tgz'\n",
    "    BIWI_SAMPLE         = f'{URL}biwi_sample.tgz'\n",
    "    CIFAR               = f'{URL}cifar10.tgz'\n",
    "    COCO_SAMPLE         = f'{S3_COCO}coco_sample.tgz'\n",
    "    COCO_TINY           = f'{URL}coco_tiny.tgz'\n",
    "    HUMAN_NUMBERS       = f'{URL}human_numbers.tgz'\n",
    "    IMDB                = f'{S3_NLP}imdb.tgz'\n",
    "    IMDB_SAMPLE         = f'{URL}imdb_sample.tgz'\n",
    "    ML_SAMPLE           = f'{URL}movie_lens_sample.tgz'\n",
    "    ML_100k             = 'http://files.grouplens.org/datasets/movielens/ml-100k.zip'\n",
    "    MNIST_SAMPLE        = f'{URL}mnist_sample.tgz'\n",
    "    MNIST_TINY          = f'{URL}mnist_tiny.tgz'\n",
    "    MNIST_VAR_SIZE_TINY = f'{S3_IMAGE}mnist_var_size_tiny.tgz'\n",
    "    PLANET_SAMPLE       = f'{URL}planet_sample.tgz'\n",
    "    PLANET_TINY         = f'{URL}planet_tiny.tgz'\n",
    "    IMAGENETTE          = f'{S3_IMAGE}imagenette2.tgz'\n",
    "    IMAGENETTE_160      = f'{S3_IMAGE}imagenette2-160.tgz'\n",
    "    IMAGENETTE_320      = f'{S3_IMAGE}imagenette2-320.tgz'\n",
    "    IMAGEWOOF           = f'{S3_IMAGE}imagewoof2.tgz'\n",
    "    IMAGEWOOF_160       = f'{S3_IMAGE}imagewoof2-160.tgz'\n",
    "    IMAGEWOOF_320       = f'{S3_IMAGE}imagewoof2-320.tgz'\n",
    "    IMAGEWANG           = f'{S3_IMAGE}imagewang.tgz'\n",
    "    IMAGEWANG_160       = f'{S3_IMAGE}imagewang-160.tgz'\n",
    "    IMAGEWANG_320       = f'{S3_IMAGE}imagewang-320.tgz'\n",
    "\n",
    "    # kaggle competitions download dogs-vs-cats -p {DOGS.absolute()}\n",
    "    DOGS = f'{URL}dogscats.tgz'\n",
    "\n",
    "    # image classification datasets\n",
    "    CALTECH_101  = f'{S3_IMAGE}caltech_101.tgz'\n",
    "    CARS         = f'{S3_IMAGE}stanford-cars.tgz'\n",
    "    CIFAR_100    = f'{S3_IMAGE}cifar100.tgz'\n",
    "    CUB_200_2011 = f'{S3_IMAGE}CUB_200_2011.tgz'\n",
    "    FLOWERS      = f'{S3_IMAGE}oxford-102-flowers.tgz'\n",
    "    FOOD         = f'{S3_IMAGE}food-101.tgz'\n",
    "    MNIST        = f'{S3_IMAGE}mnist_png.tgz'\n",
    "    PETS         = f'{S3_IMAGE}oxford-iiit-pet.tgz'\n",
    "\n",
    "    # NLP datasets\n",
    "    AG_NEWS                 = f'{S3_NLP}ag_news_csv.tgz'\n",
    "    AMAZON_REVIEWS          = f'{S3_NLP}amazon_review_full_csv.tgz'\n",
    "    AMAZON_REVIEWS_POLARITY = f'{S3_NLP}amazon_review_polarity_csv.tgz'\n",
    "    DBPEDIA                 = f'{S3_NLP}dbpedia_csv.tgz'\n",
    "    MT_ENG_FRA              = f'{S3_NLP}giga-fren.tgz'\n",
    "    SOGOU_NEWS              = f'{S3_NLP}sogou_news_csv.tgz'\n",
    "    WIKITEXT                = f'{S3_NLP}wikitext-103.tgz'\n",
    "    WIKITEXT_TINY           = f'{S3_NLP}wikitext-2.tgz'\n",
    "    YAHOO_ANSWERS           = f'{S3_NLP}yahoo_answers_csv.tgz'\n",
    "    YELP_REVIEWS            = f'{S3_NLP}yelp_review_full_csv.tgz'\n",
    "    YELP_REVIEWS_POLARITY   = f'{S3_NLP}yelp_review_polarity_csv.tgz'\n",
    "\n",
    "    # Image localization datasets\n",
    "    BIWI_HEAD_POSE     = f\"{S3_IMAGELOC}biwi_head_pose.tgz\"\n",
    "    CAMVID             = f'{S3_IMAGELOC}camvid.tgz'\n",
    "    CAMVID_TINY        = f'{URL}camvid_tiny.tgz'\n",
    "    LSUN_BEDROOMS      = f'{S3_IMAGE}bedroom.tgz'\n",
    "    PASCAL_2007        = f'{S3_IMAGELOC}pascal_2007.tgz'\n",
    "    PASCAL_2012        = f'{S3_IMAGELOC}pascal_2012.tgz'\n",
    "    \n",
    "    # Medical Imaging datasets\n",
    "    SKIN_LESION        = f'{S3_IMAGELOC}skin_lesion.tgz'\n",
    "    SIIM_SMALL         = f'{S3_IMAGELOC}siim_small.tgz'\n",
    "\n",
    "    #Pretrained models\n",
    "    OPENAI_TRANSFORMER = f'{S3_MODEL}transformer.tgz'\n",
    "    WT103_FWD          = f'{S3_MODEL}wt103-fwd.tgz'\n",
    "    WT103_BWD          = f'{S3_MODEL}wt103-bwd.tgz'\n",
    "\n",
    "    def path(url='.', c_key='archive'):\n",
    "        fname = url.split('/')[-1]\n",
    "        local_path = URLs.LOCAL_PATH/('models' if c_key=='models' else 'data')/fname\n",
    "        if local_path.exists(): return local_path\n",
    "        return Config()[c_key]/fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "path,path_bak = URLs.LOCAL_PATH/'data',URLs.LOCAL_PATH/'data1'\n",
    "if path.exists(): shutil.move(path, path_bak)\n",
    "try:\n",
    "    test_eq(URLs.path(URLs.MNIST_TINY), config.archive/'mnist_tiny.tgz')\n",
    "    test_eq(URLs.path(URLs.MNIST_TINY.replace('tgz', 'tar')), config.archive/'mnist_tiny.tar')\n",
    "    test_eq(URLs.path(URLs.MNIST_TINY,c_key='model'), config.model/'mnist_tiny.tgz')\n",
    "finally:\n",
    "    if path_bak.exists(): shutil.move(path_bak, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def download_url(url, dest, overwrite=False, pbar=None, show_progress=True, chunk_size=1024*1024,\n",
    "                 timeout=4, retries=5):\n",
    "    \"Download `url` to `dest` unless it exists and not `overwrite`\"\n",
    "    if os.path.exists(dest) and not overwrite: return\n",
    "\n",
    "    s = requests.Session()\n",
    "    s.mount('http://',requests.adapters.HTTPAdapter(max_retries=retries))\n",
    "    # additional line to identify as a firefox browser, see fastai/#2438\n",
    "    s.headers.update({'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0'})\n",
    "    u = s.get(url, stream=True, timeout=timeout)\n",
    "    try: file_size = int(u.headers[\"Content-Length\"])\n",
    "    except: show_progress = False\n",
    "\n",
    "    with open(dest, 'wb') as f:\n",
    "        nbytes = 0\n",
    "        if show_progress: pbar = progress_bar(range(file_size), leave=False, parent=pbar)\n",
    "        try:\n",
    "            if show_progress: pbar.update(0)\n",
    "            for chunk in u.iter_content(chunk_size=chunk_size):\n",
    "                nbytes += len(chunk)\n",
    "                if show_progress: pbar.update(nbytes)\n",
    "                f.write(chunk)\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            fname = url.split('/')[-1]\n",
    "            data_dir = dest.parent\n",
    "            print(f'\\n Download of {url} has failed after {retries} retries\\n'\n",
    "                  f' Fix the download manually:\\n'\n",
    "                  f'$ mkdir -p {data_dir}\\n'\n",
    "                  f'$ cd {data_dir}\\n'\n",
    "                  f'$ wget -c {url}\\n'\n",
    "                  f'$ tar xf {fname}\\n'\n",
    "                  f' And re-run your code once the download is successful\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url, fname = 'http://files.fast.ai/data/examples/mnist_tiny.tgz', Path('mnist_tiny.tgz')\n",
    "try:\n",
    "    download_url(url, fname)\n",
    "    assert fname.exists()\n",
    "    t = os.path.getmtime(fname)\n",
    "    #Launching the function again doesn't trigger a download since the file is already there.\n",
    "    download_url(url, fname)\n",
    "    test_eq(t, os.path.getmtime(fname))\n",
    "    #But with the overwrite option, we download it again.\n",
    "    download_url(url, fname, overwrite=True)\n",
    "    test_ne(t, os.path.getmtime(fname))\n",
    "finally: fname.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    download_url(f\"{URLs.MNIST_TINY}.tgz\", 'data/mnist_tiny.tgz')\n",
    "    test_eq(URLs.path(URLs.MNIST_TINY), Path.cwd()/'data'/'mnist_tiny.tgz')\n",
    "finally:\n",
    "    shutil.rmtree('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def download_data(url, fname=None, c_key='archive', force_download=False):\n",
    "    \"Download `url` to `fname`.\"\n",
    "    fname = Path(fname or URLs.path(url, c_key=c_key))\n",
    "    fname.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not fname.exists() or force_download: download_url(url, fname, overwrite=force_download)\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `fname` is None, it will default to the archive folder you have in your config file (or data, model if you specify a different `c_key`) followed by the last part of the url: for instance `URLs.MNIST_SAMPLE` is `http://files.fast.ai/data/examples/mnist_sample.tgz` and the default value for `fname` will be `~/.fastai/archive/mnist_sample.tgz`.\n",
    "\n",
    "If `force_download=True`, the file is alwayd downloaded. Otherwise, it's only when the file doesn't exists that the download is triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    test_eq(download_data(URLs.MNIST_SAMPLE), config.archive/'mnist_sample.tgz')\n",
    "    test_eq(download_data(URLs.MNIST_TINY, fname=Path('mnist.tgz')), Path('mnist.tgz'))\n",
    "finally: Path('mnist.tgz').unlink()\n",
    "\n",
    "try:\n",
    "    tst_model = config.model/'mnist_tiny.tgz'\n",
    "    test_eq(download_data(URLs.MNIST_TINY, c_key='model'), tst_model)\n",
    "    os.remove(tst_model)\n",
    "finally:\n",
    "    if tst_model.exists(): tst_model.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check datasets -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.imports import Config as NbdevConfig\n",
    "__file__ = NbdevConfig().lib_path/'data'/'external.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_check(url):\n",
    "    checks = json.load(open(Path(__file__).parent/'checks.txt', 'r'))\n",
    "    return checks.get(url, None)\n",
    "\n",
    "def _check_file(fname):\n",
    "    size = os.path.getsize(fname)\n",
    "    with open(fname, \"rb\") as f: hash_nb = hashlib.md5(f.read(2**20)).hexdigest()\n",
    "    return [size,hash_nb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(_get_check(URLs.MNIST_SAMPLE), _check_file(URLs.path(URLs.MNIST_SAMPLE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _add_check(url, fname):\n",
    "    \"Internal function to update the internal check file with `url` and check on `fname`.\"\n",
    "    checks = json.load(open(Path(__file__).parent/'checks.txt', 'r'))\n",
    "    checks[url] = _check_file(fname)\n",
    "    json.dump(checks, open(Path(__file__).parent/'checks.txt', 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def file_extract(fname, dest=None):\n",
    "    \"Extract `fname` to `dest` using `tarfile` or `zipfile\"\n",
    "    if dest is None: dest = Path(fname).parent\n",
    "    fname = str(fname)\n",
    "    if   fname.endswith('gz'):  tarfile.open(fname, 'r:gz').extractall(dest)\n",
    "    elif fname.endswith('zip'): zipfile.ZipFile(fname     ).extractall(dest)\n",
    "    else: raise Exception(f'Unrecognized archive: {fname}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`file_extract` is used by default in `untar_data` to decompress the downloaded file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _try_from_storage(dest, storage):\n",
    "    if not storage.exists(): return\n",
    "    os.makedirs(dest, exist_ok=True)\n",
    "    for f in storage.glob('*'): os.symlink(f, dest/f.name, target_is_directory=f.is_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as d:\n",
    "    d = Path(d)\n",
    "    for k in ['a', 'b', 'c']: os.makedirs(d/k)\n",
    "    for k in ['d', 'e', 'f']: (d/k).touch()\n",
    "    dest = d.parent/'tmp'\n",
    "    if dest.exists(): shutil.rmtree(dest)\n",
    "    _try_from_storage(dest, d)\n",
    "    assert dest.exists()\n",
    "    for k in ['a', 'b', 'c']: \n",
    "        assert (dest/k).exists()\n",
    "        assert (dest/k).is_dir()\n",
    "    for k in ['d', 'e', 'f']: \n",
    "        assert (dest/k).exists()\n",
    "        assert (dest/k).is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def untar_data(url, fname=None, dest=None, c_key='data', force_download=False, extract_func=file_extract):\n",
    "    \"Download `url` to `fname` if `dest` doesn't exist, and un-tgz to folder `dest`.\"\n",
    "    default_dest = URLs.path(url, c_key=c_key).with_suffix('')\n",
    "    dest = default_dest if dest is None else Path(dest)/default_dest.name\n",
    "    fname = Path(fname or URLs.path(url))\n",
    "    if fname.exists() and _get_check(url) and _check_file(fname) != _get_check(url):\n",
    "        print(\"A new version of this dataset is available, downloading...\")\n",
    "        force_download = True\n",
    "    if force_download:\n",
    "        if fname.exists(): os.remove(fname)\n",
    "        if dest.exists(): shutil.rmtree(dest)\n",
    "    if not dest.exists(): _try_from_storage(dest, URLs.path(url, c_key='storage').with_suffix(''))\n",
    "    if not dest.exists():\n",
    "        fname = download_data(url, fname=fname, c_key=c_key)\n",
    "        if _get_check(url) and _check_file(fname) != _get_check(url):\n",
    "            print(f\"File downloaded is broken. Remove {fname} and try again.\")\n",
    "        extract_func(fname, dest.parent)\n",
    "    return dest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`untar_data` is a convenience function for the fastai datasets, intended to work with the urls in `URLs`. You can use it with another url only if it ends with `.tgz` (otherwise the function can download it but not decompress it). For other extensions, you should use `download_data` then the necessary decompress function.\n",
    "\n",
    "If `fname` is specified, the data will be downloaded to that destination, otherwise it will default to the archive path in your config file (default `~/.fastai/archive/`) followed by the last part of your url. For instance `URLs.MNIST_SAMPLE` is `http://files.fast.ai/data/examples/mnist_sample.tgz` and the default value for `fname` will be `~/.fastai/archive/mnist_sample.tgz`.\n",
    "\n",
    "If `dest` is specified, the data will be decompressed to that folder. Otherwise, it will default to the data path (or model/archive if you specify a different `c_key`) in your config file (default `~/.fastai/data/`) followed by the last part of your url without extension. For instance `URLs.MNIST_SAMPLE` is `http://files.fast.ai/data/examples/mnist_sample.tgz` and the default value for `dest` will be `~/.fastai/data/mnist_sample`.\n",
    "\n",
    "`force_download=True` will retrigger a download, otherwise the behavior is to:\n",
    "- not do anything when `dest` exists\n",
    "- otherwise decompress `fname` to `dest` if `fname` exists\n",
    "- otherwise download then decompress `fname` to `dest`\n",
    "\n",
    "You can pass any function that takes `fname` and `dest` arguments as `extract_func`. By default, `file_extract` is used, which extracts the file using `tarfile` or `zipfile`, based on the extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_eq(untar_data(URLs.MNIST_SAMPLE), config.data/'mnist_sample')\n",
    "\n",
    "#Test specific fname\n",
    "untar_data(URLs.MNIST_TINY, fname='mnist_tiny.tgz', force_download=True)\n",
    "p = Path('mnist_tiny.tgz')\n",
    "assert p.exists()\n",
    "p.unlink()\n",
    "    \n",
    "#Test specific dest\n",
    "test_eq(untar_data(URLs.MNIST_TINY, dest='.'), Path('mnist_tiny'))\n",
    "assert Path('mnist_tiny').exists()\n",
    "shutil.rmtree(Path('mnist_tiny'))\n",
    "\n",
    "#Test c_key\n",
    "tst_model = config.model/'mnist_sample'\n",
    "test_eq(untar_data(URLs.MNIST_SAMPLE, c_key='model'), tst_model)\n",
    "assert not tst_model.with_suffix('.tgz').exists() #Archive wasn't downloaded in the models path\n",
    "assert (config.archive/'mnist_sample.tgz').exists() #Archive was downloaded there\n",
    "shutil.rmtree(tst_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Check all URLs are in the checks.txt file and match for downloaded archives\n",
    "_whitelist = \"MDL LOCAL_PATH URL WT103_BWD WT103_FWD\".split()\n",
    "checks = json.load(open(Path(__file__).parent/'checks.txt', 'r'))\n",
    "for d in dir(URLs): \n",
    "    if d.upper() == d and not d.startswith(\"S3\") and not d in _whitelist: \n",
    "        url = getattr(URLs, d)\n",
    "        assert url in checks,f\"\"\"{d} is not in the check file for all URLs.\n",
    "To fix this, you need to run the following code in this notebook before making a PR (there is a commented cell for this below):\n",
    "url = URLs.{d}\n",
    "untar_data(url, force_download=True)\n",
    "_add_check(url, URLs.path(url))\n",
    "\"\"\"\n",
    "        f = URLs.path(url)\n",
    "        if f.exists():\n",
    "            assert checks[url] == _check_file(f),f\"\"\"The log we have for {d} in checks does not match the actual archive.\n",
    "To fix this, you need to run the following code in this notebook before making a PR (there is a commented cell for this below):\n",
    "url = URLs.{d}\n",
    "_add_check(url, URLs.path(url))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# url = URLs.IMAGEWANG_160\n",
    "# untar_data(url, force_download=True)\n",
    "# _add_check(url, URLs.path(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#url = URLs.PASCAL_2012\n",
    "#_add_check(url, URLs.path(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_torch_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_data.load.ipynb.\n",
      "Converted 03_data.core.ipynb.\n",
      "Converted 04_data.external.ipynb.\n",
      "Converted 05_data.transforms.ipynb.\n",
      "Converted 06_data.block.ipynb.\n",
      "Converted 07_vision.core.ipynb.\n",
      "Converted 08_vision.data.ipynb.\n",
      "Converted 09_vision.augment.ipynb.\n",
      "Converted 09b_vision.utils.ipynb.\n",
      "Converted 09c_vision.widgets.ipynb.\n",
      "Converted 10_tutorial.pets.ipynb.\n",
      "Converted 11_vision.models.xresnet.ipynb.\n",
      "Converted 12_optimizer.ipynb.\n",
      "Converted 13_callback.core.ipynb.\n",
      "Converted 13a_learner.ipynb.\n",
      "Converted 13b_metrics.ipynb.\n",
      "Converted 14_callback.schedule.ipynb.\n",
      "Converted 14a_callback.data.ipynb.\n",
      "Converted 15_callback.hook.ipynb.\n",
      "Converted 15a_vision.models.unet.ipynb.\n",
      "Converted 16_callback.progress.ipynb.\n",
      "Converted 17_callback.tracker.ipynb.\n",
      "Converted 18_callback.fp16.ipynb.\n",
      "Converted 19_callback.mixup.ipynb.\n",
      "Converted 20_interpret.ipynb.\n",
      "Converted 20a_distributed.ipynb.\n",
      "Converted 21_vision.learner.ipynb.\n",
      "Converted 22_tutorial.imagenette.ipynb.\n",
      "Converted 23_tutorial.vision.ipynb.\n",
      "Converted 24_tutorial.siamese.ipynb.\n",
      "Converted 30_text.core.ipynb.\n",
      "Converted 31_text.data.ipynb.\n",
      "Converted 32_text.models.awdlstm.ipynb.\n",
      "Converted 33_text.models.core.ipynb.\n",
      "Converted 34_callback.rnn.ipynb.\n",
      "Converted 35_tutorial.wikitext.ipynb.\n",
      "Converted 36_text.models.qrnn.ipynb.\n",
      "Converted 37_text.learner.ipynb.\n",
      "Converted 38_tutorial.text.ipynb.\n",
      "Converted 40_tabular.core.ipynb.\n",
      "Converted 41_tabular.data.ipynb.\n",
      "Converted 42_tabular.model.ipynb.\n",
      "Converted 43_tabular.learner.ipynb.\n",
      "Converted 44_tutorial.tabular.ipynb.\n",
      "Converted 45_collab.ipynb.\n",
      "Converted 50_tutorial.datablock.ipynb.\n",
      "Converted 60_medical.imaging.ipynb.\n",
      "Converted 61_tutorial.medical_imaging.ipynb.\n",
      "Converted 65_medical.text.ipynb.\n",
      "Converted 70_callback.wandb.ipynb.\n",
      "Converted 71_callback.tensorboard.ipynb.\n",
      "Converted 72_callback.neptune.ipynb.\n",
      "Converted 97_test_utils.ipynb.\n",
      "Converted 99_pytorch_doc.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted tutorial.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
