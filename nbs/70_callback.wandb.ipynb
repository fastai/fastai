{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.basics import *\n",
    "from fastai2.callback.progress import *\n",
    "from fastai2.text.data import TensorText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp callback.wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb\n",
    "\n",
    "> Integration with [wandb](https://www.wandb.com/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing first, you need to install wandb with\n",
    "```\n",
    "pip install wandb\n",
    "```\n",
    "Create a free account then run \n",
    "``` \n",
    "wandb login\n",
    "```\n",
    "in your terminal. Follow the link to get an API token that you will need to paste, then you're all set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class WandbCallback(Callback):\n",
    "    \"Saves model topology, losses & metrics\"\n",
    "    toward_end,remove_on_fetch,run_after = True,True,FetchPreds\n",
    "    # Record if watch has been called previously (even in another instance)\n",
    "    _wandb_watch_called = False\n",
    "\n",
    "    def __init__(self, log=\"gradients\", log_preds=True, valid_dl=None, n_preds=36, seed=12345):\n",
    "        # W&B log step (number of training updates)\n",
    "        self._wandb_step = 0\n",
    "        self._wandb_epoch = 0\n",
    "        # Check if wandb.init has been called\n",
    "        if wandb.run is None:\n",
    "            raise ValueError('You must call wandb.init() before WandbCallback()')\n",
    "        store_attr(self, 'log,log_preds,valid_dl,n_preds,seed')\n",
    "\n",
    "    def begin_fit(self):\n",
    "        \"Call watch method to log model topology, gradients & weights\"\n",
    "        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\") and rank_distrib()==0\n",
    "        if not self.run: return\n",
    "        if not WandbCallback._wandb_watch_called:\n",
    "            WandbCallback._wandb_watch_called = True\n",
    "            # Logs model topology and optionally gradients and weights\n",
    "            wandb.watch(self.learn.model, log=self.log)\n",
    "\n",
    "        if hasattr(self, 'save_model'): self.save_model.add_save = Path(wandb.run.dir)/'bestmodel.pth'\n",
    "\n",
    "        if self.log_preds and not self.valid_dl:\n",
    "            #Initializes the batch watched\n",
    "            wandbRandom = random.Random(self.seed)  # For repeatability\n",
    "            self.n_preds = min(self.n_preds, len(self.dls.valid_ds))\n",
    "            idxs = wandbRandom.sample(range(len(self.dls.valid_ds)), self.n_preds)\n",
    "            test_items = [self.dls.valid_ds.items[i] for i in idxs]\n",
    "            self.valid_dl = self.dls.test_dl(test_items, with_labels=True)\n",
    "        \n",
    "        if self.valid_dl:\n",
    "            self.learn.add_cb(FetchPreds(dl=self.valid_dl, with_input=True, with_decoded=True))            \n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Log hyper-parameters and training loss\"\n",
    "        if self.training:\n",
    "            self._wandb_step += 1\n",
    "            self._wandb_epoch += 1/self.n_iter\n",
    "            hypers = {f'{k}_{i}':v for i,h in enumerate(self.opt.hypers) for k,v in h.items()}\n",
    "            wandb.log({'epoch': self._wandb_epoch,'train_loss': self.smooth_loss, **hypers}, step=self._wandb_step)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Log validation loss and custom metrics & log prediction samples\"\n",
    "        # Correct any epoch rounding error and overwrite value\n",
    "        self._wandb_epoch = round(self._wandb_epoch)\n",
    "        wandb.log({'epoch': self._wandb_epoch}, step=self._wandb_step)\n",
    "        # Log sample predictions\n",
    "        if self.log_preds:\n",
    "            inp,preds,targs,out = self.learn.fetch_preds.preds\n",
    "            b = tuplify(inp) + tuplify(targs)\n",
    "            x,y,its,outs = self.valid_dl.show_results(b, out, show=False, max_n=self.n_preds)\n",
    "            wandb.log(wandb_process(x, y, its, outs), step=self._wandb_step)\n",
    "        wandb.log({n:s for n,s in zip(self.recorder.metric_names, self.recorder.log) if n not in ['train_loss', 'epoch', 'time']}, step=self._wandb_step)\n",
    "\n",
    "    def after_fit(self):\n",
    "        self.run = True\n",
    "        if self.log_preds: self.remove_cb(self.learn.fetch_preds)\n",
    "        wandb.log({}) #To trigger one last synch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally logs weights and or gradients depending on `log` (can be \"gradients\", \"parameters\", \"all\" or None), sample predictions if ` log_preds=True` that will come from `valid_dl` or a random sample pf the validation set (determined by `seed`). `n_preds` are logged in this case.\n",
    "\n",
    "If used in combination with `SaveModelCallback`, the best model is saved as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of use:\n",
    "\n",
    "Once your have defined your `Learner`, before you call to `fit` or `fit_one_cycle`, you need to initialize wandb:\n",
    "```\n",
    "import wandb\n",
    "wandb.init(project=PROJECT_NAME, entity=USER_NAME)\n",
    "```\n",
    "(replace `PROJECT_NAME` and `USER_NAME`). Then you add the callback in your call to fit, potentially with `SaveModelCallback` if you want to save the best model:\n",
    "```\n",
    "from fastai2.callback.wandb import *\n",
    "\n",
    "# To log only during one training phase\n",
    "learn.fit(..., cbs=WandbCallback())\n",
    "\n",
    "# To log continuously for all training phases\n",
    "learn = learner(..., cbs=WandbCallback())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def wandb_process(x:TensorImage, y, samples, outs):\n",
    "    \"Process `sample` and `out` depending on the type of `x/y`\"\n",
    "    res = []\n",
    "    for s,o in zip(samples, outs):\n",
    "        img = s[0].permute(1,2,0)\n",
    "        res.append(wandb.Image(img, caption='Input data', grouping=3))\n",
    "        for t, capt in ((o[0], \"Prediction\"), (s[1], \"Ground Truth\")):\n",
    "            # Resize plot to image resolution (from https://stackoverflow.com/a/13714915)\n",
    "            my_dpi = 100\n",
    "            fig = plt.figure(frameon=False, dpi=my_dpi)\n",
    "            h, w = img.shape[:2]\n",
    "            fig.set_size_inches(w / my_dpi, h / my_dpi)\n",
    "            ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "            ax.set_axis_off()\n",
    "            fig.add_axes(ax)\n",
    "            # Superimpose label or prediction to input image\n",
    "            ax = img.show(ctx=ax)\n",
    "            ax = t.show(ctx=ax)\n",
    "            res.append(wandb.Image(fig, caption=capt))\n",
    "            plt.close(fig)\n",
    "    return {\"Prediction Samples\": res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def wandb_process(x:TensorImage, y:(TensorCategory,TensorMultiCategory), samples, outs):\n",
    "    return {\"Prediction Samples\": [wandb.Image(s[0].permute(1,2,0), caption=f'Ground Truth: {s[1]}\\nPrediction: {o[0]}')\n",
    "            for s,o in zip(samples,outs)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def wandb_process(x:TensorText, y:(TensorCategory,TensorMultiCategory), samples, outs):\n",
    "    data = [[s[0], s[1], o[0]] for s,o in zip(samples,outs)]\n",
    "    return {\"Prediction Samples\": wandb.Table(data=data, columns=[\"Text\", \"Target\", \"Prediction\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            Using <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> in dryrun mode. Not logging results to the cloud.<br/>\n",
       "            Call wandb.login() to authenticate this machine.<br/>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "wandb: Wandb version 0.8.29 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.755290</td>\n",
       "      <td>0.347419</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from fastai2.vision.all import *\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "items = get_image_files(path)\n",
    "tds = Datasets(items, [PILImageBW.create, [parent_label, Categorize()]], splits=GrandparentSplitter()(items))\n",
    "dls = tds.dataloaders(after_item=[ToTensor(), IntToFloatTensor()])\n",
    "\n",
    "os.environ['WANDB_MODE'] = 'dryrun' # run offline\n",
    "wandb.init(anonymous='allow')\n",
    "learn = cnn_learner(dls, resnet18, loss_func=CrossEntropyLossFlat(), cbs=WandbCallback())\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_all_ = ['wandb_process']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
