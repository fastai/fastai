{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [[ -e /content ]] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.imports import *\n",
    "from fastai.data.all import *\n",
    "from fastai.optimizer import *\n",
    "from fastai.learner import *\n",
    "from fastai.callback.core import *\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp test_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Learner\n",
    "\n",
    "> For quick testing of the training loop and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def synth_dbunch(a=2, b=3, bs=16, n_train=10, n_valid=2, cuda=False):\n",
    "    def get_data(n):\n",
    "        x = torch.randn(bs*n, 1)\n",
    "        return TensorDataset(x, a*x + b + 0.1*torch.randn(bs*n, 1))\n",
    "    train_ds = get_data(n_train)\n",
    "    valid_ds = get_data(n_valid)\n",
    "    device = default_device() if cuda else None\n",
    "    train_dl = TfmdDL(train_ds, bs=bs, shuffle=True, num_workers=0)\n",
    "    valid_dl = TfmdDL(valid_ds, bs=bs, num_workers=0)\n",
    "    return DataLoaders(train_dl, valid_dl, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RegModel(Module):\n",
    "    def __init__(self): self.a,self.b = nn.Parameter(torch.randn(1)),nn.Parameter(torch.randn(1))\n",
    "    def forward(self, x): return x*self.a + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(Learner.__init__)\n",
    "def synth_learner(n_trn=10, n_val=2, cuda=False, lr=1e-3, data=None, model=None, **kwargs):\n",
    "    if data is None: data=synth_dbunch(n_train=n_trn,n_valid=n_val, cuda=cuda)\n",
    "    if model is None: model=RegModel()\n",
    "    return Learner(data, model, lr=lr, loss_func=MSELossFlat(),\n",
    "                   opt_func=partial(SGD, mom=0.9), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VerboseCallback(Callback):\n",
    "    \"Callback that prints the name of each event called\"\n",
    "    def __call__(self, event_name):\n",
    "        print(event_name)\n",
    "        super().__call__(event_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_env(name):\n",
    "    \"Return env var value if it's defined and not an empty string, or return Unknown\"\n",
    "    res = os.environ.get(name,'')\n",
    "    return res if len(res) else \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def try_import(module):\n",
    "    \"Try to import `module`. Returns module's object on success, None on failure\"\n",
    "    try: return importlib.import_module(module)\n",
    "    except: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_install(show_nvidia_smi:bool=False):\n",
    "    \"Print user's setup information\"\n",
    "\n",
    "    import platform, fastprogress\n",
    "\n",
    "    rep = []\n",
    "    opt_mods = []\n",
    "\n",
    "    rep.append([\"=== Software ===\", None])\n",
    "    rep.append([\"python\", platform.python_version()])\n",
    "    rep.append([\"fastai\", fastai.__version__])\n",
    "    rep.append([\"fastprogress\", fastprogress.__version__])\n",
    "    rep.append([\"torch\",  torch.__version__])\n",
    "\n",
    "    # nvidia-smi\n",
    "    cmd = \"nvidia-smi\"\n",
    "    have_nvidia_smi = False\n",
    "    try: result = subprocess.run(cmd.split(), shell=False, check=False, stdout=subprocess.PIPE)\n",
    "    except: pass\n",
    "    else:\n",
    "        if result.returncode == 0 and result.stdout: have_nvidia_smi = True\n",
    "\n",
    "    # XXX: if nvidia-smi is not available, another check could be:\n",
    "    # /proc/driver/nvidia/version on most systems, since it's the\n",
    "    # currently active version\n",
    "\n",
    "    if have_nvidia_smi:\n",
    "        smi = result.stdout.decode('utf-8')\n",
    "        # matching: \"Driver Version: 396.44\"\n",
    "        match = re.findall(r'Driver Version: +(\\d+\\.\\d+)', smi)\n",
    "        if match: rep.append([\"nvidia driver\", match[0]])\n",
    "\n",
    "    available = \"available\" if torch.cuda.is_available() else \"**Not available** \"\n",
    "    rep.append([\"torch cuda\", f\"{torch.version.cuda} / is {available}\"])\n",
    "\n",
    "    # no point reporting on cudnn if cuda is not available, as it\n",
    "    # seems to be enabled at times even on cpu-only setups\n",
    "    if torch.cuda.is_available():\n",
    "        enabled = \"enabled\" if torch.backends.cudnn.enabled else \"**Not enabled** \"\n",
    "        rep.append([\"torch cudnn\", f\"{torch.backends.cudnn.version()} / is {enabled}\"])\n",
    "\n",
    "    rep.append([\"\\n=== Hardware ===\", None])\n",
    "\n",
    "    # it's possible that torch might not see what nvidia-smi sees?\n",
    "    gpu_total_mem = []\n",
    "    nvidia_gpu_cnt = 0\n",
    "    if have_nvidia_smi:\n",
    "        try:\n",
    "            cmd = \"nvidia-smi --query-gpu=memory.total --format=csv,nounits,noheader\"\n",
    "            result = subprocess.run(cmd.split(), shell=False, check=False, stdout=subprocess.PIPE)\n",
    "        except:\n",
    "            print(\"have nvidia-smi, but failed to query it\")\n",
    "        else:\n",
    "            if result.returncode == 0 and result.stdout:\n",
    "                output = result.stdout.decode('utf-8')\n",
    "                gpu_total_mem = [int(x) for x in output.strip().split('\\n')]\n",
    "                nvidia_gpu_cnt = len(gpu_total_mem)\n",
    "\n",
    "\n",
    "    if nvidia_gpu_cnt: rep.append([\"nvidia gpus\", nvidia_gpu_cnt])\n",
    "\n",
    "    torch_gpu_cnt = torch.cuda.device_count()\n",
    "    if torch_gpu_cnt:\n",
    "        rep.append([\"torch devices\", torch_gpu_cnt])\n",
    "        # information for each gpu\n",
    "        for i in range(torch_gpu_cnt):\n",
    "            rep.append([f\"  - gpu{i}\", (f\"{gpu_total_mem[i]}MB | \" if gpu_total_mem else \"\") + torch.cuda.get_device_name(i)])\n",
    "    else:\n",
    "        if nvidia_gpu_cnt:\n",
    "            rep.append([f\"Have {nvidia_gpu_cnt} GPU(s), but torch can't use them (check nvidia driver)\", None])\n",
    "        else:\n",
    "            rep.append([f\"No GPUs available\", None])\n",
    "\n",
    "\n",
    "    rep.append([\"\\n=== Environment ===\", None])\n",
    "\n",
    "    rep.append([\"platform\", platform.platform()])\n",
    "\n",
    "    if platform.system() == 'Linux':\n",
    "        distro = try_import('distro')\n",
    "        if distro:\n",
    "            # full distro info\n",
    "            rep.append([\"distro\", ' '.join(distro.linux_distribution())])\n",
    "        else:\n",
    "            opt_mods.append('distro');\n",
    "            # partial distro info\n",
    "            rep.append([\"distro\", platform.uname().version])\n",
    "\n",
    "    rep.append([\"conda env\", get_env('CONDA_DEFAULT_ENV')])\n",
    "    rep.append([\"python\", sys.executable])\n",
    "    rep.append([\"sys.path\", \"\\n\".join(sys.path)])\n",
    "\n",
    "    print(\"\\n\\n```text\")\n",
    "\n",
    "    keylen = max([len(e[0]) for e in rep if e[1] is not None])\n",
    "    for e in rep:\n",
    "        print(f\"{e[0]:{keylen}}\", (f\": {e[1]}\" if e[1] is not None else \"\"))\n",
    "\n",
    "    if have_nvidia_smi:\n",
    "        if show_nvidia_smi: print(f\"\\n{smi}\")\n",
    "    else:\n",
    "        if torch_gpu_cnt: print(\"no nvidia-smi is found\")\n",
    "        else: print(\"no supported gpus found on this system\")\n",
    "\n",
    "    print(\"```\\n\")\n",
    "\n",
    "    print(\"Please make sure to include opening/closing ``` when you paste into forums/github to make the reports appear formatted as code sections.\\n\")\n",
    "\n",
    "    if opt_mods:\n",
    "        print(\"Optional package(s) to enhance the diagnostics can be installed with:\")\n",
    "        print(f\"pip install {' '.join(opt_mods)}\")\n",
    "        print(\"Once installed, re-run this utility to get the additional information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```text\n",
      "=== Software === \n",
      "python       : 3.8.5\n",
      "fastai       : 2.0.9\n",
      "fastprogress : 0.2.7\n",
      "torch        : 1.6.0\n",
      "torch cuda   : 10.2 / is **Not available** \n",
      "\n",
      "=== Hardware === \n",
      "No GPUs available \n",
      "\n",
      "=== Environment === \n",
      "platform     : Linux-5.4.0-7642-generic-x86_64-with-glibc2.10\n",
      "distro       : #46~1598628707~20.04~040157c-Ubuntu SMP Fri Aug 28 18:02:16 UTC \n",
      "conda env    : fastai\n",
      "python       : /home/tcapelle/miniconda3/envs/fastai/bin/python\n",
      "sys.path     : /home/tcapelle/Apps/fastai/nbs\n",
      "/home/tcapelle/miniconda3/envs/fastai/lib/python38.zip\n",
      "/home/tcapelle/miniconda3/envs/fastai/lib/python3.8\n",
      "/home/tcapelle/miniconda3/envs/fastai/lib/python3.8/lib-dynload\n",
      "\n",
      "/home/tcapelle/miniconda3/envs/fastai/lib/python3.8/site-packages\n",
      "/home/tcapelle/Apps/fastai\n",
      "/home/tcapelle/Apps/fastcore\n",
      "/home/tcapelle/Apps/nbdev\n",
      "/home/tcapelle/SteadySun/nbdev_image_lib\n",
      "/home/tcapelle/miniconda3/envs/fastai/lib/python3.8/site-packages/IPython/extensions\n",
      "/home/tcapelle/.ipython\n",
      "no supported gpus found on this system\n",
      "```\n",
      "\n",
      "Please make sure to include opening/closing ``` when you paste into forums/github to make the reports appear formatted as code sections.\n",
      "\n",
      "Optional package(s) to enhance the diagnostics can be installed with:\n",
      "pip install distro\n",
      "Once installed, re-run this utility to get the additional information\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "show_install()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
