{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.data.all import *\n",
    "from fastai.optimizer import *\n",
    "from fastai.learner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp metrics\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "> Definition of the metrics that can be used in training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sklearn.metrics as skm\n",
    "import scipy.stats as scs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fastai metrics are classes which inherit from `Metric` and run on `Learner` via the `Recorder` callback. \n",
    "\n",
    "There are three main metric types: `AvgMetric`, `AccumMetric`, and `AvgSmoothMetric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"Metric\" class=\"doc_header\"><code>class</code> <code>Metric</code><a href=\"https://github.com/fastai/fastai/tree/master/fastai/learner.py#L409\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>Metric</code>(**`dim_argmax`**=*`None`*, **`activation`**=*`'no'`*, **`thresh`**=*`None`*, **`to_np`**=*`False`*, **`invert_arg`**=*`False`*, **`log_metric`**=*`None`*, **`name`**=*`None`*)\n",
       "\n",
       "Blueprint for defining a metric with accumulate"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Metric, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"AvgMetric\" class=\"doc_header\"><code>class</code> <code>AvgMetric</code><a href=\"https://github.com/fastai/fastai/tree/master/fastai/learner.py#L457\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>AvgMetric</code>(**`func`**, **`dim_argmax`**=*`None`*, **`activation`**=*`'no'`*, **`thresh`**=*`None`*, **`to_np`**=*`False`*, **`invert_arg`**=*`False`*, **`log_metric`**=*`None`*, **`name`**=*`None`*) :: [`Metric`](/learner.html#Metric)\n",
       "\n",
       "Average the values of `func` taking into account potential different batch sizes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(AvgMetric, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"AccumMetric\" class=\"doc_header\"><code>class</code> <code>AccumMetric</code><a href=\"https://github.com/fastai/fastai/tree/master/fastai/learner.py#L482\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>AccumMetric</code>(**`func`**, **`flatten`**=*`True`*, **`dim_argmax`**=*`None`*, **`activation`**=*`'no'`*, **`thresh`**=*`None`*, **`to_np`**=*`False`*, **`invert_arg`**=*`False`*, **`log_metric`**=*`None`*, **`name`**=*`None`*) :: [`Metric`](/learner.html#Metric)\n",
       "\n",
       "Stores predictions and targets on CPU in accumulate to perform final calculations with `func`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(AccumMetric, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"AvgSmoothMetric\" class=\"doc_header\"><code>class</code> <code>AvgSmoothMetric</code><a href=\"https://github.com/fastai/fastai/tree/master/fastai/learner.py#L526\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>AvgSmoothMetric</code>(**`func`**, **`beta`**=*`0.98`*, **`to_np`**=*`False`*, **`invert_arg`**=*`False`*, **`dim_argmax`**=*`None`*, **`activation`**=*`'no'`*, **`thresh`**=*`None`*, **`name`**=*`None`*) :: [`Metric`](/learner.html#Metric)\n",
       "\n",
       "Smooth average the values of `func` (exponentially weighted with `beta`). Only logs training set."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(AvgSmoothMetric, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AvgMetric`, `AccumMetric`, and `AvgSmoothMetric` all require `func`, which is a funcational implementation of the metric. The signature of `func` should be `inp,targ` (where `inp` are the predictions of the model and `targ` the corresponding labels).\n",
    "\n",
    "Fastai metrics can be logged during training, validation, or both by setting the `log_metric` argument to `LogMetric.Train`, `LogMetric.Valid`, or `LogMetric.Both`. The sole exception is `AvgSmoothMetric` which only computes during training.\n",
    "\n",
    "`AvgMetric`, `AccumMetric`, and `AvgSmoothMetric` will automatically recognize and pass any `func`'s unique arguments to `func`.\n",
    "\n",
    "> Important: Some metrics, like Root Mean Squared Error, will have incorrect results if passed to `AvgMetric` via `MetricType.Avg`, as the mean of multiple batches of RMSE isn't equal to the RMSE of the whole dataset. For these metrics use `AccumMetric`.\n",
    "\n",
    "An example of creating a metric from a functional implementation in the fastai style can be seen below:\n",
    "\n",
    "```python\n",
    "def example_accuracy(inp, targ):\n",
    "    return (inp == targ).float().mean()\n",
    "\n",
    "def ExampleAccuracy(dim_argmax=-1, log_metric=LogMetric.Valid, **kwargs):\n",
    "    return AvgMetric(example_accuracy, dim_argmax=dim_argmax, log_metric=log_metric, **kwargs)\n",
    "```\n",
    "\n",
    "Alternatively, you could use the `func_to_metric` convenience method to create the metric:\n",
    "\n",
    "```python\n",
    "def ExampleAccuracy(axis=-1, log_metric=LogMetric.Valid, **kwargs):\n",
    "    return func_to_metric(example_accuracy, MetricType.Avg, True, axis=axis, log_metric=log_metric, **kwargs)\n",
    "```\n",
    "\n",
    "It is also possible to inherit directly from `Metric` to create a fastai metric.\n",
    "\n",
    "> Note: If your custom <code>Metric</code> has state depending on tensors, don't forget to store it on the CPU to avoid any potential memory leaks.\n",
    "\n",
    "```python\n",
    "class ExampleAccuracy(Metric):\n",
    "    def __init__(self, dim_argmax=-1, log_metric=LogMetric.Valid, **kwargs):\n",
    "    super().__init__(dim_argmax=dim_argmax, log_metric=log_metric, **kwargs)\n",
    "\n",
    "    def reset(self): self.preds,self.targs = [],[]\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        super().accumulate(learn)\n",
    "        self.preds.append(learn.to_detach(self.pred))\n",
    "        self.targs.append(learn.to_detach(self.targ))\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if len(self.preds) == 0: return\n",
    "        preds,targs = torch.cat(self.preds),torch.cat(self.targs)\n",
    "        return (preds == targs).float().mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Metric\n",
    "\n",
    "To use the example accuracy metric while training, or any fastai metrics detailed below, you would create a `Learner` like normal (or task specific learner such as `cnn_learner`, `text_classifier_learner`, etc.) and add the metric(s) to the `metrics` argument:\n",
    "\n",
    "```python\n",
    "Learner(..., metrics=ExampleAccuracy())\n",
    "```\n",
    "\n",
    "Fastai metrics can be logged during training, validation, or both by setting the `log_metric` argument to `LogMetric.Train`, `LogMetric.Valid`, or `LogMetric.Both`. The sole exception is `AvgSmoothMetric` which only logs during training.\n",
    "\n",
    "If you wanted the metric to log during training you could pass `LogMetric.Train` to `log_metric`:\n",
    "\n",
    "```python\n",
    "Learner(..., metrics=ExampleAccuracy(log_metric=LogMetric.Train))\n",
    "```\n",
    "\n",
    "Fastai metrics also support custom names via the `name` argument:\n",
    "\n",
    "```python\n",
    "Learner(..., metrics=ExampleAccuracy(name='real_metric'))\n",
    "```\n",
    "\n",
    "which will result in ExampleAccuracy logging under \"real_metric\" instead of the default \"example_accuracy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Metrics Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Metric`, and classes which inherit from `Metric` such as `AvgMetric`, `AccumMetric`, and `AvgSmoothMetric`, have optional helper functionility in `Metric.accumulate` to assist in developing metrics.\n",
    "\n",
    "For classification problems with single label, predictions need to be transformed with a softmax then an argmax before being compared to the targets. Since a softmax doesn't change the order of the numbers, we can just apply the argmax. Pass along `dim_argmax` to have this done by `Metric` (usually -1 will work pretty well). If you need to pass to your metrics the probabilities and not the predictions, use `softmax=True`.\n",
    "\n",
    "For classification problems with multiple labels, or if your targets are one-hot encoded, predictions may need to pass through a sigmoid (if it wasn't included in your model) then be compared to a given threshold (to decide between 0 and 1), this is done by `Metric` if you pass `sigmoid=True` and/or a value for `thresh`.\n",
    "\n",
    "`AvgMetric`, `AccumMetric`, and `AvgSmoothMetric` have two additional arguments to assist in creating metrics: `to_np` and `invert_arg`.\n",
    "\n",
    "For example, if you want to use a metric function sklearn.metrics, you will need to convert predictions and labels to numpy arrays with `to_np=True`. Also, scikit-learn metrics adopt the convention `y_true`, `y_preds` which is the opposite from us, so you will need to pass `invert_arg=True` to make `AvgMetric`, `AccumMetric`, and `AvgSmoothMetric` do the inversion for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#For testing: a fake learner and a metric that isn't an average\n",
    "@delegates()\n",
    "class TstLearner(Learner):\n",
    "    def __init__(self,dls=None,model=None,**kwargs): self.pred,self.xb,self.yb = None,None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def _l2_mean(x,y): return torch.sqrt((x.float()-y.float()).pow(2).mean())\n",
    "\n",
    "#Go through a fake cycle with various batch sizes and computes the value of met\n",
    "def compute_val(met, x1, x2):\n",
    "    met.reset()\n",
    "    vals = [0,6,15,20]\n",
    "    learn = TstLearner()\n",
    "    for i in range(3):\n",
    "        learn.pred,learn.yb = x1[vals[i]:vals[i+1]],(x2[vals[i]:vals[i+1]],)\n",
    "        met.accumulate(learn)\n",
    "    return met.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def _l2_mean(x,y): return torch.sqrt((x.argmax(dim=-1).float()-y.float()).pow(2).mean())\n",
    "x1,x2 = torch.randn(20,5),torch.randint(0, 5, (20,))\n",
    "tst = AccumMetric(_l2_mean, dim_argmax=-1, flatten=False, activation=ActivationType.Softmax)\n",
    "test_close(compute_val(tst, x1, x2), _l2_mean(F.softmax(x1, dim=-1), x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(Metric)\n",
    "def func_to_metric(func, metric_type, is_class, thresh=None, axis=-1, activation=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Convert `func` metric to a fastai metric\"\n",
    "\n",
    "    dim_argmax = axis if is_class and thresh is None else None\n",
    "    if activation is None:\n",
    "        activation = ActivationType.Sigmoid if (is_class and thresh is not None) else ActivationType.No\n",
    "\n",
    "    if metric_type==MetricType.Accum:\n",
    "        return AccumMetric(func, dim_argmax=dim_argmax, activation=activation, \n",
    "                           thresh=thresh, log_metric=log_metric, **kwargs)\n",
    "    elif metric_type==MetricType.Avg:\n",
    "        return AvgMetric(func, dim_argmax=dim_argmax, activation=activation, \n",
    "                         thresh=thresh, log_metric=log_metric, **kwargs)\n",
    "    elif metric_type==MetricType.Smooth:\n",
    "        if log_metric!=LogMetric.Train: \n",
    "            name = func.func.__name__ if hasattr(func, 'func') else  func.__name__\n",
    "            raise ValueError(f'Error with {name}: AvgSmoothMetric can only run on train. Set `log_metric` to LogMetric.Train.')\n",
    "        return AvgSmoothMetric(func, dim_argmax=dim_argmax, activation=activation, thresh=thresh, **kwargs)\n",
    "    else:\n",
    "        name = func.func.__name__ if hasattr(func, 'func') else  func.__name__\n",
    "        raise ValueError(f\"Unsupported `metric_type` {metric_type} for metric {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the quickest way to use a functional metric in a fastai training loop.\n",
    "\n",
    "`metric_type` is one of `MetricType.Avg`, `MetricType.Accum`, or `MetricType.Smooth` which set the metric to use `AvgMetric`, `AccumMetric`, or `AvgSmoothMetric`, respectively. \n",
    "\n",
    "`is_class` indicates if you are in a classification problem or not. In this case:\n",
    "- leaving `thresh` to `None` indicates it's a single-label classification problem and predictions will pass through an argmax over `axis` before being compared to the targets\n",
    "- setting a value for `thresh` indicates it's a multi-label classification problem and predictions will pass through a sigmoid (can be deactivated with `sigmoid=False`) and be compared to `thresh` before being compared to the targets\n",
    "\n",
    "If `is_class=False`, it indicates you are in a regression problem, and predictions are compared to the targets without being modified. In all cases, `kwargs` are extra keyword arguments passed to `func`.\n",
    "\n",
    "> Important: Some metrics, like Root Mean Squared Error, will have incorrect results if passed to `AvgMetric` via `MetricType.Avg`, as the mean of multiple batches of RMSE isn't equal to the RMSE of the whole dataset. For these metrics use `AccumMetric` by setting `metric_type` to `MetricType.Accum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(Metric)\n",
    "def skm_to_fastai(func, is_class=True, thresh=None, axis=-1, activation=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Convert `func` from sklearn.metrics to a fastai metric\"\n",
    "    return func_to_metric(func, MetricType.Accum, is_class, thresh, axis, activation, \n",
    "                          log_metric, to_np=True, invert_arg=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the quickest way to use a scikit-learn metric in a fastai training loop. It is the same as `func_to_metric` except it defaults to using `AccumMetric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_single = skm_to_fastai(skm.precision_score)\n",
    "x1,x2 = torch.randn(20,2),torch.randint(0, 2, (20,))\n",
    "test_close(compute_val(tst_single, x1, x2), skm.precision_score(x2, x1.argmax(dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_multi = skm_to_fastai(skm.precision_score, thresh=0.2)\n",
    "x1,x2 = torch.randn(20),torch.randint(0, 2, (20,))\n",
    "test_close(compute_val(tst_multi, x1, x2), skm.precision_score(x2, torch.sigmoid(x1) >= 0.2))\n",
    "\n",
    "tst_multi = skm_to_fastai(skm.precision_score, thresh=0.2, activation=ActivationType.No)\n",
    "x1,x2 = torch.randn(20),torch.randint(0, 2, (20,))\n",
    "test_close(compute_val(tst_multi, x1, x2), skm.precision_score(x2, x1 >= 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_reg = skm_to_fastai(skm.r2_score, is_class=False)\n",
    "x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "test_close(compute_val(tst_reg, x1, x2), skm.r2_score(x2.view(-1), x1.view(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(tst_reg(x1, x2), skm.r2_score(x2.view(-1), x1.view(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def optim_metric(f, argname, bounds, tol=0.01, do_neg=True, get_x=False):\n",
    "    \"Replace metric `f` with a version that optimizes argument `argname`\"\n",
    "    def _f(preds, targs):\n",
    "        def minfunc(x):\n",
    "            kwargs = {argname:x}\n",
    "            res = f(preds, targs, **kwargs)\n",
    "            return -res if do_neg else res\n",
    "        optres = scipy.optimize.minimize_scalar(minfunc, bounds=bounds, method='bounded',\n",
    "                                                options={'xatol':0.01})\n",
    "        fun = -optres.fun if do_neg else optres.fun\n",
    "        return (fun,optres.x) if get_x else fun\n",
    "    _f.__name__ = f'opt_{f.__name__}'\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: All functions defined in this section are intended for single-label classification and targets that are not one-hot encoded. For multi-label problems or one-hot encoded targets, use the version suffixed with multi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: Many metrics in fastai are thin wrappers around sklearn functionality. However, sklearn metrics can handle python list strings, amongst other things, whereas fastai metrics work with PyTorch, and thus require tensors. The arguments that are passed to metrics are after all transformations, such as categories being converted to indices, have occurred. This means that when you pass a label of a metric, for instance, that you must pass indices, not strings. This can be converted with `vocab.map_obj`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def accuracy(inp, targ):\n",
    "    \"Compute accuracy with `targ` when `pred` is bs * n_classes\"\n",
    "    pred,targ = flatten_check(inp, targ)\n",
    "    return (pred == targ).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Accuracy(axis=-1, metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Compute accuracy with `targ` when `pred` is bs * n_classes\"\n",
    "    return func_to_metric(accuracy, metric_type, True, axis=axis, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing\n",
    "def change_targ(targ, n, c):\n",
    "    idx = torch.randperm(len(targ))[:n]\n",
    "    res = targ.clone()\n",
    "    for i in idx: res[i] = (res[i]+random.randint(1,c-1))%c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing\n",
    "def compute_single(met, x1, x2):\n",
    "    met.reset()\n",
    "    learn = TstLearner()\n",
    "    learn.pred,learn.yb = x1,(x2,)\n",
    "    met.accumulate(learn)\n",
    "    return met.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4,5)\n",
    "y = x.argmax(dim=1)\n",
    "test_eq(compute_single(Accuracy(), x, y), 1)\n",
    "y1 = change_targ(y, 2, 5)\n",
    "test_eq(compute_single(Accuracy(),x,y1), 0.5)\n",
    "test_eq(compute_single(Accuracy(),x.unsqueeze(1).expand(4,2,5), torch.stack([y,y1], dim=1)), 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def error_rate(inp, targ):\n",
    "    \"1 - `accuracy`\"\n",
    "    return 1 - accuracy(inp, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ErrorRate(axis=-1, metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Compute 1 - accuracy with `targ` when `pred` is bs * n_classes\"\n",
    "    return func_to_metric(error_rate, metric_type, True, axis=axis, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4,5)\n",
    "y = x.argmax(dim=1)\n",
    "test_eq(compute_single(ErrorRate(), x,y), 0)\n",
    "y1 = change_targ(y, 2, 5)\n",
    "test_eq(compute_single(ErrorRate(), x,y1), 0.5)\n",
    "test_eq(compute_single(ErrorRate(), x.unsqueeze(1).expand(4,2,5), torch.stack([y,y1], dim=1)), 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def top_k_accuracy(inp, targ, k=5, axis=-1):\n",
    "    \"Computes the Top-k accuracy (`targ` is in the top `k` predictions of `inp`)\"\n",
    "    inp = inp.topk(k=k, dim=axis)[1]\n",
    "    targ = targ.unsqueeze(dim=axis).expand_as(inp)\n",
    "    return (inp == targ).sum(dim=-1).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def TopKAccuracy(k=5, axis=-1, metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Computes the Top-k accuracy (`targ` is in the top `k` predictions of `inp`)\"\n",
    "    return func_to_metric(partial(top_k_accuracy, k=k, axis=axis), metric_type, False, \n",
    "                          log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(6,5)\n",
    "y = torch.arange(0,6)\n",
    "test_eq(compute_single(TopKAccuracy(), x[:5],y[:5]), 1)\n",
    "test_eq(compute_single(TopKAccuracy(), x, y), 5/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def APScoreBinary(axis=-1, average='macro', pos_label=1, sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Average Precision for single-label binary classification problems\"\n",
    "    return skm_to_fastai(skm.average_precision_score, axis=axis, activation=ActivationType.BinarySoftmax,\n",
    "                         average=average, pos_label=pos_label, sample_weight=sample_weight, log_metric=log_metric, \n",
    "                         **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def BalancedAccuracy(axis=-1, sample_weight=None, adjusted=False, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Balanced Accuracy for single-label binary classification problems\"\n",
    "    return skm_to_fastai(skm.balanced_accuracy_score, axis=axis,\n",
    "                         sample_weight=sample_weight, adjusted=adjusted, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def BrierScore(axis=-1, sample_weight=None, pos_label=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Brier score for single-label classification problems\"\n",
    "    return skm_to_fastai(skm.brier_score_loss, axis=axis,\n",
    "                         sample_weight=sample_weight, pos_label=pos_label, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def CohenKappa(axis=-1, labels=None, weights=None, sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Cohen kappa for single-label classification problems\"\n",
    "    return skm_to_fastai(skm.cohen_kappa_score, axis=axis, labels=labels, weights=weights,\n",
    "                         sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def F1Score(axis=-1, labels=None, pos_label=1, average='binary', sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"F1 score for single-label classification problems\"\n",
    "    return skm_to_fastai(skm.f1_score, axis=axis, labels=labels, pos_label=pos_label, \n",
    "                         average=average, sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def FBeta(beta, axis=-1, labels=None, pos_label=1, average='binary', sample_weight=None, \n",
    "          log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"FBeta score with `beta` for single-label classification problems\"\n",
    "    return skm_to_fastai(skm.fbeta_score, axis=axis, beta=beta, labels=labels, pos_label=pos_label, \n",
    "                         average=average, sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def HammingLoss(axis=-1, sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Hamming loss for single-label classification problems\"\n",
    "    return skm_to_fastai(skm.hamming_loss, axis=axis,\n",
    "                         sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Jaccard(axis=-1, labels=None, pos_label=1, average='binary', sample_weight=None, \n",
    "            log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Jaccard score for single-label classification problems\"\n",
    "    return skm_to_fastai(skm.jaccard_score, axis=axis, labels=labels, pos_label=pos_label, \n",
    "                         average=average, sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html#sklearn.metrics.jaccard_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Precision(axis=-1, labels=None, pos_label=1, average='binary', sample_weight=None, \n",
    "              log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Precision for single-label classification problems\"\n",
    "    return skm_to_fastai(skm.precision_score, axis=axis, labels=labels, pos_label=pos_label, \n",
    "                         average=average, sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Recall(axis=-1, labels=None, pos_label=1, average='binary', sample_weight=None, \n",
    "           log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Recall for single-label classification problems\"\n",
    "    return skm_to_fastai(skm.recall_score, axis=axis, labels=labels, pos_label=pos_label, \n",
    "                         average=average, sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def RocAuc(axis=-1, average='macro', sample_weight=None, max_fpr=None, multi_class='ovr', \n",
    "           log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Area Under the Receiver Operating Characteristic Curve for single-label multiclass classification problems\"\n",
    "    assert multi_class in ['ovr', 'ovo']\n",
    "    return skm_to_fastai(skm.roc_auc_score, axis=axis, activation=ActivationType.Softmax, flatten=False,\n",
    "                         average=average, sample_weight=sample_weight, max_fpr=max_fpr, multi_class=multi_class, \n",
    "                         log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def RocAucBinary(axis=-1, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', \n",
    "                 log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Area Under the Receiver Operating Characteristic Curve for single-label binary classification problems\"\n",
    "    return skm_to_fastai(skm.roc_auc_score, axis=axis, activation=ActivationType.BinarySoftmax,\n",
    "                         average=average, sample_weight=sample_weight, max_fpr=max_fpr, multi_class=multi_class, \n",
    "                         log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MatthewsCorrCoef(sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Matthews correlation coefficient for single-label classification problems\"\n",
    "    return skm_to_fastai(skm.matthews_corrcoef, sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def accuracy_multi(inp, targ):\n",
    "    \"Compute accuracy when `inp` and `targ` are the same size.\"\n",
    "    inp,targ = flatten_check(inp,targ)\n",
    "    return (inp==targ.bool()).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def AccuracyMulti(thresh=0.5, sigmoid=True, metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Compute accuracy when `inp` and `targ` are the same size.\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return func_to_metric(accuracy_multi, metric_type, False, thresh=thresh, activation=activation, \n",
    "                          log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing\n",
    "def change_1h_targ(targ, n):\n",
    "    idx = torch.randperm(targ.numel())[:n]\n",
    "    res = targ.clone().view(-1)\n",
    "    for i in idx: res[i] = 1-res[i]\n",
    "    return res.view(targ.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4,5)\n",
    "y = (torch.sigmoid(x) >= 0.5).byte()\n",
    "test_eq(compute_single(AccuracyMulti(),x,y), 1)\n",
    "test_eq(compute_single(AccuracyMulti(),x,1-y), 0)\n",
    "y1 = change_1h_targ(y, 5)\n",
    "test_eq(compute_single(AccuracyMulti(),x,y1), 0.75)\n",
    "\n",
    "#Different thresh\n",
    "y = (torch.sigmoid(x) >= 0.2).byte()\n",
    "test_eq(compute_single(AccuracyMulti(thresh=0.2),x,y), 1)\n",
    "test_eq(compute_single(AccuracyMulti(thresh=0.2),x,1-y), 0)\n",
    "y1 = change_1h_targ(y, 5)\n",
    "test_eq(compute_single(AccuracyMulti(thresh=0.2),x,y1), 0.75)\n",
    "\n",
    "#No sigmoid\n",
    "y = (x >= 0.5).byte()\n",
    "test_eq(compute_single(AccuracyMulti(sigmoid=False),x,y), 1)\n",
    "test_eq(compute_single(AccuracyMulti(sigmoid=False),x,1-y), 0)\n",
    "y1 = change_1h_targ(y, 5)\n",
    "test_eq(compute_single(AccuracyMulti(sigmoid=False),x,y1), 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def APScoreMulti(sigmoid=True, average='macro', pos_label=1, sample_weight=None, \n",
    "                 log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Average Precision for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastai(skm.average_precision_score, activation=activation, flatten=False,\n",
    "                         average=average, pos_label=pos_label, sample_weight=sample_weight, \n",
    "                         log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def BrierScoreMulti(thresh=0.5, sigmoid=True, sample_weight=None, pos_label=None, \n",
    "                    log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Brier score for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastai(skm.brier_score_loss, thresh=thresh, activation=activation, flatten=False,\n",
    "                         sample_weight=sample_weight, pos_label=pos_label, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def F1ScoreMulti(thresh=0.5, sigmoid=True, labels=None, pos_label=1, average='macro', sample_weight=None, \n",
    "                 log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"F1 score for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastai(skm.f1_score, thresh=thresh, activation=activation, flatten=False,\n",
    "                         labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight, \n",
    "                         log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def FBetaMulti(beta, thresh=0.5, sigmoid=True, labels=None, pos_label=1, average='macro', sample_weight=None, \n",
    "               log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"FBeta score with `beta` for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastai(skm.fbeta_score, thresh=thresh, activation=activation, flatten=False,\n",
    "                beta=beta, labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight, \n",
    "                log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def HammingLossMulti(thresh=0.5, sigmoid=True, labels=None, sample_weight=None, \n",
    "                     log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Hamming loss for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastai(skm.hamming_loss, thresh=thresh, activation=activation, flatten=False,\n",
    "                         sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def JaccardMulti(thresh=0.5, sigmoid=True, labels=None, pos_label=1, average='macro', sample_weight=None, \n",
    "                 log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Jaccard score for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastai(skm.jaccard_score, thresh=thresh, activation=activation, flatten=False,\n",
    "                         labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight, \n",
    "                         log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html#sklearn.metrics.jaccard_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MatthewsCorrCoefMulti(thresh=0.5, sigmoid=True, sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Matthews correlation coefficient for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastai(skm.matthews_corrcoef, thresh=thresh, activation=activation, flatten=False, \n",
    "                         sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def PrecisionMulti(thresh=0.5, sigmoid=True, labels=None, pos_label=1, average='macro', sample_weight=None, \n",
    "                   log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Precision for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastai(skm.precision_score, thresh=thresh, activation=activation, flatten=False,\n",
    "                         labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight, \n",
    "                         log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def RecallMulti(thresh=0.5, sigmoid=True, labels=None, pos_label=1, average='macro', sample_weight=None, \n",
    "                log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Recall for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastai(skm.recall_score, thresh=thresh, activation=activation, flatten=False,\n",
    "                         labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight, \n",
    "                         log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def RocAucMulti(sigmoid=True, average='macro', sample_weight=None, max_fpr=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Area Under the Receiver Operating Characteristic Curve for multi-label binary classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastai(skm.roc_auc_score, activation=activation, flatten=False,\n",
    "                         average=average, sample_weight=sample_weight, max_fpr=max_fpr, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6183/1899176771.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x,y = torch.tensor([np.arange(start=0, stop=0.2, step=0.04)]*20), torch.tensor([0, 0, 1, 1]).repeat(5)\n"
     ]
    }
   ],
   "source": [
    "roc_auc_metric = RocAucMulti(sigmoid=False)\n",
    "x,y = torch.tensor([np.arange(start=0, stop=0.2, step=0.04)]*20), torch.tensor([0, 0, 1, 1]).repeat(5)\n",
    "assert compute_val(roc_auc_metric, x, y) == 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def mse(inp,targ):\n",
    "    \"Mean squared error between `inp` and `targ`.\"\n",
    "    return F.mse_loss(*flatten_check(inp,targ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MSE(metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Mean squared error between `inp` and `targ`.\"\n",
    "    return func_to_metric(mse, metric_type, False, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = torch.randn(4,5),torch.randn(4,5)\n",
    "test_close(compute_single(MSE(),x1,x2), (x1-x2).pow(2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def rmse(inp, targ): \n",
    "    return torch.sqrt(F.mse_loss(inp, targ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def RMSE(log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Mean squared error between `inp` and `targ`.\"\n",
    "    return func_to_metric(rmse, MetricType.Accum, False, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "test_eq(compute_val(RMSE(), x1, x2), torch.sqrt(F.mse_loss(x1,x2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def mae(inp,targ):\n",
    "    \"Mean absolute error between `inp` and `targ`.\"\n",
    "    inp,targ = flatten_check(inp,targ)\n",
    "    return torch.abs(inp - targ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MAE(metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Mean absolute error between `inp` and `targ`.\"\n",
    "    return func_to_metric(mae, metric_type, False, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = torch.randn(4,5),torch.randn(4,5)\n",
    "test_eq(compute_single(MAE(),x1,x2), torch.abs(x1-x2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def msle(inp, targ):\n",
    "    \"Mean squared logarithmic error between `inp` and `targ`.\"\n",
    "    inp,targ = flatten_check(inp,targ)\n",
    "    return F.mse_loss(torch.log(1 + inp), torch.log(1 + targ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MSLE(metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Mean squared logarithmic error between `inp` and `targ`.\"\n",
    "    return func_to_metric(msle, metric_type, False, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = torch.randn(4,5),torch.randn(4,5)\n",
    "x1,x2 = torch.relu(x1),torch.relu(x2)\n",
    "test_close(compute_single(MSLE(), x1,x2), (torch.log(x1+1)-torch.log(x2+1)).pow(2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def exp_rmspe(inp,targ):\n",
    "    inp,targ = torch.exp(inp),torch.exp(targ)\n",
    "    return torch.sqrt(((targ - inp)/targ).pow(2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ExpRMSE(log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Root mean square percentage error of the exponential of  predictions and targets\"\n",
    "    return func_to_metric(exp_rmspe, MetricType.Accum, False, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "test_eq(compute_val(ExpRMSE(), x1, x2), torch.sqrt((((torch.exp(x2) - torch.exp(x1))/torch.exp(x2))**2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ExplainedVariance(sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Explained variance between predictions and targets\"\n",
    "    return skm_to_fastai(skm.explained_variance_score, is_class=False, \n",
    "                         sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def R2Score(sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"R2 score between predictions and targets\"\n",
    "    return skm_to_fastai(skm.r2_score, is_class=False, sample_weight=sample_weight, \n",
    "                         log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def PearsonCorrCoef(dim_argmax=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Pearson correlation coefficient for regression problem\"\n",
    "    def pearsonr(x,y): return scs.pearsonr(x,y)[0]\n",
    "    return AccumMetric(pearsonr, invert_arg=False, dim_argmax=dim_argmax, \n",
    "                       log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scipy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html?highlight=pearson#scipy.stats.pearsonr) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(-999, 999,(20,))\n",
    "y = torch.randint(-999, 999,(20,))\n",
    "test_eq(compute_val(PearsonCorrCoef(), x, y), scs.pearsonr(x.view(-1), y.view(-1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SpearmanCorrCoef(dim_argmax=None, axis=0, nan_policy='propagate', log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Spearman correlation coefficient for regression problem\"\n",
    "    def spearmanr(a,b=None,**kwargs): return scs.spearmanr(a,b,**kwargs)[0]\n",
    "    return AccumMetric(partial(spearmanr, axis=axis, nan_policy=nan_policy),\n",
    "                       invert_arg=False, dim_argmax=dim_argmax, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scipy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html?highlight=spearman#scipy.stats.spearmanr) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(-999, 999,(20,))\n",
    "y = torch.randint(-999, 999,(20,))\n",
    "test_eq(compute_val(SpearmanCorrCoef(), x, y), scs.spearmanr(x.view(-1), y.view(-1))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet34()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cast(torch.rand(1,3,128,128), TensorImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.torch_core.TensorImage"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def foreground_acc(inp, targ, bkg_idx=0, axis=1):\n",
    "    \"Computes non-background accuracy for multiclass segmentation\"\n",
    "    targ = cast(targ.squeeze(1), TensorBase)\n",
    "    mask = targ != bkg_idx\n",
    "    return (inp[mask]==targ[mask]).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ForegroundAcc(bkg_idx=0, axis=1, metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Computes non-background accuracy for multiclass segmentation\"\n",
    "    return func_to_metric(foreground_acc, metric_type, True, bkg_idx=bkg_idx, axis=axis, \n",
    "                          log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cast(torch.randn(4,5,3,3), TensorImage)\n",
    "y = cast(x, TensorMask).argmax(dim=1)[:,None]\n",
    "test_eq(compute_single(ForegroundAcc(),x,y), 1)\n",
    "y[0] = 0 #the 0s are ignored so we get the same value\n",
    "test_eq(compute_single(ForegroundAcc(),x,y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dice(Metric):\n",
    "    \"Dice coefficient metric for binary target in segmentation\"\n",
    "    def __init__(self, axis=1, log_metric=LogMetric.Valid, **kwargs):\n",
    "        super().__init__(dim_argmax=axis, log_metric=log_metric, **kwargs)\n",
    "    def reset(self): self.inter,self.union = 0,0\n",
    "    def accumulate(self, learn):\n",
    "        super().accumulate(learn)\n",
    "        self.pred,self.targ = flatten_check(self.pred, self.targ)\n",
    "        self.inter += (self.pred*self.targ).float().sum().item()\n",
    "        self.union += (self.pred+self.targ).float().sum().item()\n",
    "\n",
    "    @property\n",
    "    def value(self): return 2. * self.inter/self.union if self.union > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = cast(torch.randn(20,2,3,3), TensorImage)\n",
    "x2 = cast(torch.randint(0, 2, (20, 3, 3)), TensorMask)\n",
    "pred = x1.argmax(1)\n",
    "inter = (pred*x2).float().sum().item()\n",
    "union = (pred+x2).float().sum().item()\n",
    "test_eq(compute_val(Dice(), x1, x2), 2*inter/union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DiceMulti(Metric):\n",
    "    \"Averaged Dice metric (Macro F1) for multiclass target in segmentation\"\n",
    "    def __init__(self, axis=1, log_metric=LogMetric.Valid, **kwargs):\n",
    "        super().__init__(dim_argmax=axis, log_metric=log_metric, **kwargs)\n",
    "    def reset(self): self.inter,self.union = {},{}\n",
    "    def accumulate(self, learn):\n",
    "        super().accumulate(learn)\n",
    "        self.pred,self.targ = flatten_check(self.pred, self.targ)\n",
    "        for c in range(learn.pred.shape[self.dim_argmax]):\n",
    "            p = torch.where(self.pred == c, 1, 0)\n",
    "            t = torch.where(self.targ == c, 1, 0)\n",
    "            c_inter = (p*t).float().sum().item()\n",
    "            c_union = (p+t).float().sum().item()\n",
    "            if c in self.inter:\n",
    "                self.inter[c] += c_inter\n",
    "                self.union[c] += c_union\n",
    "            else:\n",
    "                self.inter[c] = c_inter\n",
    "                self.union[c] = c_union\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        binary_dice_scores = np.array([])\n",
    "        for c in self.inter:\n",
    "            binary_dice_scores = np.append(binary_dice_scores, 2.*self.inter[c]/self.union[c] if self.union[c] > 0 else np.nan)\n",
    "        return np.nanmean(binary_dice_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DiceMulti method implements the \"Averaged F1: arithmetic mean over harmonic means\" described in this publication: https://arxiv.org/pdf/1911.03347.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1a = torch.ones(20,1,1,1)\n",
    "x1b = torch.clone(x1a)*0.5\n",
    "x1c = torch.clone(x1a)*0.3\n",
    "x1 = torch.cat((x1a,x1b,x1c),dim=1)   # Prediction: 20xClass0\n",
    "x2 = torch.zeros(20,1,1)              # Target: 20xClass0\n",
    "test_eq(compute_val(DiceMulti(), x1, x2), 1.)\n",
    "\n",
    "x2 = torch.ones(20,1,1)               # Target: 20xClass1\n",
    "test_eq(compute_val(DiceMulti(), x1, x2), 0.)\n",
    "\n",
    "x2a = torch.zeros(10,1,1)\n",
    "x2b = torch.ones(5,1,1)\n",
    "x2c = torch.ones(5,1,1) * 2\n",
    "x2 = torch.cat((x2a,x2b,x2c),dim=0)   # Target: 10xClass0, 5xClass1, 5xClass2\n",
    "dice1 = (2*10)/(2*10+10)              # Dice: 2*TP/(2*TP+FP+FN)\n",
    "dice2 = 0\n",
    "dice3 = 0\n",
    "test_eq(compute_val(DiceMulti(), x1, x2), (dice1+dice2+dice3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class JaccardCoeff(Dice):\n",
    "    \"Implementation of the Jaccard coefficient that is lighter in RAM\"\n",
    "    @property\n",
    "    def value(self): return self.inter/(self.union-self.inter) if self.union > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = cast(torch.randn(20,2,3,3), TensorImage)\n",
    "x2 = cast(torch.randint(0, 2, (20, 3, 3)), TensorMask)\n",
    "pred = x1.argmax(1)\n",
    "inter = (pred*x2).float().sum().item()\n",
    "union = (pred+x2).float().sum().item()\n",
    "test_eq(compute_val(JaccardCoeff(), x1, x2), inter/(union-inter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CorpusBLEUMetric(Metric):\n",
    "    \"BLEU Metric calculated over the validation corpus\"\n",
    "    def __init__(self, vocab_sz=5000, axis=-1, log_metric=LogMetric.Valid, name='CorpusBLEU', **kwargs):\n",
    "        super().__init__(log_metric=log_metric, name=name, **kwargs)\n",
    "        self.axis, self.vocab_sz = axis, vocab_sz\n",
    "        self.pred_len,self.targ_len,self.samp_idx,self.corrects,self.counts, = 0,0,0,[0]*4,[0]*4\n",
    "\n",
    "    def reset(self):\n",
    "        self.pred_len,self.targ_len,self.corrects,self.counts = 0,0,[0]*4,[0]*4\n",
    "\n",
    "    class NGram():\n",
    "        def __init__(self, ngram, max_n=5000): self.ngram,self.max_n = ngram,max_n\n",
    "        def __eq__(self, other):\n",
    "            if len(self.ngram) != len(other.ngram): return False\n",
    "            return np.all(np.array(self.ngram) == np.array(other.ngram))\n",
    "        def __hash__(self): return int(sum([o * self.max_n**i for i,o in enumerate(self.ngram)]))\n",
    "\n",
    "    def get_grams(self, x, n, max_n=5000):\n",
    "        return x if n==1 else [self.NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)]\n",
    "\n",
    "    def get_correct_ngrams(self, pred, targ, n, max_n=5000):\n",
    "        pred_grams,targ_grams = self.get_grams(pred, n, max_n=max_n),self.get_grams(targ, n, max_n=max_n)\n",
    "        pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams)\n",
    "        return sum([min(c, targ_cnt[g]) for g,c in pred_cnt.items()]),len(pred_grams)\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        if learn.training: return None\n",
    "        else:\n",
    "            last_output = learn.pred.argmax(dim=self.axis)\n",
    "            last_target = learn.y\n",
    "            for pred,targ in zip(last_output.cpu().numpy(),last_target.cpu().numpy()):\n",
    "                self.pred_len += len(pred)\n",
    "                self.targ_len += len(targ)\n",
    "                smooth_mteval = 1\n",
    "                for i in range(4):\n",
    "                    c,t = self.get_correct_ngrams(pred, targ, i+1, max_n=self.vocab_sz)\n",
    "                    if c == 0:\n",
    "                        smooth_mteval *= 2\n",
    "                        c = 1 / smooth_mteval    # exp smoothing, method 3 from http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf\n",
    "                    self.corrects[i] += c\n",
    "                    self.counts[i]   += t\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.counts == 0: return None\n",
    "        elif max(self.corrects) == 0: return 0.0\n",
    "        else:\n",
    "            precs = [c/t for c,t in zip(self.corrects,self.counts)]\n",
    "            len_penalty = math.exp(1 - self.targ_len/self.pred_len) if self.pred_len < self.targ_len else 1\n",
    "            return len_penalty * ((precs[0]*precs[1]*precs[2]*precs[3]) ** 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vcb_emb(pred, targ):\n",
    "    # create vocab \"embedding\" for predictions\n",
    "    vcb_sz = max(torch.unique(torch.cat([pred, targ])))+1\n",
    "    pred_emb=torch.zeros(pred.size()[0], pred.size()[1] ,vcb_sz)\n",
    "    for i,v in enumerate(pred):\n",
    "        pred_emb[i].scatter_(1, v.view(len(v),1),1)\n",
    "    return pred_emb\n",
    "\n",
    "def compute_bleu_val(met, x1, x2):\n",
    "    met.reset()\n",
    "    learn = TstLearner()\n",
    "    learn.training=False    \n",
    "    for i in range(len(x1)): \n",
    "        learn.pred,learn.yb = x1, (x2,)\n",
    "        met.accumulate(learn)\n",
    "    return met.value\n",
    "\n",
    "targ = torch.tensor([[1,2,3,4,5,6,1,7,8]]) \n",
    "pred = torch.tensor([[1,9,3,4,5,6,1,10,8]])\n",
    "pred_emb = create_vcb_emb(pred, targ)\n",
    "test_close(compute_bleu_val(CorpusBLEUMetric(), pred_emb, targ), 0.48549)\n",
    "\n",
    "targ = torch.tensor([[1,2,3,4,5,6,1,7,8],[1,2,3,4,5,6,1,7,8]]) \n",
    "pred = torch.tensor([[1,9,3,4,5,6,1,10,8],[1,9,3,4,5,6,1,10,8]])\n",
    "pred_emb = create_vcb_emb(pred, targ)\n",
    "test_close(compute_bleu_val(CorpusBLEUMetric(), pred_emb, targ), 0.48549)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BLEU metric was introduced in [this article](https://www.aclweb.org/anthology/P02-1040) to come up with a way to evaluate the performance of translation models. It's based on the precision of n-grams in your prediction compared to your target. See the [fastai NLP course BLEU notebook](https://github.com/fastai/course-nlp/blob/master/bleu_metric.ipynb) for a more detailed description of BLEU.\n",
    "\n",
    "The smoothing used in the precision calculation is the same as in [SacreBLEU](https://github.com/mjpost/sacrebleu/blob/32c54cdd0dfd6a9fadd5805f2ea189ac0df63907/sacrebleu/sacrebleu.py#L540-L542), which in turn is \"method 3\" from the [Chen & Cherry, 2014](http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Perplexity(AvgLoss):\n",
    "    \"Perplexity (exponential of cross-entropy loss) for Language Models\"\n",
    "    @property\n",
    "    def value(self): return torch.exp(self.total/self.count) if self.count != 0 else None\n",
    "    @property\n",
    "    def name(self):  return \"perplexity\"\n",
    "\n",
    "perplexity = Perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = torch.randn(20,5),torch.randint(0, 5, (20,))\n",
    "tst = perplexity\n",
    "tst.reset()\n",
    "vals = [0,6,15,20]\n",
    "learn = TstLearner()\n",
    "for i in range(3): \n",
    "    learn.yb = (x2[vals[i]:vals[i+1]],)\n",
    "    learn.loss = F.cross_entropy(x1[vals[i]:vals[i+1]],x2[vals[i]:vals[i+1]])\n",
    "    tst.accumulate(learn)\n",
    "test_close(tst.value, torch.exp(F.cross_entropy(x1,x2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LossMetrics -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LossMetric(AvgMetric):\n",
    "    \"Create a metric from `loss_func.attr` named `nm`\"\n",
    "    def __init__(self, attr, nm=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "        super().__init__(noop, log_metric=log_metric, **kwargs)\n",
    "        store_attr('attr,nm')\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += learn.to_detach(getattr(learn.loss_func, self.attr, 0))*bs\n",
    "        self.count += bs\n",
    "\n",
    "    @property\n",
    "    def name(self): return self.attr if self.nm is None else self.nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def LossMetrics(attrs, nms=None):\n",
    "    \"List of `LossMetric` for each of `attrs` and `nms`\"\n",
    "    if isinstance(attrs, str): attrs = attrs.split(',')\n",
    "    nms = attrs if nms is None else nms.split(',') if isinstance(nms, str) else nms\n",
    "    return [LossMetric(a, n) for a,n in zip(attrs,nms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineL1L2(Module):\n",
    "    def forward(self, out, targ):\n",
    "        self.l1 = F.l1_loss(out, targ)\n",
    "        self.l2 = F.mse_loss(out, targ)\n",
    "        return self.l1+self.l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.990499</td>\n",
       "      <td>8.720571</td>\n",
       "      <td>2.131581</td>\n",
       "      <td>6.588990</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.511189</td>\n",
       "      <td>5.715763</td>\n",
       "      <td>1.665535</td>\n",
       "      <td>4.050228</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = synth_learner(metrics=LossMetrics('l1,l2'))\n",
    "learn.loss_func = CombineL1L2()\n",
    "learn.fit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_torch_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 01a_losses.ipynb.\n",
      "Converted 02_data.load.ipynb.\n",
      "Converted 03_data.core.ipynb.\n",
      "Converted 04_data.external.ipynb.\n",
      "Converted 05_data.transforms.ipynb.\n",
      "Converted 06_data.block.ipynb.\n",
      "Converted 07_vision.core.ipynb.\n",
      "Converted 08_vision.data.ipynb.\n",
      "Converted 09_vision.augment.ipynb.\n",
      "Converted 09b_vision.utils.ipynb.\n",
      "Converted 09c_vision.widgets.ipynb.\n",
      "Converted 10_tutorial.pets.ipynb.\n",
      "Converted 10b_tutorial.albumentations.ipynb.\n",
      "Converted 11_vision.models.xresnet.ipynb.\n",
      "Converted 12_optimizer.ipynb.\n",
      "Converted 13_callback.core.ipynb.\n",
      "Converted 13a_learner.ipynb.\n",
      "Converted 13b_metrics.ipynb.\n",
      "Converted 14_callback.schedule.ipynb.\n",
      "Converted 14a_callback.data.ipynb.\n",
      "Converted 15_callback.hook.ipynb.\n",
      "Converted 15a_vision.models.unet.ipynb.\n",
      "Converted 16_callback.progress.ipynb.\n",
      "Converted 17_callback.tracker.ipynb.\n",
      "Converted 18_callback.fp16.ipynb.\n",
      "Converted 18a_callback.training.ipynb.\n",
      "Converted 18b_callback.preds.ipynb.\n",
      "Converted 19_callback.mixup.ipynb.\n",
      "Converted 20_interpret.ipynb.\n",
      "Converted 20a_distributed.ipynb.\n",
      "Converted 21_vision.learner.ipynb.\n",
      "Converted 22_tutorial.imagenette.ipynb.\n",
      "Converted 23_tutorial.vision.ipynb.\n",
      "Converted 24_tutorial.image_sequence.ipynb.\n",
      "Converted 24_tutorial.siamese.ipynb.\n",
      "Converted 24_vision.gan.ipynb.\n",
      "Converted 30_text.core.ipynb.\n",
      "Converted 31_text.data.ipynb.\n",
      "Converted 32_text.models.awdlstm.ipynb.\n",
      "Converted 33_text.models.core.ipynb.\n",
      "Converted 34_callback.rnn.ipynb.\n",
      "Converted 35_tutorial.wikitext.ipynb.\n",
      "Converted 37_text.learner.ipynb.\n",
      "Converted 38_tutorial.text.ipynb.\n",
      "Converted 39_tutorial.transformers.ipynb.\n",
      "Converted 40_tabular.core.ipynb.\n",
      "Converted 41_tabular.data.ipynb.\n",
      "Converted 42_tabular.model.ipynb.\n",
      "Converted 43_tabular.learner.ipynb.\n",
      "Converted 44_tutorial.tabular.ipynb.\n",
      "Converted 45_collab.ipynb.\n",
      "Converted 46_tutorial.collab.ipynb.\n",
      "Converted 50_tutorial.datablock.ipynb.\n",
      "Converted 60_medical.imaging.ipynb.\n",
      "Converted 61_tutorial.medical_imaging.ipynb.\n",
      "Converted 65_medical.text.ipynb.\n",
      "Converted 70_callback.wandb.ipynb.\n",
      "Converted 71_callback.tensorboard.ipynb.\n",
      "Converted 72_callback.neptune.ipynb.\n",
      "Converted 73_callback.captum.ipynb.\n",
      "Converted 74_callback.azureml.ipynb.\n",
      "Converted 97_test_utils.ipynb.\n",
      "Converted 99_pytorch_doc.ipynb.\n",
      "Converted dev-setup.ipynb.\n",
      "Converted app_examples.ipynb.\n",
      "Converted camvid.ipynb.\n",
      "Converted migrating_catalyst.ipynb.\n",
      "Converted migrating_ignite.ipynb.\n",
      "Converted migrating_lightning.ipynb.\n",
      "Converted migrating_pytorch.ipynb.\n",
      "Converted migrating_pytorch_verbose.ipynb.\n",
      "Converted ulmfit.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted quick_start.ipynb.\n",
      "Converted tutorial.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
