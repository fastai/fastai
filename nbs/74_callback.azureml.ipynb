{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.basics import *\n",
    "from fastai.learner import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp callback.azureml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AzureML Callback\n",
    "\n",
    "Track fastai experiments with the azure machine learning plattform.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Install the azureml SDK:\n",
    "\n",
    "```python\n",
    "pip install azureml-core\n",
    "```\n",
    "\n",
    "## How to use it?\n",
    "\n",
    "Import and use `AzureMLCallback` during model fitting.\n",
    "\n",
    "If you are submitting your training run with azureml SDK [ScriptRunConfig](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets), the callback will automatically detect the run and log metrics. For example:\n",
    "\n",
    "```python\n",
    "from fastai.callback.azureml import AzureMLCallback\n",
    "learn.fit_one_cycle(epoch, lr, cbs=AzureMLCallback())\n",
    "```\n",
    "\n",
    "If you are running an experiment manually and just want to have interactive logging of the run, use azureml's `Experiment.start_logging` to create the interactive `run`, and pass that into `AzureMLCallback`. For example:\n",
    "\n",
    "```python\n",
    "from azureml.core import Experiment\n",
    "experiment = Experiment(workspace=ws, name='experiment_name')\n",
    "run = experiment.start_logging(outputs=None, snapshot_directory=None)\n",
    "\n",
    "from fastai.callback.azureml import AzureMLCallback\n",
    "learn.fit_one_cycle(epoch, lr, cbs=AzureMLCallback(run))\n",
    "```\n",
    "\n",
    "If you are running an experiment on your local machine (i.e. not using `ScriptRunConfig` and not passing an azureml `run` into the callback), it will recognize that there is no AzureML run to log to, and print the log attempts instead.\n",
    "\n",
    "To save the model weights, use the usual fastai methods and save the model to the `outputs` folder, which is a \"special\" (for Azure) folder that is automatically tracked in AzureML.\n",
    "\n",
    "As it stands, note that if you pass the callback into your `Learner` directly, e.g.:\n",
    "```python\n",
    "learn = Learner(dls, model, cbs=AzureMLCallback())\n",
    "```\n",
    "…some `Learner` methods (e.g. `learn.show_results()`) might add unwanted logging into your azureml experiment runs. Adding further checks into the callback should help eliminate this – another PR needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from azureml.core.run import Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AzureMLCallback(Callback):\n",
    "    \"Log losses, metrics, model architecture summary to AzureML\"\n",
    "    order = Recorder.order+1\n",
    "\n",
    "    def __init__(self, azurerun=None):\n",
    "        if azurerun:\n",
    "            self.azurerun = azurerun\n",
    "        else:\n",
    "            self.azurerun = Run.get_context()\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.azurerun.log(\"n_epoch\", self.learn.n_epoch)\n",
    "        self.azurerun.log(\"model_class\", str(type(self.learn.model)))\n",
    "\n",
    "        try:\n",
    "            summary_file = Path(\"outputs\") / 'model_summary.txt'\n",
    "            with summary_file.open(\"w\") as f:\n",
    "                f.write(repr(self.learn.model))\n",
    "        except:\n",
    "            print('Did not log model summary. Check if your model is PyTorch model.')\n",
    "\n",
    "    def after_batch(self):\n",
    "        # log loss and opt.hypers\n",
    "        if self.learn.training:\n",
    "            self.azurerun.log('batch__loss', self.learn.loss.item())\n",
    "            self.azurerun.log('batch__train_iter', self.learn.train_iter)\n",
    "            for i, h in enumerate(self.learn.opt.hypers):\n",
    "                for k, v in h.items():\n",
    "                    self.azurerun.log(f'batch__opt.hypers.{k}', v)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        # log metrics\n",
    "        for n, v in zip(self.learn.recorder.metric_names, self.learn.recorder.log):\n",
    "            if n not in ['epoch', 'time']:\n",
    "                self.azurerun.log(f'epoch__{n}', v)\n",
    "            if n == 'time':\n",
    "                # split elapsed time string, then convert into 'seconds' to log\n",
    "                m, s = str(v).split(':')\n",
    "                elapsed = int(m)*60 + int(s)\n",
    "                self.azurerun.log(f'epoch__{n}', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
