{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp losses\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.imports import *\n",
    "from fastai.torch_imports import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "> Custom fastai loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.binary_cross_entropy_with_logits(torch.randn(4,5), torch.randint(0, 2, (4,5)).float(), reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@log_args\n",
    "class BaseLoss():\n",
    "    \"Same as `loss_cls`, but flattens input and target.\"\n",
    "    activation=decodes=noops\n",
    "    def __init__(self, loss_cls, *args, axis=-1, flatten=True, floatify=False, is_2d=True, **kwargs):\n",
    "        store_attr(\"axis,flatten,floatify,is_2d\")\n",
    "        self.func = loss_cls(*args,**kwargs)\n",
    "        functools.update_wrapper(self, self.func)\n",
    "\n",
    "    def __repr__(self): return f\"FlattenedLoss of {self.func}\"\n",
    "    @property\n",
    "    def reduction(self): return self.func.reduction\n",
    "    @reduction.setter\n",
    "    def reduction(self, v): self.func.reduction = v\n",
    "\n",
    "    def __call__(self, inp, targ, **kwargs):\n",
    "        inp  = inp .transpose(self.axis,-1).contiguous()\n",
    "        targ = targ.transpose(self.axis,-1).contiguous()\n",
    "        if self.floatify and targ.dtype!=torch.float16: targ = targ.float()\n",
    "        if targ.dtype in [torch.int8, torch.int16, torch.int32]: targ = targ.long()\n",
    "        if self.flatten: inp = inp.view(-1,inp.shape[-1]) if self.is_2d else inp.view(-1)\n",
    "        return self.func.__call__(inp, targ.view(-1) if self.flatten else targ, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping a general loss function inside of `BaseLoss` provides extra functionalities to your loss functions:\n",
    "- flattens the tensors before trying to take the losses since it's more convenient (with a potential tranpose to put `axis` at the end)\n",
    "- a potential `activation` method that tells the library if there is an activation fused in the loss (useful for inference and methods such as `Learner.get_preds` or `Learner.predict`)\n",
    "- a potential <code>decodes</code> method that is used on predictions in inference (for instance, an argmax in classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `args` and `kwargs` will be passed to `loss_cls` during the initialization to instantiate a loss function. `axis` is put at the end for losses like softmax that are often performed on the last axis. If `floatify=True`, the `targs` will be converted to floats (useful for losses that only accept float targets like `BCEWithLogitsLoss`), and `is_2d` determines if we flatten while keeping the first dimension (batch size) or completely flatten the input. We want the first for losses like Cross Entropy, and the second for pretty much anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@log_args\n",
    "@delegates()\n",
    "class CrossEntropyLossFlat(BaseLoss):\n",
    "    \"Same as `nn.CrossEntropyLoss`, but flattens input and target.\"\n",
    "    y_int = True\n",
    "    @use_kwargs_dict(keep=True, weight=None, ignore_index=-100, reduction='mean')\n",
    "    def __init__(self, *args, axis=-1, **kwargs): super().__init__(nn.CrossEntropyLoss, *args, axis=axis, **kwargs)\n",
    "    def decodes(self, x):    return x.argmax(dim=self.axis)\n",
    "    def activation(self, x): return F.softmax(x, dim=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = CrossEntropyLossFlat()\n",
    "output = torch.randn(32, 5, 10)\n",
    "target = torch.randint(0, 10, (32,5))\n",
    "#nn.CrossEntropy would fail with those two tensors, but not our flattened version.\n",
    "_ = tst(output, target)\n",
    "test_fail(lambda x: nn.CrossEntropyLoss()(output,target))\n",
    "\n",
    "#Associated activation is softmax\n",
    "test_eq(tst.activation(output), F.softmax(output, dim=-1))\n",
    "#This loss function has a decodes which is argmax\n",
    "test_eq(tst.decodes(output), output.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In a segmentation task, we want to take the softmax over the channel dimension\n",
    "tst = CrossEntropyLossFlat(axis=1)\n",
    "output = torch.randn(32, 5, 128, 128)\n",
    "target = torch.randint(0, 5, (32, 128, 128))\n",
    "_ = tst(output, target)\n",
    "\n",
    "test_eq(tst.activation(output), F.softmax(output, dim=1))\n",
    "test_eq(tst.decodes(output), output.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@log_args\n",
    "@delegates()\n",
    "class BCEWithLogitsLossFlat(BaseLoss):\n",
    "    \"Same as `nn.BCEWithLogitsLoss`, but flattens input and target.\"\n",
    "    @use_kwargs_dict(keep=True, weight=None, reduction='mean', pos_weight=None)\n",
    "    def __init__(self, *args, axis=-1, floatify=True, thresh=0.5, **kwargs):\n",
    "        if kwargs.get('pos_weight', None) is not None and kwargs.get('flatten', None) is True:\n",
    "            raise ValueError(\"`flatten` must be False when using `pos_weight` to avoid a RuntimeError due to shape mismatch\")\n",
    "        if kwargs.get('pos_weight', None) is not None: kwargs['flatten'] = False\n",
    "        super().__init__(nn.BCEWithLogitsLoss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs)\n",
    "        self.thresh = thresh\n",
    "\n",
    "    def decodes(self, x):    return x>self.thresh\n",
    "    def activation(self, x): return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = BCEWithLogitsLossFlat()\n",
    "output = torch.randn(32, 5, 10)\n",
    "target = torch.randn(32, 5, 10)\n",
    "#nn.BCEWithLogitsLoss would fail with those two tensors, but not our flattened version.\n",
    "_ = tst(output, target)\n",
    "test_fail(lambda x: nn.BCEWithLogitsLoss()(output,target))\n",
    "output = torch.randn(32, 5)\n",
    "target = torch.randint(0,2,(32, 5))\n",
    "#nn.BCEWithLogitsLoss would fail with int targets but not our flattened version.\n",
    "_ = tst(output, target)\n",
    "test_fail(lambda x: nn.BCEWithLogitsLoss()(output,target))\n",
    "\n",
    "tst = BCEWithLogitsLossFlat(pos_weight=torch.ones(10))\n",
    "output = torch.randn(32, 5, 10)\n",
    "target = torch.randn(32, 5, 10)\n",
    "_ = tst(output, target)\n",
    "test_fail(lambda x: nn.BCEWithLogitsLoss()(output,target))\n",
    "\n",
    "#Associated activation is sigmoid\n",
    "test_eq(tst.activation(output), torch.sigmoid(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@log_args(to_return=True)\n",
    "@use_kwargs_dict(weight=None, reduction='mean')\n",
    "def BCELossFlat(*args, axis=-1, floatify=True, **kwargs):\n",
    "    \"Same as `nn.BCELoss`, but flattens input and target.\"\n",
    "    return BaseLoss(nn.BCELoss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = BCELossFlat()\n",
    "output = torch.sigmoid(torch.randn(32, 5, 10))\n",
    "target = torch.randint(0,2,(32, 5, 10))\n",
    "_ = tst(output, target)\n",
    "test_fail(lambda x: nn.BCELoss()(output,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@log_args(to_return=True)\n",
    "@use_kwargs_dict(reduction='mean')\n",
    "def MSELossFlat(*args, axis=-1, floatify=True, **kwargs):\n",
    "    \"Same as `nn.MSELoss`, but flattens input and target.\"\n",
    "    return BaseLoss(nn.MSELoss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = MSELossFlat()\n",
    "output = torch.sigmoid(torch.randn(32, 5, 10))\n",
    "target = torch.randint(0,2,(32, 5, 10))\n",
    "_ = tst(output, target)\n",
    "test_fail(lambda x: nn.MSELoss()(output,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#cuda\n",
    "#Test losses work in half precision\n",
    "output = torch.sigmoid(torch.randn(32, 5, 10)).half().cuda()\n",
    "target = torch.randint(0,2,(32, 5, 10)).half().cuda()\n",
    "for tst in [BCELossFlat(), MSELossFlat()]: _ = tst(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@log_args(to_return=True)\n",
    "@use_kwargs_dict(reduction='mean')\n",
    "def L1LossFlat(*args, axis=-1, floatify=True, **kwargs):\n",
    "    \"Same as `nn.L1Loss`, but flattens input and target.\"\n",
    "    return BaseLoss(nn.L1Loss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_args\n",
    "class LabelSmoothingCrossEntropy(Module):\n",
    "    y_int = True\n",
    "    def __init__(self, eps:float=0.1, reduction='mean'): self.eps,self.reduction = eps,reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        c = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        if self.reduction=='sum': loss = -log_preds.sum()\n",
    "        else:\n",
    "            loss = -log_preds.sum(dim=-1) #We divide by that size at the return line so sum and not mean\n",
    "            if self.reduction=='mean':  loss = loss.mean()\n",
    "        return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), reduction=self.reduction)\n",
    "\n",
    "    def activation(self, out): return F.softmax(out, dim=-1)\n",
    "    def decodes(self, out):    return out.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of the formula we define:\n",
    "- a `reduction` attribute, that will be used when we call `Learner.get_preds`\n",
    "- an `activation` function that represents the activation fused in the loss (since we use cross entropy behind the scenes). It will be applied to the output of the model when calling `Learner.get_preds` or `Learner.predict`\n",
    "- a <code>decodes</code> function that converts the output of the model to a format similar to the target (here indices). This is used in `Learner.predict` and `Learner.show_results` to decode the predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@log_args\n",
    "@delegates()\n",
    "class LabelSmoothingCrossEntropyFlat(BaseLoss):\n",
    "    \"Same as `LabelSmoothingCrossEntropy`, but flattens input and target.\"\n",
    "    y_int = True\n",
    "    @use_kwargs_dict(keep=True, eps=0.1, reduction='mean')\n",
    "    def __init__(self, *args, axis=-1, **kwargs): super().__init__(LabelSmoothingCrossEntropy, *args, axis=axis, **kwargs)\n",
    "    def activation(self, out): return F.softmax(out, dim=-1)\n",
    "    def decodes(self, out):    return out.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal loss is introduced by Lin et al. in this [paper](https://arxiv.org/pdf/1708.02002.pdf). It's basically cross entropy loss with a focal parameter, `gamma`, that downweights easy to predict observations. In fact, when `gamma = 0` it reduces to cross entropy. In the paper, the authors makes use of another parameter, `alpha`, that rebalances the loss between binary classes. The authors suggest setting `alpha` to 1/frequency of the positive class such that the classes are reblanced. This can also be implemented through the `weight` parameter in pytorch's cross entropy implementation. Therefore, the inclusion of `alpha` is redundant but it is done so for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FocalLoss(Module):\n",
    "    def __init__(self, weight=None, reduction='mean', alpha=0.5, gamma=2):\n",
    "        super().__init__()\n",
    "        self.alpha = torch.tensor([alpha, 1-alpha], device=default_device())\n",
    "        store_attr(\"weight,reduction,gamma\")\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        ce_loss = F.cross_entropy(output, target, weight=self.weight, reduction='none')\n",
    "        # only use alpha to weight classes. `targe`t must be 0s and 1s and not two other \n",
    "        # integers, otherwise this will error out\n",
    "        at = self.alpha.gather(0, target) if output.shape[1] == 2 else tensor([1], device=default_device())\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = at * (1-pt)**self.gamma * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "    \n",
    "    def decodes(self, x):    return x.argmax(dim=self.axis)\n",
    "    def activation(self, x): return F.softmax(x, dim=self.axis)\n",
    "\n",
    "@delegates()\n",
    "class FocalLossFlat(BaseLoss):\n",
    "    y_int = True\n",
    "    @use_kwargs_dict(keep=True, weight=None, ignore_index=-100, reduction='mean')\n",
    "    def __init__(self, *args, axis=-1, **kwargs): \n",
    "        super().__init__(FocalLoss, *args, axis=axis, **kwargs)\n",
    "    def decodes(self, x):    return x.argmax(dim=self.axis)\n",
    "    def activation(self, x): return F.softmax(x, dim=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi-class with gamma = 0. Should equal cross entropy\n",
    "fl = FocalLossFlat(gamma=0.)\n",
    "ce = CrossEntropyLossFlat()\n",
    "output = torch.randn(32, 5, 10)\n",
    "target = torch.randint(0, 10, (32, 5))\n",
    "test_close(fl(output, target), ce(output, target))\n",
    "#binary with gamma = 0 and alpha = 0.5. Should equal cross entropy / 2\n",
    "fl = FocalLossFlat(gamma=0., alpha=0.5)\n",
    "output = torch.randn(32, 5, 2)\n",
    "target = torch.randint(0, 2, (32, 5))\n",
    "test_close(fl(output, target), ce(output, target) / 2)\n",
    "#Associated activation is softmax\n",
    "test_eq(fl.activation(output), F.softmax(output, dim=-1))\n",
    "#This loss function has a decodes which is argmax\n",
    "test_eq(fl.decodes(output), output.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
