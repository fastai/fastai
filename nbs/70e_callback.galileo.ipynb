{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp callback.galileo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import os\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from torch.nn import Module\n",
    "from typing import Any, Callable, Dict, List\n",
    "\n",
    "from fastai.learner import Callback\n",
    "from fastai.data.load import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Galileo\n",
    "\n",
    "> Integration with [Galileo](https://docs.rungalileo.io/integrations/fastai) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Galileo on your dataset need to install the dataquality client and register your account.\n",
    "```\n",
    "pip install dataquality\n",
    "```\n",
    "\n",
    "\n",
    "1. Create account: [rungalileo.io](https://console.cloud.rungalileo.io/sign-up).\n",
    "2. Grab your [token](https://console.cloud.rungalileo.io/get-token). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import dataquality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class _PatchDLGetIdxs:\n",
    "    \"\"\"\n",
    "    Patch the DataLoader to store the indices of the batches.\n",
    "    For example:\n",
    "    self.dl.get_idxs = _PatchDLGetIdxs(self.dl.get_idxs, self.idx_log)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, old_func: Callable, store: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Patch the DataLoader to store the indices of the batches.\n",
    "        For example:\n",
    "        self.dl.get_idxs = IdxLogPatch(self.dl.get_idxs, self.idx_log)\n",
    "        :param old_func: The original function to patch.\n",
    "        :param store: The store to store the indices in.\n",
    "        \"\"\"\n",
    "        self.old_func = old_func\n",
    "        self.store = store\n",
    "        self.store[\"dataloader_indices\"] = []\n",
    "\n",
    "    def __call__(self, *args: Any, **kwargs: Any) -> Any:\n",
    "        \"\"\"\n",
    "        Call the original function and store the indices.\n",
    "        :param args: The arguments to pass to the original function.\n",
    "        :param kwargs: The keyword arguments to pass to the original function.\n",
    "\n",
    "        \"\"\"\n",
    "        res = self.old_func(*args, **kwargs)\n",
    "        if res:\n",
    "            self.store[\"dataloader_indices\"].append(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class DataqualityCallback(Callback):\n",
    "    \"\"\"\n",
    "    Dataquality logs the model embeddings and logtis to measure the quality of the dataset.\n",
    "    Provide the label names and the classifier layer to log the embeddings and logits.\n",
    "    If no classifier layer is provided, the last layer of the model will be used.\n",
    "    Here is how to take the last layer of the model:\n",
    "    dqc = DataqualityCallback(labels=['negative','positive'], layer=model.fc)\n",
    "    \"\"\"\n",
    "\n",
    "    hook = None\n",
    "    is_initialized = False\n",
    "    labels = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        labels: List[str],\n",
    "        layer: Any = None,\n",
    "        log_dataset: bool = True,\n",
    "        task_type: str = \"image_classification\",\n",
    "        options: Dict[str, Any] = {},\n",
    "        *args: Any,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Dataquality logs the model embeddings and logits to measure the quality\n",
    "        of the dataset. This helps to find mislabelled samples in a data centric approach.\n",
    "        :param layer: Classifier layer with embeddings as input and logits as output.\n",
    "        :param log_dataset: Enable automatic extraction of the dataset to data quality.\n",
    "        :param disable_dq: Disable data quality logging.\n",
    "        :param args: The arguments to pass to the super class.\n",
    "        :param kwargs: The keyword arguments to pass to the super class.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.disable_dq = os.environ.get(\"DQ_NOOP\", False)\n",
    "        self.labels = labels\n",
    "        self.log_dataset = log_dataset\n",
    "        self.options = options\n",
    "        self.layer = layer\n",
    "        self.model_outputs_log = {}\n",
    "        self.current_idx = []\n",
    "        self.idx_store = {\"idx_queue\": []}\n",
    "        self.logger_config = {}\n",
    "        self.counter = 0\n",
    "        self.options[\"task_type\"] = task_type\n",
    "\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\n",
    "                \"\"\"Labels must be provided. For example: \n",
    "          DataqualityCallback(labels=['negative','positive'])\"\"\"\n",
    "            )\n",
    "\n",
    "        if not self.disable_dq:\n",
    "            dataquality.init(**options)\n",
    "            self.logger_config = dataquality.get_model_logger().logger_config\n",
    "\n",
    "    def get_layer(self) -> Module:\n",
    "        \"\"\"\n",
    "        Get the classifier layer, which inputs and outputs will be logged\n",
    "        (embeddings and logits).\n",
    "        :return: The classifier layer.\n",
    "        \"\"\"\n",
    "        if self.layer is None:\n",
    "            # Get the last layer of the model\n",
    "            return list(list(self.model.children())[-1].children())[-1]\n",
    "        else:\n",
    "            return self.layer\n",
    "\n",
    "    def before_epoch(self) -> None:\n",
    "        if not self.disable_dq:\n",
    "            dataquality.set_epoch(self.epoch)\n",
    "\n",
    "    def before_fit(self) -> None:\n",
    "        if self.is_initialized or self.disable_dq:\n",
    "            return\n",
    "        self.wrap_indices()\n",
    "        self.register_hooks()\n",
    "        self.log_data()\n",
    "        self.is_initialized = True\n",
    "\n",
    "    def before_train(self) -> None:\n",
    "        \"\"\"\n",
    "        Sets the split in data quality and registers the classifier layer hook.\n",
    "        \"\"\"\n",
    "        if self.disable_dq:\n",
    "            return\n",
    "        dataquality.set_split(dataquality.schemas.split.Split.train)\n",
    "        self.wrap_indices()\n",
    "        if self.is_initialized:\n",
    "            return\n",
    "        self.register_hooks()\n",
    "        self.log_data()\n",
    "        self.is_initialized = True\n",
    "\n",
    "    def log_data(self):\n",
    "        \"\"\"\n",
    "        Log datasets to dataquality\n",
    "        \"\"\"\n",
    "        if self.disable_dq or not self.log_dataset:\n",
    "            return\n",
    "\n",
    "        dataquality.set_labels_for_run(self.labels)\n",
    "        num_datasets = self.dls.n_subsets\n",
    "        train_dl, valid_dl, test_dl = None, None, None\n",
    "        if num_datasets == 1:\n",
    "            train_dl = self.dls\n",
    "        elif num_datasets == 2:\n",
    "            train_dl, valid_dl = self.dls\n",
    "        elif num_datasets == 3:\n",
    "            train_dl, valid_dl, test_dl = self.dls\n",
    "        if self.options.get(\"task_type\") == \"image_classification\":\n",
    "            if train_dl is not None:\n",
    "                dataquality.log_image_dataset(\n",
    "                    self.convert_img_dl_to_df(train_dl),\n",
    "                    imgs_colname=\"image\",\n",
    "                    imgs_location_colname=\"path\",\n",
    "                    split=dataquality.schemas.split.Split.training,\n",
    "                )\n",
    "            if valid_dl is not None:\n",
    "                dataquality.log_image_dataset(\n",
    "                    self.convert_img_dl_to_df(valid_dl),\n",
    "                    imgs_colname=\"image\",\n",
    "                    imgs_location_colname=\"path\",\n",
    "                    split=dataquality.schemas.split.Split.validation,\n",
    "                )\n",
    "        else:\n",
    "            if train_dl is not None:\n",
    "                dataquality.log_dataset(\n",
    "                    self.convert_tab_dl_to_df(train_dl),\n",
    "                    split=dataquality.schemas.split.Split.training,\n",
    "                )\n",
    "            if valid_dl is not None:\n",
    "                dataquality.log_dataset(\n",
    "                    self.convert_tab_dl_to_df(valid_dl),\n",
    "                    split=dataquality.schemas.split.Split.validation,\n",
    "                )\n",
    "\n",
    "    def wrap_indices(self) -> None:\n",
    "        \"\"\"\n",
    "        Wraps the get_idxs function of the dataloader to store the indices.\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"dl\"):\n",
    "            return\n",
    "        if not isinstance(self.dl.get_idxs, _PatchDLGetIdxs):\n",
    "            self.dl.get_idxs = _PatchDLGetIdxs(self.dl.get_idxs, self.idx_store)\n",
    "\n",
    "    def after_validate(self):\n",
    "        dataquality.set_split(dataquality.schemas.split.Split.train)\n",
    "\n",
    "    def before_validate(self):\n",
    "        \"\"\"\n",
    "        Sets the split in data quality and registers the classifier layer hook.\n",
    "        \"\"\"\n",
    "        self.wrap_indices()\n",
    "        if self.disable_dq:\n",
    "            return\n",
    "        self.disable_dq = True\n",
    "        dataquality.set_split(dataquality.schemas.split.Split.validation)\n",
    "        self.idx_store[\"idx_queue\"] = []\n",
    "\n",
    "    def after_fit(self):\n",
    "        \"\"\"\n",
    "        Uploads data to galileo and removes the classifier layer hook.\n",
    "        \"\"\"\n",
    "        if (self.n_epoch - 1) == self.epoch:\n",
    "            self.counter += 1\n",
    "\n",
    "        if self.counter != 2:\n",
    "            return\n",
    "        print(\"Finishing dataquality\")\n",
    "        try:\n",
    "            self.h.remove()\n",
    "        except Exception:\n",
    "            pass\n",
    "        dataquality.finish()\n",
    "\n",
    "    def before_batch(self):\n",
    "        \"\"\"\n",
    "        Clears the model outputs log.\n",
    "        \"\"\"\n",
    "        self.model_outputs_log.clear()\n",
    "\n",
    "    # def after_batch(self) -> None:\n",
    "    def after_pred(self) -> None:\n",
    "        \"\"\"\n",
    "        Logs the model outputs.\n",
    "        \"\"\"\n",
    "        # Get the current batch size\n",
    "        bs_len = len(self.model_outputs_log[\"model_output\"])\n",
    "        # Store the current batch ids by trimming the stored ids by\n",
    "        # the batch size length\n",
    "        indices = self.idx_store[\"dataloader_indices\"][-1][:bs_len].copy()\n",
    "        idx_store = self.idx_store[\"dataloader_indices\"][-1][bs_len:]\n",
    "        self.idx_store[\"dataloader_indices\"][-1] = idx_store\n",
    "        try:\n",
    "            cur_split = self.logger_config.cur_split\n",
    "            if cur_split:\n",
    "                cur_split.lower()\n",
    "            else:\n",
    "                print(\"Callback could not detect split\")\n",
    "            id_map = self.logger_config.idx_to_id_map[cur_split]\n",
    "            ids = np.array([id_map[i] for i in indices])\n",
    "        except Exception as e:\n",
    "            print(\"cur_split error\", cur_split, e)\n",
    "            return\n",
    "        # Log the model outputs\n",
    "        embs = self.model_outputs_log[\"model_input\"][0].detach().cpu().numpy()\n",
    "        logits = self.model_outputs_log[\"model_output\"].detach().cpu().numpy()\n",
    "        equal_len = len(embs) == len(logits) and len(ids) == len(embs)\n",
    "        if not equal_len:\n",
    "            print(\n",
    "                f\"length not equal. logits: {len(logits)},ids: {len(ids)},\\\n",
    "embs: {len(embs)}\"\n",
    "            )\n",
    "        if self.disable_dq or not equal_len:\n",
    "            return\n",
    "        dataquality.log_model_outputs(embs=embs, logits=logits, ids=ids)\n",
    "\n",
    "    def register_hooks(self) -> None:\n",
    "        \"\"\"\n",
    "        Registers the classifier layer hook.\n",
    "        \"\"\"\n",
    "        h = None\n",
    "        if not self.hook:\n",
    "            forward_hook = partial(self.forward_hook_with_store, self.model_outputs_log)\n",
    "            h = self.get_layer().register_forward_hook(forward_hook)\n",
    "            self.hook = h\n",
    "        return h\n",
    "\n",
    "    def forward_hook_with_store(\n",
    "        self, store: Dict[str, Any], layer: Module, model_input: Any, model_output: Any\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Forward hook to store the output of a layer.\n",
    "        :param store: Dictionary to store the output in.\n",
    "        :param layer: Layer to store the output of.\n",
    "        :param model_input: Input to the model.\n",
    "        :param model_output: Output of the model.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        store[\"model_input\"] = model_input\n",
    "        store[\"model_output\"] = model_output\n",
    "\n",
    "    def convert_img_dl_to_df(\n",
    "        self, dl: DataLoader, x_col: str = \"image\"\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts a fastai DataLoader to a pandas DataFrame.\n",
    "        :param dl: Fast ai DataLoader to convert.\n",
    "        :param x_col: Name of the column to use for the x values, for example image.\n",
    "        :return: Pandas DataFrame with the data from the DataLoader.\n",
    "        \"\"\"\n",
    "        additional_data = {}\n",
    "        if x_col == \"image\":\n",
    "            additional_data[\"path\"] = dl.items\n",
    "        x, y = [], []\n",
    "        for x_item, y_item in dl.dataset:\n",
    "            x.append(x_item)\n",
    "            y.append(int(y_item))\n",
    "        ids = dl.vocab.o2i.keys()\n",
    "        if len(ids) == 2 and isinstance(next(iter(ids)), bool):\n",
    "            ids = dl.dataset.splits[dl.dataset.split_idx]\n",
    "        df = pd.DataFrame({\"id\": ids, x_col: x, \"label\": y, **additional_data})\n",
    "        del additional_data, x, y\n",
    "        return df\n",
    "\n",
    "    def convert_tab_dl_to_df(\n",
    "        self, dl: DataLoader, x_col: str = \"text\", y_col=\"label\"\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts a fastai DataLoader to a pandas DataFrame.\n",
    "        :param dl: Fast ai DataLoader to convert.\n",
    "        :param x_col: Name of the column to use for the x values, for example text.\n",
    "        :param y_col: Name of the column to use for the y values, for example label.\n",
    "        :return: Pandas DataFrame with the data from the DataLoader.\n",
    "        \"\"\"\n",
    "        df = dl.items.copy()\n",
    "        rename_options = {}\n",
    "        if \"x_col\" != \"text\":\n",
    "            rename_options[x_col] = \"text\"\n",
    "        if \"y_col\" != \"text\":\n",
    "            rename_options[x_col] = \"label\"\n",
    "        if rename_options:\n",
    "            df = df.rename(columns=rename_options)\n",
    "        if \"id\" not in df.columns:\n",
    "            df[\"id\"] = df.index\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DataqualityCallback\n",
       "\n",
       ">      DataqualityCallback (labels:List[str], layer:Any=None,\n",
       ">                           log_dataset:bool=True,\n",
       ">                           task_type:str='image_classification',\n",
       ">                           options:Dict[str,Any]={}, *args:Any, **kwargs:Any)\n",
       "\n",
       "Dataquality logs the model embeddings and logtis to measure the quality of the dataset.\n",
       "Provide the label names and the classifier layer to log the embeddings and logits.\n",
       "If no classifier layer is provided, the last layer of the model will be used.\n",
       "Here is how to take the last layer of the model:\n",
       "dqc = DataqualityCallback(labels=['negative','positive'], layer=model.fc)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DataqualityCallback\n",
       "\n",
       ">      DataqualityCallback (labels:List[str], layer:Any=None,\n",
       ">                           log_dataset:bool=True,\n",
       ">                           task_type:str='image_classification',\n",
       ">                           options:Dict[str,Any]={}, *args:Any, **kwargs:Any)\n",
       "\n",
       "Dataquality logs the model embeddings and logtis to measure the quality of the dataset.\n",
       "Provide the label names and the classifier layer to log the embeddings and logits.\n",
       "If no classifier layer is provided, the last layer of the model will be used.\n",
       "Here is how to take the last layer of the model:\n",
       "dqc = DataqualityCallback(labels=['negative','positive'], layer=model.fc)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DataqualityCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "_all_ = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev import *\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
