{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.test import *\n",
    "from fastai2.data.all import *\n",
    "from fastai2.optimizer import *\n",
    "from fastai2.learner import *\n",
    "from fastai2.metrics import *\n",
    "from fastai2.text.core import *\n",
    "from fastai2.text.data import *\n",
    "from fastai2.text.models.core import *\n",
    "from fastai2.text.models.awdlstm import *\n",
    "from fastai2.callback.rnn import *\n",
    "from fastai2.callback.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp text.learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text learner\n",
    "\n",
    "> Convenience functions to easily create a `Learner` for text applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def match_embeds(old_wgts, old_vocab, new_vocab):\n",
    "    \"Convert the embedding in `wgts` to go with a new vocabulary.\"\n",
    "    bias, wgts = old_wgts.get('1.decoder.bias', None), old_wgts['0.encoder.weight']\n",
    "    wgts_m = wgts.mean(0)\n",
    "    new_wgts = wgts.new_zeros((len(new_vocab),wgts.size(1)))\n",
    "    if bias is not None:\n",
    "        bias_m = bias.mean(0)\n",
    "        new_bias = bias.new_zeros((len(new_vocab),))\n",
    "    old_o2i = old_vocab.o2i if hasattr(old_vocab, 'o2i') else {w:i for i,w in enumerate(old_vocab)}\n",
    "    for i,w in enumerate(new_vocab):\n",
    "        idx = old_o2i.get(w, -1)\n",
    "        new_wgts[i] = wgts[idx] if idx>=0 else wgts_m\n",
    "        if bias is not None: new_bias[i] = bias[idx] if idx>=0 else bias_m\n",
    "    old_wgts['0.encoder.weight'] = new_wgts\n",
    "    if '0.encoder_dp.emb.weight' in old_wgts: old_wgts['0.encoder_dp.emb.weight'] = new_wgts.clone()\n",
    "    old_wgts['1.decoder.weight'] = new_wgts.clone()\n",
    "    if bias is not None: old_wgts['1.decoder.bias'] = new_bias\n",
    "    return old_wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = {'0.encoder.weight': torch.randn(5,3)}\n",
    "new_wgts = match_embeds(wgts.copy(), ['a', 'b', 'c'], ['a', 'c', 'd', 'b'])\n",
    "old,new = wgts['0.encoder.weight'],new_wgts['0.encoder.weight']\n",
    "test_eq(new[0], old[0])\n",
    "test_eq(new[1], old[2])\n",
    "test_eq(new[2], old.mean(0))\n",
    "test_eq(new[3], old[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With bias\n",
    "wgts = {'0.encoder.weight': torch.randn(5,3), '1.decoder.bias': torch.randn(5)}\n",
    "new_wgts = match_embeds(wgts.copy(), ['a', 'b', 'c'], ['a', 'c', 'd', 'b'])\n",
    "old_w,new_w = wgts['0.encoder.weight'],new_wgts['0.encoder.weight']\n",
    "old_b,new_b = wgts['1.decoder.bias'],  new_wgts['1.decoder.bias']\n",
    "test_eq(new_w[0], old_w[0])\n",
    "test_eq(new_w[1], old_w[2])\n",
    "test_eq(new_w[2], old_w.mean(0))\n",
    "test_eq(new_w[3], old_w[1])\n",
    "test_eq(new_b[0], old_b[0])\n",
    "test_eq(new_b[1], old_b[2])\n",
    "test_eq(new_b[2], old_b.mean(0))\n",
    "test_eq(new_b[3], old_b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(Learner.__init__)\n",
    "class RNNLearner(Learner):\n",
    "    \"Basic class for a `Learner` in NLP.\"\n",
    "    def __init__(self, model, dbunch, loss_func, alpha=2., beta=1., **kwargs):\n",
    "        super().__init__(model, dbunch, loss_func, **kwargs)\n",
    "        self.add_cb(RNNTrainer(alpha=alpha, beta=beta))\n",
    "\n",
    "    def save_encoder(self, file):\n",
    "        \"Save the encoder to `self.path/self.model_dir/file`\"\n",
    "        if rank_distrib(): return # don't save if slave proc\n",
    "        encoder = get_model(self.model)[0]\n",
    "        if hasattr(encoder, 'module'): encoder = encoder.module\n",
    "        torch.save(encoder.state_dict(), join_path_file(file,self.path/self.model_dir, ext='.pth'))\n",
    "\n",
    "    def load_encoder(self, file, device=None):\n",
    "        \"Load the encoder `name` from the model directory.\"\n",
    "        encoder = get_model(self.model)[0]\n",
    "        if device is None: device = self.dbunch.device\n",
    "        if hasattr(encoder, 'module'): encoder = encoder.module\n",
    "        distrib_barrier()\n",
    "        encoder.load_state_dict(torch.load(join_path_file(file,self.path/self.model_dir, ext='.pth'), map_location=device))\n",
    "        self.freeze()\n",
    "        return self\n",
    "\n",
    "    #TODO: When access is easier, grab new_vocab from self.dbunch\n",
    "    def load_pretrained(self, wgts_fname, vocab_fname, new_vocab, strict=True):\n",
    "        \"Load a pretrained model and adapt it to the data vocabulary.\"\n",
    "        old_vocab = Path(vocab_fname).load()\n",
    "        wgts = torch.load(wgts_fname, map_location = lambda storage,loc: storage)\n",
    "        if 'model' in wgts: wgts = wgts['model'] #Just in case the pretrained model was saved with an optimizer\n",
    "        wgts = match_embeds(wgts, old_vocab, new_vocab)\n",
    "        self.model.load_state_dict(wgts, strict=strict)\n",
    "        self.freeze()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.text.models.core import _model_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO: When access is easier, grab vocab from dbunch\n",
    "@delegates(Learner.__init__)\n",
    "def language_model_learner(dbunch, arch, vocab, config=None, drop_mult=1., pretrained=True, pretrained_fnames=None, **kwargs):\n",
    "    \"Create a `Learner` with a language model from `data` and `arch`.\"\n",
    "    model = get_language_model(arch, len(vocab), config=config, drop_mult=drop_mult)\n",
    "    meta = _model_meta[arch]\n",
    "    learn = RNNLearner(dbunch, model, loss_func=CrossEntropyLossFlat(), splitter=meta['split_lm'], **kwargs)\n",
    "    #TODO: add backard\n",
    "    #url = 'url_bwd' if data.backwards else 'url'\n",
    "    if pretrained or pretrained_fnames:\n",
    "        if pretrained_fnames is not None:\n",
    "            fnames = [learn.path/learn.model_dir/f'{fn}.{ext}' for fn,ext in zip(pretrained_fnames, ['pth', 'pkl'])]\n",
    "        else:\n",
    "            if 'url' not in meta:\n",
    "                warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "                return learn\n",
    "            model_path = untar_data(meta['url'] , c_key='model')\n",
    "            fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        learn = learn.load_pretrained(*fnames, vocab)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO: When access is easier, grab vocab from dbunch\n",
    "@delegates(Learner.__init__)\n",
    "def text_classifier_learner(dbunch, arch, vocab, bptt=72, config=None, pretrained=True, drop_mult=1.,\n",
    "                            lin_ftrs=None, ps=None, **kwargs):\n",
    "    \"Create a `Learner` with a text classifier from `data` and `arch`.\"\n",
    "    model = get_text_classifier(arch, len(vocab), get_c(dbunch), bptt=bptt, config=config,\n",
    "                                drop_mult=drop_mult, lin_ftrs=lin_ftrs, ps=ps)\n",
    "    meta = _model_meta[arch]\n",
    "    learn = RNNLearner(dbunch, model, loss_func=CrossEntropyLossFlat(), splitter=meta['split_clas'], **kwargs)\n",
    "    if pretrained:\n",
    "        if 'url' not in meta:\n",
    "            warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "            return learn\n",
    "        model_path = untar_data(meta['url'], c_key='model')\n",
    "        fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        learn = learn.load_pretrained(*fnames, vocab, strict=False)\n",
    "        learn.freeze()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#from fastai2.text.data import _get_empty_df\n",
    "from fastai2.core.utils import get_empty_df\n",
    "\n",
    "@typedispatch\n",
    "def show_results(x: LMTensorText, y, samples, outs, ctxs=None, max_n=10, **kwargs):\n",
    "    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n",
    "    for i,l in enumerate(['input', 'target']):\n",
    "        ctxs = [b.show(ctx=c, label=l, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]\n",
    "    ctxs = [b.show(ctx=c, label='pred', **kwargs) for b,c,_ in zip(outs.itemgot(0),ctxs,range(max_n))]\n",
    "    display_df(pd.DataFrame(ctxs))\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x: TensorText, y, samples, outs, ctxs=None, max_n=10, **kwargs):\n",
    "    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n",
    "    ctxs = show_results[object](x, y, samples, outs, ctxs=ctxs, max_n=max_n, **kwargs)\n",
    "    display_df(pd.DataFrame(ctxs))\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def plot_top_losses(x: TensorText, y:TensorCategory, samples, outs, raws, losses, **kwargs):\n",
    "    rows = get_empty_df(len(samples))\n",
    "    for i,l in enumerate(['input', 'target']):\n",
    "        rows = [b.show(ctx=c, label=l, **kwargs) for b,c in zip(samples.itemgot(i),rows)]\n",
    "    outs = L(o + (Float(r.max().item()), Float(l.item())) for o,r,l in zip(outs, raws, losses))\n",
    "    for i,l in enumerate(['predicted', 'probability', 'loss']):\n",
    "        rows = [b.show(ctx=c, label=l, **kwargs) for b,c in zip(outs.itemgot(i),rows)]\n",
    "    display_df(pd.DataFrame(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_test.ipynb.\n",
      "Converted 01_core.ipynb.\n",
      "Converted 01a_utils.ipynb.\n",
      "Converted 01b_dispatch.ipynb.\n",
      "Converted 01c_transform.ipynb.\n",
      "Converted 02_script.ipynb.\n",
      "Converted 03_torch_core.ipynb.\n",
      "Converted 03a_layers.ipynb.\n",
      "Converted 04_dataloader.ipynb.\n",
      "Converted 05_data_core.ipynb.\n",
      "Converted 06_data_transforms.ipynb.\n",
      "Converted 07_data_block.ipynb.\n",
      "Converted 08_vision_core.ipynb.\n",
      "Converted 09_vision_augment.ipynb.\n",
      "Converted 09a_vision_data.ipynb.\n",
      "Converted 10_pets_tutorial.ipynb.\n",
      "Converted 11_vision_models_xresnet.ipynb.\n",
      "Converted 12_optimizer.ipynb.\n",
      "Converted 13_learner.ipynb.\n",
      "Converted 13a_metrics.ipynb.\n",
      "Converted 14_callback_schedule.ipynb.\n",
      "Converted 14a_callback_data.ipynb.\n",
      "Converted 15_callback_hook.ipynb.\n",
      "Converted 15a_vision_models_unet.ipynb.\n",
      "Converted 16_callback_progress.ipynb.\n",
      "Converted 17_callback_tracker.ipynb.\n",
      "Converted 18_callback_fp16.ipynb.\n",
      "Converted 19_callback_mixup.ipynb.\n",
      "Converted 20_interpret.ipynb.\n",
      "Converted 21_vision_learner.ipynb.\n",
      "Converted 22_tutorial_imagenette.ipynb.\n",
      "Converted 23_tutorial_transfer_learning.ipynb.\n",
      "Converted 30_text_core.ipynb.\n",
      "Converted 31_text_data.ipynb.\n",
      "Converted 32_text_models_awdlstm.ipynb.\n",
      "Converted 33_text_models_core.ipynb.\n",
      "Converted 34_callback_rnn.ipynb.\n",
      "Converted 35_tutorial_wikitext.ipynb.\n",
      "Converted 36_text_models_qrnn.ipynb.\n",
      "Converted 37_text_learner.ipynb.\n",
      "Converted 38_tutorial_ulmfit.ipynb.\n",
      "Converted 40_tabular_core.ipynb.\n",
      "Converted 41_tabular_model.ipynb.\n",
      "Converted 42_tabular_rapids.ipynb.\n",
      "Converted 50_data_block_examples.ipynb.\n",
      "Converted 60_medical_imaging.ipynb.\n",
      "Converted 65_medical_text.ipynb.\n",
      "Converted 90_notebook_core.ipynb.\n",
      "Converted 91_notebook_export.ipynb.\n",
      "Converted 92_notebook_showdoc.ipynb.\n",
      "Converted 93_notebook_export2html.ipynb.\n",
      "Converted 94_notebook_test.ipynb.\n",
      "Converted 95_index.ipynb.\n",
      "Converted 96_data_external.ipynb.\n",
      "Converted 97_utils_test.ipynb.\n",
      "Converted notebook2jekyll.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
