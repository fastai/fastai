{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.basics import *\n",
    "from fastai2.text.core import *\n",
    "from fastai2.text.data import *\n",
    "from fastai2.text.models.core import *\n",
    "from fastai2.text.models.awdlstm import *\n",
    "from fastai2.callback.rnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp text.learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text learner\n",
    "\n",
    "> Convenience functions to easily create a `Learner` for text applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def match_embeds(old_wgts, old_vocab, new_vocab):\n",
    "    \"Convert the embedding in `wgts` to go with a new vocabulary.\"\n",
    "    bias, wgts = old_wgts.get('1.decoder.bias', None), old_wgts['0.encoder.weight']\n",
    "    wgts_m = wgts.mean(0)\n",
    "    new_wgts = wgts.new_zeros((len(new_vocab),wgts.size(1)))\n",
    "    if bias is not None:\n",
    "        bias_m = bias.mean(0)\n",
    "        new_bias = bias.new_zeros((len(new_vocab),))\n",
    "    old_o2i = old_vocab.o2i if hasattr(old_vocab, 'o2i') else {w:i for i,w in enumerate(old_vocab)}\n",
    "    for i,w in enumerate(new_vocab):\n",
    "        idx = old_o2i.get(w, -1)\n",
    "        new_wgts[i] = wgts[idx] if idx>=0 else wgts_m\n",
    "        if bias is not None: new_bias[i] = bias[idx] if idx>=0 else bias_m\n",
    "    old_wgts['0.encoder.weight'] = new_wgts\n",
    "    if '0.encoder_dp.emb.weight' in old_wgts: old_wgts['0.encoder_dp.emb.weight'] = new_wgts.clone()\n",
    "    old_wgts['1.decoder.weight'] = new_wgts.clone()\n",
    "    if bias is not None: old_wgts['1.decoder.bias'] = new_bias\n",
    "    return old_wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = {'0.encoder.weight': torch.randn(5,3)}\n",
    "new_wgts = match_embeds(wgts.copy(), ['a', 'b', 'c'], ['a', 'c', 'd', 'b'])\n",
    "old,new = wgts['0.encoder.weight'],new_wgts['0.encoder.weight']\n",
    "test_eq(new[0], old[0])\n",
    "test_eq(new[1], old[2])\n",
    "test_eq(new[2], old.mean(0))\n",
    "test_eq(new[3], old[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With bias\n",
    "wgts = {'0.encoder.weight': torch.randn(5,3), '1.decoder.bias': torch.randn(5)}\n",
    "new_wgts = match_embeds(wgts.copy(), ['a', 'b', 'c'], ['a', 'c', 'd', 'b'])\n",
    "old_w,new_w = wgts['0.encoder.weight'],new_wgts['0.encoder.weight']\n",
    "old_b,new_b = wgts['1.decoder.bias'],  new_wgts['1.decoder.bias']\n",
    "test_eq(new_w[0], old_w[0])\n",
    "test_eq(new_w[1], old_w[2])\n",
    "test_eq(new_w[2], old_w.mean(0))\n",
    "test_eq(new_w[3], old_w[1])\n",
    "test_eq(new_b[0], old_b[0])\n",
    "test_eq(new_b[1], old_b[2])\n",
    "test_eq(new_b[2], old_b.mean(0))\n",
    "test_eq(new_b[3], old_b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_text_vocab(dls):\n",
    "    vocab = dls.vocab\n",
    "    if isinstance(vocab, L): vocab = vocab[0]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_ignore_keys(model, wgts):\n",
    "    \"Load `wgts` in `model` ignoring the names of the keys, just taking parameters in order\"\n",
    "    sd = model.state_dict()\n",
    "    for k1,k2 in zip(sd.keys(), wgts.keys()): sd[k1].data = wgts[k2].data.clone()\n",
    "    return model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(Learner.__init__)\n",
    "class TextLearner(Learner):\n",
    "    \"Basic class for a `Learner` in NLP.\"\n",
    "    def __init__(self, model, dls, alpha=2., beta=1., moms=(0.8,0.7,0.8), **kwargs):\n",
    "        super().__init__(model, dls, moms=moms, **kwargs)\n",
    "        self.add_cbs([ModelReseter(), RNNRegularizer(alpha=alpha, beta=beta)])\n",
    "\n",
    "    def save_encoder(self, file):\n",
    "        \"Save the encoder to `self.path/self.model_dir/file`\"\n",
    "        if rank_distrib(): return # don't save if slave proc\n",
    "        encoder = get_model(self.model)[0]\n",
    "        if hasattr(encoder, 'module'): encoder = encoder.module\n",
    "        torch.save(encoder.state_dict(), join_path_file(file,self.path/self.model_dir, ext='.pth'))\n",
    "\n",
    "    def load_encoder(self, file, device=None):\n",
    "        \"Load the encoder `name` from the model directory.\"\n",
    "        encoder = get_model(self.model)[0]\n",
    "        if device is None: device = self.dls.device\n",
    "        if hasattr(encoder, 'module'): encoder = encoder.module\n",
    "        distrib_barrier()\n",
    "        encoder.load_state_dict(torch.load(join_path_file(file,self.path/self.model_dir, ext='.pth'), map_location=device))\n",
    "        self.freeze()\n",
    "        return self\n",
    "\n",
    "    def load_pretrained(self, wgts_fname, vocab_fname, model=None):\n",
    "        \"Load a pretrained model and adapt it to the data vocabulary.\"\n",
    "        old_vocab = Path(vocab_fname).load()\n",
    "        new_vocab = _get_text_vocab(self.dls)\n",
    "        wgts = torch.load(wgts_fname, map_location = lambda storage,loc: storage)\n",
    "        if 'model' in wgts: wgts = wgts['model'] #Just in case the pretrained model was saved with an optimizer\n",
    "        wgts = match_embeds(wgts, old_vocab, new_vocab)\n",
    "        load_ignore_keys(self.model if model is None else model, wgts)\n",
    "        self.freeze()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def decode_spec_tokens(tokens):\n",
    "    new_toks,rule,arg = [],None,None\n",
    "    for t in tokens:\n",
    "        if t in [TK_MAJ, TK_UP, TK_REP, TK_WREP]: rule = t\n",
    "        elif rule is None: new_toks.append(t)\n",
    "        elif rule == TK_MAJ:\n",
    "            new_toks.append(t[:1].upper() + t[1:].lower())\n",
    "            rule = None\n",
    "        elif rule == TK_UP:\n",
    "            new_toks.append(t.upper())\n",
    "            rule = None\n",
    "        elif arg is None:\n",
    "            try:    arg = int(t)\n",
    "            except: rule = None\n",
    "        else:\n",
    "            if rule == TK_REP: new_toks.append(t * arg)\n",
    "            else:              new_toks += [t] * arg\n",
    "    return new_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LMLearner(TextLearner):\n",
    "    \"Add functionality to `TextLearner` when dealingwith a language model\"\n",
    "    @delegates(tokenize1)\n",
    "    def predict(self, text, n_words=1, no_unk=True, temperature=1., min_p=None, rm_type_tfms=0, no_bar=False,\n",
    "                decoder=decode_spec_tokens, **kwargs):\n",
    "        \"Return `text` and the `n_words` that come after\"\n",
    "        self.model.reset()\n",
    "        tokens = tokenize1(text, **kwargs)\n",
    "        tfm = self.dls.train_ds.numericalize\n",
    "        idxs = tfm(tokens).to(self.dls.device)\n",
    "        if no_unk: unk_idx = self.dls.vocab.index(UNK)\n",
    "        for _ in (range(n_words) if no_bar else progress_bar(range(n_words), leave=False)):\n",
    "            with self.no_bar(): preds,_ = self.get_preds(dl=[(idxs[None],)])\n",
    "            res = preds[0][-1]\n",
    "            if no_unk: res[unk_idx] = 0.\n",
    "            if min_p is not None:\n",
    "                if (res >= min_p).float().sum() == 0:\n",
    "                    warn(f\"There is no item with probability >= {min_p}, try a lower value.\")\n",
    "                else: res[res < min_p] = 0.\n",
    "            if temperature != 1.: res.pow_(1 / temperature)\n",
    "            idx = torch.multinomial(res, 1).item()\n",
    "            idxs = torch.cat([idxs, idxs.new([idx])])\n",
    "\n",
    "        tokens = [tfm.vocab[i] for i in idxs if tfm.vocab[i] not in [BOS, PAD]]\n",
    "        sep = self.dls.train_ds.tokenizer\n",
    "        return sep.join(decoder(tokens))\n",
    "\n",
    "    @delegates(Learner.get_preds)\n",
    "    def get_preds(self, concat_dim=1, **kwargs): return super().get_preds(concat_dim=1, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.text.models.core import _model_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_text_vocab(dls):\n",
    "    vocab = dls.vocab\n",
    "    if isinstance(vocab, L): vocab = vocab[0]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(Learner.__init__)\n",
    "def language_model_learner(dls, arch, config=None, drop_mult=1., pretrained=True, pretrained_fnames=None, **kwargs):\n",
    "    \"Create a `Learner` with a language model from `data` and `arch`.\"\n",
    "    vocab = _get_text_vocab(dls)\n",
    "    model = get_language_model(arch, len(vocab), config=config, drop_mult=drop_mult)\n",
    "    meta = _model_meta[arch]\n",
    "    learn = LMLearner(dls, model, loss_func=CrossEntropyLossFlat(), splitter=meta['split_lm'], **kwargs)\n",
    "    #TODO: add backard\n",
    "    #url = 'url_bwd' if data.backwards else 'url'\n",
    "    if pretrained or pretrained_fnames:\n",
    "        if pretrained_fnames is not None:\n",
    "            fnames = [learn.path/learn.model_dir/f'{fn}.{ext}' for fn,ext in zip(pretrained_fnames, ['pth', 'pkl'])]\n",
    "        else:\n",
    "            if 'url' not in meta:\n",
    "                warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "                return learn\n",
    "            model_path = untar_data(meta['url'] , c_key='model')\n",
    "            fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        learn = learn.load_pretrained(*fnames)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(Learner.__init__)\n",
    "def text_classifier_learner(dls, arch, seq_len=72, config=None, pretrained=True, drop_mult=0.5, n_out=None,\n",
    "                            lin_ftrs=None, ps=None, max_len=72*20, **kwargs):\n",
    "    \"Create a `Learner` with a text classifier from `data` and `arch`.\"\n",
    "    vocab = _get_text_vocab(dls)\n",
    "    if n_out is None: n_out = get_c(dls)\n",
    "    assert n_out, \"`n_out` is not defined, and could not be infered from data, set `dls.c` or pass `n_out`\"\n",
    "    model = get_text_classifier(arch, len(vocab), n_out, seq_len=seq_len, config=config,\n",
    "                                drop_mult=drop_mult, lin_ftrs=lin_ftrs, ps=ps, max_len=max_len)\n",
    "    meta = _model_meta[arch]\n",
    "    learn = TextLearner(dls, model, splitter=meta['split_clas'], **kwargs)\n",
    "    if pretrained:\n",
    "        if 'url' not in meta:\n",
    "            warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "            return learn\n",
    "        model_path = untar_data(meta['url'], c_key='model')\n",
    "        fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        learn = learn.load_pretrained(*fnames, model=learn.model[0])\n",
    "        learn.freeze()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x: LMTensorText, y, samples, outs, ctxs=None, max_n=10, **kwargs):\n",
    "    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n",
    "    for i,l in enumerate(['input', 'target']):\n",
    "        ctxs = [b.show(ctx=c, label=l, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]\n",
    "    ctxs = [b.show(ctx=c, label='pred', **kwargs) for b,c,_ in zip(outs.itemgot(0),ctxs,range(max_n))]\n",
    "    display_df(pd.DataFrame(ctxs))\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x: TensorText, y, samples, outs, ctxs=None, max_n=10, trunc_at=150, **kwargs):\n",
    "    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n",
    "    samples = L((s[0].truncate(trunc_at),*s[1:]) for s in samples)\n",
    "    ctxs = show_results[object](x, y, samples, outs, ctxs=ctxs, max_n=max_n, **kwargs)\n",
    "    display_df(pd.DataFrame(ctxs))\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def plot_top_losses(x: TensorText, y:TensorCategory, samples, outs, raws, losses, trunc_at=150, **kwargs):\n",
    "    rows = get_empty_df(len(samples))\n",
    "    samples = L((s[0].truncate(trunc_at),*s[1:]) for s in samples)\n",
    "    for i,l in enumerate(['input', 'target']):\n",
    "        rows = [b.show(ctx=c, label=l, **kwargs) for b,c in zip(samples.itemgot(i),rows)]\n",
    "    outs = L(o + (TitledFloat(r.max().item()), TitledFloat(l.item())) for o,r,l in zip(outs, raws, losses))\n",
    "    for i,l in enumerate(['predicted', 'probability', 'loss']):\n",
    "        rows = [b.show(ctx=c, label=l, **kwargs) for b,c in zip(outs.itemgot(i),rows)]\n",
    "    display_df(pd.DataFrame(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_torch_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_data.load.ipynb.\n",
      "Converted 03_data.core.ipynb.\n",
      "Converted 04_data.external.ipynb.\n",
      "Converted 05_data.transforms.ipynb.\n",
      "Converted 06_data.block.ipynb.\n",
      "Converted 07_vision.core.ipynb.\n",
      "Converted 08_vision.data.ipynb.\n",
      "Converted 09_vision.augment.ipynb.\n",
      "Converted 09b_vision.utils.ipynb.\n",
      "Converted 09c_vision.widgets.ipynb.\n",
      "Converted 10_tutorial.pets.ipynb.\n",
      "Converted 11_vision.models.xresnet.ipynb.\n",
      "Converted 12_optimizer.ipynb.\n",
      "Converted 13_learner.ipynb.\n",
      "Converted 13a_metrics.ipynb.\n",
      "Converted 14_callback.schedule.ipynb.\n",
      "Converted 14a_callback.data.ipynb.\n",
      "Converted 15_callback.hook.ipynb.\n",
      "Converted 15a_vision.models.unet.ipynb.\n",
      "Converted 16_callback.progress.ipynb.\n",
      "Converted 17_callback.tracker.ipynb.\n",
      "Converted 18_callback.fp16.ipynb.\n",
      "Converted 19_callback.mixup.ipynb.\n",
      "Converted 20_interpret.ipynb.\n",
      "Converted 20a_distributed.ipynb.\n",
      "Converted 21_vision.learner.ipynb.\n",
      "Converted 22_tutorial.imagenette.ipynb.\n",
      "Converted 23_tutorial.transfer_learning.ipynb.\n",
      "Converted 24_vision.gan.ipynb.\n",
      "Converted 30_text.core.ipynb.\n",
      "Converted 31_text.data.ipynb.\n",
      "Converted 32_text.models.awdlstm.ipynb.\n",
      "Converted 33_text.models.core.ipynb.\n",
      "Converted 34_callback.rnn.ipynb.\n",
      "Converted 35_tutorial.wikitext.ipynb.\n",
      "Converted 36_text.models.qrnn.ipynb.\n",
      "Converted 37_text.learner.ipynb.\n",
      "Converted 38_tutorial.ulmfit.ipynb.\n",
      "Converted 40_tabular.core.ipynb.\n",
      "Converted 41_tabular.data.ipynb.\n",
      "Converted 42_tabular.model.ipynb.\n",
      "Converted 43_tabular.learner.ipynb.\n",
      "Converted 45_collab.ipynb.\n",
      "Converted 50_datablock_examples.ipynb.\n",
      "Converted 60_medical.imaging.ipynb.\n",
      "Converted 65_medical.text.ipynb.\n",
      "Converted 70_callback.wandb.ipynb.\n",
      "Converted 71_callback.tensorboard.ipynb.\n",
      "Converted 97_test_utils.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
