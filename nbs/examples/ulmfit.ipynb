{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.text.all import *\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFiT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune a pretrained Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get our data and tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = get_files(path, extensions=['.txt'], folders=['unsup', 'train', 'test'])\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we put it in a `DataSource`. For a language model, we don't have targets, so there is only one transform to numericalize the texts. Note that `tokenize_df` returns the count of the words in the corpus to make it easy to create a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(f): return L(f.read().split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = RandomSplitter(valid_pct=0.1)(texts)\n",
    "tfms = [Tokenizer.from_folder(path), Numericalize()]\n",
    "dsrc = DataSource(texts, [tfms], splits=splits, dl_type=LMDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use that `DataSource` to create a `DataBunch`. Here the class of `TfmdDL` we need to use is `LMDataLoader` which will concatenate all the texts in a source (with a shuffle at each epoch for the training set), split it in `bs` chunks then read continuously through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,sl=256,80\n",
    "dbunch_lm = dsrc.databunch(bs=bs, seq_len=sl, val_bs=bs, after_batch=Cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos \" pandora 's xxmaj clock \" is a gripping suspense / thriller that 's a cross between a virus movie and a disaster film . xxmaj this movie , which aired in two parts on xxup nbc in its debut showing in 1996 , is about an airplane flight that becomes infected with a virus when one of the passengers just happens to be carrying this disease . xxmaj the xxup u.s . xxmaj government debates on whether the</td>\n",
       "      <td>\" pandora 's xxmaj clock \" is a gripping suspense / thriller that 's a cross between a virus movie and a disaster film . xxmaj this movie , which aired in two parts on xxup nbc in its debut showing in 1996 , is about an airplane flight that becomes infected with a virus when one of the passengers just happens to be carrying this disease . xxmaj the xxup u.s . xxmaj government debates on whether the plane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>going to be built through xxmaj royston xxmaj vasey . xxmaj consequently , more foreigners visit the small town . xxmaj but xxmaj edward and xxmaj tubbs , the owners of a \" local \" shop , which is actually far away from the town , do not like foreigners . xxmaj whenever a visitor enters their shop , they kill him . xxmaj in my opinion some scenes are kind of tasteless and not funny at all , for</td>\n",
       "      <td>to be built through xxmaj royston xxmaj vasey . xxmaj consequently , more foreigners visit the small town . xxmaj but xxmaj edward and xxmaj tubbs , the owners of a \" local \" shop , which is actually far away from the town , do not like foreigners . xxmaj whenever a visitor enters their shop , they kill him . xxmaj in my opinion some scenes are kind of tasteless and not funny at all , for example</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>about that is in here . ) xxmaj then there 's organ pianist xxmaj billy xxmaj preston who might have officially become the fifth xxmaj beatle had the group not split up some time after this film . xxmaj the real exciting part was the legendary rooftop concert that caused some traffic and had xxmaj paul ad libbing some lines about getting arrested at the end of \" get xxmaj back \" ! xxmaj what a way to end the</td>\n",
       "      <td>that is in here . ) xxmaj then there 's organ pianist xxmaj billy xxmaj preston who might have officially become the fifth xxmaj beatle had the group not split up some time after this film . xxmaj the real exciting part was the legendary rooftop concert that caused some traffic and had xxmaj paul ad libbing some lines about getting arrested at the end of \" get xxmaj back \" ! xxmaj what a way to end the film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frightened when i think of what those two would put me in . xxmaj no way i would let them near me . \\n\\n xxmaj thanks , xxmaj peace to all xxbos xxmaj if you would have asked me 1 month ago how this movie was i probably would have left most of this out , but i am a fan and as any fan i visit the movies sites often well when xxmaj super xxmaj troopers came out i</td>\n",
       "      <td>when i think of what those two would put me in . xxmaj no way i would let them near me . \\n\\n xxmaj thanks , xxmaj peace to all xxbos xxmaj if you would have asked me 1 month ago how this movie was i probably would have left most of this out , but i am a fan and as any fan i visit the movies sites often well when xxmaj super xxmaj troopers came out i visited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are questions xxmaj greenaway left me unmotivated to answer . xxbos xxmaj real cool , smart movie . i loved xxmaj sheedy 's colors , especially the purple car . xxmaj alice xxmaj drummond is xxmaj wise xxmaj and xxmaj wonderful as xxmaj stella . i liked xxmaj sheedy 's reference to how her face had gotten fatter . xxmaj the roadside dance scene is brilliant . xxmaj really liked this one . xxbos xxmaj while it is not a</td>\n",
       "      <td>questions xxmaj greenaway left me unmotivated to answer . xxbos xxmaj real cool , smart movie . i loved xxmaj sheedy 's colors , especially the purple car . xxmaj alice xxmaj drummond is xxmaj wise xxmaj and xxmaj wonderful as xxmaj stella . i liked xxmaj sheedy 's reference to how her face had gotten fatter . xxmaj the roadside dance scene is brilliant . xxmaj really liked this one . xxbos xxmaj while it is not a complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a full length film . xxbos i am a 20 year old bloke from xxmaj england . i do n't cry at films . xxmaj but this one moved me . i cried like a girl ! xxmaj this is absolutely the most moving film i have ever seen , even though the story was questionable . xxmaj joseph xxmaj mazello 's little face when he dreams of crying made me sob every time . xxmaj how could anyone hurt</td>\n",
       "      <td>full length film . xxbos i am a 20 year old bloke from xxmaj england . i do n't cry at films . xxmaj but this one moved me . i cried like a girl ! xxmaj this is absolutely the most moving film i have ever seen , even though the story was questionable . xxmaj joseph xxmaj mazello 's little face when he dreams of crying made me sob every time . xxmaj how could anyone hurt such</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>storyteller . xxmaj his previous work has covered such hard topics like hate crimes , depression , suicide , guilt , etc . xxmaj i 've also seen some of his funnier stuff like his two silent films and it 's clear that this is a filmmaker who likes to work in many different genres and be one of a kind . xxmaj so when i read on his xxmaj myspace page that he was making a horror film ,</td>\n",
       "      <td>. xxmaj his previous work has covered such hard topics like hate crimes , depression , suicide , guilt , etc . xxmaj i 've also seen some of his funnier stuff like his two silent films and it 's clear that this is a filmmaker who likes to work in many different genres and be one of a kind . xxmaj so when i read on his xxmaj myspace page that he was making a horror film , i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>to do listen to orders , even if at the moment , it sounds absurd ( i believe those who have been through service in the armed forces will agree on this ) . xxmaj it is the conflict , and the need to lie through their teeth , which makes it all the more a sorrowful struggle , especially when you have to deny a fellow brother his moment of recognition , and denying his family the need for</td>\n",
       "      <td>do listen to orders , even if at the moment , it sounds absurd ( i believe those who have been through service in the armed forces will agree on this ) . xxmaj it is the conflict , and the need to lie through their teeth , which makes it all the more a sorrowful struggle , especially when you have to deny a fellow brother his moment of recognition , and denying his family the need for closure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>of xxmaj parliament to rectify the anomaly , but until the xxmaj act receives the xxmaj royal xxmaj xxunk the area remains outside the xxmaj united xxmaj kingdom and xxmaj british laws do not apply . \\n\\n xxmaj because xxmaj pimlico is not subject to xxmaj british law , the landlord of the local pub is free to open whatever hours he chooses and local shopkeepers can sell whatever they please to whomever they please , unhindered by the rationing</td>\n",
       "      <td>xxmaj parliament to rectify the anomaly , but until the xxmaj act receives the xxmaj royal xxmaj xxunk the area remains outside the xxmaj united xxmaj kingdom and xxmaj british laws do not apply . \\n\\n xxmaj because xxmaj pimlico is not subject to xxmaj british law , the landlord of the local pub is free to open whatever hours he chooses and local shopkeepers can sell whatever they please to whomever they please , unhindered by the rationing laws</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbunch_lm.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have a convenience method to directly grab a `Learner` from it, using the `AWD_LSTM` architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func = partial(Adam, wd=0.1)\n",
    "learn = language_model_learner(dbunch_lm, AWD_LSTM, vocab, opt_func=opt_func, metrics=[accuracy, Perplexity()], path=path)\n",
    "learn = learn.to_fp16(clip=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.426135</td>\n",
       "      <td>3.984901</td>\n",
       "      <td>0.292371</td>\n",
       "      <td>53.779987</td>\n",
       "      <td>07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.163227</td>\n",
       "      <td>3.870354</td>\n",
       "      <td>0.306840</td>\n",
       "      <td>47.959347</td>\n",
       "      <td>07:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.055693</td>\n",
       "      <td>3.790802</td>\n",
       "      <td>0.316436</td>\n",
       "      <td>44.291908</td>\n",
       "      <td>07:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.979279</td>\n",
       "      <td>3.729021</td>\n",
       "      <td>0.323357</td>\n",
       "      <td>41.638317</td>\n",
       "      <td>07:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.919654</td>\n",
       "      <td>3.688891</td>\n",
       "      <td>0.327770</td>\n",
       "      <td>40.000469</td>\n",
       "      <td>07:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.889432</td>\n",
       "      <td>3.660633</td>\n",
       "      <td>0.330762</td>\n",
       "      <td>38.885933</td>\n",
       "      <td>07:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.842923</td>\n",
       "      <td>3.637397</td>\n",
       "      <td>0.333315</td>\n",
       "      <td>37.992798</td>\n",
       "      <td>07:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.813823</td>\n",
       "      <td>3.619074</td>\n",
       "      <td>0.335308</td>\n",
       "      <td>37.303013</td>\n",
       "      <td>07:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.793213</td>\n",
       "      <td>3.608010</td>\n",
       "      <td>0.336566</td>\n",
       "      <td>36.892574</td>\n",
       "      <td>07:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.766456</td>\n",
       "      <td>3.602140</td>\n",
       "      <td>0.337257</td>\n",
       "      <td>36.676647</td>\n",
       "      <td>07:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.759768</td>\n",
       "      <td>3.600955</td>\n",
       "      <td>0.337450</td>\n",
       "      <td>36.633202</td>\n",
       "      <td>07:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 2e-3, moms=(0.8,0.7,0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have fine-tuned the pretrained language model to this corpus, we save the encoder since we will use it for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('finetuned1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use it to train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = get_files(path, extensions=['.txt'], folders=['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = GrandparentSplitter(valid_name='test')(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, we need to use two set of transforms: one to numericalize the texts and the other to encode the labels as categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfms = [Tokenizer.from_folder(path), Numericalize(vocab=dbunch_lm.vocab)]\n",
    "dsrc = DataSource(texts, [x_tfms, [parent_label, Categorize()]], splits=splits, dl_type=SortedDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = dsrc.databunch(before_batch=pad_input, after_batch=Cuda, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos * * attention xxmaj spoilers * * \\n\\n xxmaj first of all , let me say that xxmaj rob xxmaj roy is one of the best films of the 90 's . xxmaj it was an amazing achievement for all those involved , especially the acting of xxmaj liam xxmaj neeson , xxmaj jessica xxmaj lange , xxmaj john xxmaj hurt , xxmaj brian xxmaj cox , and xxmaj tim xxmaj roth . xxmaj michael xxmaj canton xxmaj jones painted a wonderful portrait of the honor and dishonor that men can represent in themselves . xxmaj but alas â€¦ \\n\\n it constantly , and unfairly gets compared to \" braveheart \" . xxmaj these are two entirely different films , probably only similar in the fact that they are both about xxmaj scots in historical xxmaj scotland . xxmaj yet , this comparison frequently bothers me because it seems</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj by now you 've probably heard a bit about the new xxmaj disney dub of xxmaj miyazaki 's classic film , xxmaj laputa : xxmaj castle xxmaj in xxmaj the xxmaj sky . xxmaj during late summer of 1998 , xxmaj disney released \" kiki 's xxmaj delivery xxmaj service \" on video which included a preview of the xxmaj laputa dub saying it was due out in \" 1 xxrep 3 9 \" . xxmaj it 's obviously way past that year now , but the dub has been finally completed . xxmaj and it 's not \" laputa : xxmaj castle xxmaj in xxmaj the xxmaj sky \" , just \" castle xxmaj in xxmaj the xxmaj sky \" for the dub , since xxmaj laputa is not such a nice word in xxmaj spanish ( even though they use the word xxmaj laputa many times</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbunch.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we once again have a convenience function to create a classifier from this `DataBunch` with the `AWD_LSTM` architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func = partial(Adam, wd=0.1)\n",
    "learn = text_classifier_learner(dbunch, AWD_LSTM, vocab, metrics=[accuracy], path=path, drop_mult=0.5, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our pretrained encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_encoder('finetuned1')\n",
    "learn = learn.to_fp16(clip=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can train with gradual unfreezing and differential learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1 * bs/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.328318</td>\n",
       "      <td>0.200650</td>\n",
       "      <td>0.922120</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr, moms=(0.8,0.7,0.8), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.208120</td>\n",
       "      <td>0.166004</td>\n",
       "      <td>0.937440</td>\n",
       "      <td>01:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-2)\n",
    "lr /= 2\n",
    "learn.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7,0.8), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.162498</td>\n",
       "      <td>0.154959</td>\n",
       "      <td>0.942400</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-3)\n",
    "lr /= 2\n",
    "learn.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7,0.8), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>0.163456</td>\n",
       "      <td>0.940560</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.095326</td>\n",
       "      <td>0.154301</td>\n",
       "      <td>0.945120</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "lr /= 5\n",
    "learn.fit_one_cycle(2, slice(lr/(2.6**4),lr), moms=(0.8,0.7,0.8), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
