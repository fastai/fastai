{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Metrics* for training fastai models are simply functions that take `input` and `target` tensors, and return some metric of interest for training. You can write your own metrics by defining a function of that type, and passing it to [`Learner`](/basic_train.html#Learner) in the [code]metrics[/code] parameter, or use one of the following pre-defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from fastai.gen_doc.nbdoc import *\n",
    "from fastai.basics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"accuracy\"><code>accuracy</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/metrics.py#L25\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>accuracy</code>(<b>`input`</b>:`Tensor`, <b>`targs`</b>:`Tensor`) → `Rank0Tensor`\n",
       "\n",
       "Compute accuracy with `targs` when `input` is bs * n_classes.  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div markdown=\"span\" class=\"alert alert-danger\" role=\"alert\"><i class=\"fa fa-danger-circle\"></i> <b>Warning: </b>This metric is intended for classification of objects belonging to a single class.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jekyll_warn(\"This metric is intended for classification of objects belonging to a single class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"accuracy_thresh\"><code>accuracy_thresh</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/metrics.py#L33\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>accuracy_thresh</code>(<b>`y_pred`</b>:`Tensor`, <b>`y_true`</b>:`Tensor`, <b>`thresh`</b>:`float`=<b><i>`0.5`</i></b>, <b>`sigmoid`</b>:`bool`=<b><i>`True`</i></b>) → `Rank0Tensor`\n",
       "\n",
       "Compute accuracy when `y_pred` and `y_true` are the same size.  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(accuracy_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction are compared to `thresh` after `sigmoid` is maybe applied. Then we count the numbers that match the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div markdown=\"span\" class=\"alert alert-info\" role=\"alert\"><i class=\"fa fa-info-circle\"></i> <b>Note: </b>This function is intended for one-hot-encoded targets (often in a multiclassification problem).</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jekyll_note(\"This function is intended for one-hot-encoded targets (often in a multiclassification problem).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"dice\"><code>dice</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/metrics.py#L50\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>dice</code>(<b>`input`</b>:`Tensor`, <b>`targs`</b>:`Tensor`, <b>`iou`</b>:`bool`=<b><i>`False`</i></b>) → `Rank0Tensor`\n",
       "\n",
       "Dice coefficient metric for binary target. If iou=True, returns iou metric, classic for segmentation problems.  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"error_rate\"><code>error_rate</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/metrics.py#L45\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>error_rate</code>(<b>`input`</b>:`Tensor`, <b>`targs`</b>:`Tensor`) → `Rank0Tensor`\n",
       "\n",
       "1 - [`accuracy`](/metrics.html#accuracy)  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"exp_rmspe\"><code>exp_rmspe</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/metrics.py#L61\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>exp_rmspe</code>(<b>`pred`</b>:`FloatTensor`, <b>`targ`</b>:`FloatTensor`) → `Rank0Tensor`\n",
       "\n",
       "Exp RMSE between `pred` and `targ`.  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(exp_rmspe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"fbeta\"><code>fbeta</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/metrics.py#L12\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>fbeta</code>(<b>`y_pred`</b>:`Tensor`, <b>`y_true`</b>:`Tensor`, <b>`thresh`</b>:`float`=<b><i>`0.2`</i></b>, <b>`beta`</b>:`float`=<b><i>`2`</i></b>, <b>`eps`</b>:`float`=<b><i>`1e-09`</i></b>, <b>`sigmoid`</b>:`bool`=<b><i>`True`</i></b>) → `Rank0Tensor`\n",
       "\n",
       "Computes the f_beta between `preds` and `targets`  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(fbeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`beta` determines the value of the fbeta applied, `eps` is there for numeric stability. If `sigmoid=True`, a sigmoid is applied to the predictions before comparing them to `thresh` then to the targets. See the [F1 score wikipedia page](https://en.wikipedia.org/wiki/F1_score) for details on the fbeta score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div markdown=\"span\" class=\"alert alert-info\" role=\"alert\"><i class=\"fa fa-info-circle\"></i> <b>Note: </b>This function is intended for one-hot-encoded targets (often in a multiclassification problem).</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jekyll_note(\"This function is intended for one-hot-encoded targets (often in a multiclassification problem).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"FBeta\"><code>class</code> <code>FBeta</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/metrics.py#L180\" class=\"source_link\">[source]</a></h3>\n",
       "\n",
       "> <code>FBeta</code>(<b>`n_classes`</b>:`int`=<b><i>`2`</i></b>, <b>`average`</b>:`Optional`\\[`str`\\]=<b><i>`'binary'`</i></b>, <b>`pos_label`</b>:`int`=<b><i>`1`</i></b>, <b>`eps`</b>:`float`=<b><i>`1e-09`</i></b>, <b>`beta`</b>:`float`=<b><i>`2`</i></b>) :: [`CMScores`](/metrics.html#CMScores)\n",
       "\n",
       "FBeta(n_classes: int = 2, average: Union[str, NoneType] = 'binary', pos_label: int = 1, eps: float = 1e-09, beta: float = 2)  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FBeta, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Creating your own metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new metric can be as simple as creating a new function. If you metric is an average over the total number of elements in your dataset, just write the function that will compute it on a batch (taking `pred` and `targ` as arguments). It will then be automatically averaged over the batches (taking their different sizes into acount).\n",
    "\n",
    "Sometimes metrics aren't simple averages however. If we take the example of precision for instance, we have to divide the number of true positives by the number of predictions we made for that class. This isn't an average over the number of elements we have in the dataset, we only consider those where we made a positive prediction for a specific thing. Computing the precision for each batch, then averaging them will yield to a result that may be close to the real value, but won't be it exactly (and it really depends on how you deal with special case of 0 positive predicitions).\n",
    "\n",
    "This why in fastai, every metric is implemented as a callback. If you pass a regular function, the library transforms it to a proper callback called `AverageCallback`. The callback metrics are only called during the validation phase, and only for the following events: \n",
    "- <code>on_epoch_begin</code> (for initialization)\n",
    "- <code>on_batch_begin</code> (if we need to have a look at the input/target and maybe modify them)\n",
    "- <code>on_batch_end</code> (to analyze the last results and update our computation)\n",
    "- <code>on_epoch_end</code>(to wrap up the final result that should be stored in `.metric`)\n",
    "\n",
    "As an example, is here the exact implementation of the [`AverageMetric`](/callback.html#AverageMetric) callback that transforms a function like [`accuracy`](/metrics.html#accuracy) into a metric callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMetric(Callback):\n",
    "    def __init__(self, func):\n",
    "        self.func, self.name = func, func.__name__\n",
    "\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.val, self.count = 0.,0\n",
    "\n",
    "    def on_batch_end(self, last_output, last_target, train, **kwargs):\n",
    "        self.count += last_target.size(0)\n",
    "        self.val += last_target.size(0) * self.func(last_output, last_target).detach().item()\n",
    "\n",
    "    def on_epoch_end(self, **kwargs):\n",
    "        self.metric = self.val/self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is another example that properly computes the precision for a given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Precision(Callback):\n",
    "    \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.correct, self.total = 0, 0\n",
    "    \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        preds = last_output.argmax(1)\n",
    "        self.correct += ((preds==0) * (last_target==0)).float().sum()\n",
    "        self.total += (preds==0).float().sum()\n",
    "    \n",
    "    def on_epoch_end(self, **kwargs):\n",
    "        self.metric = self.correct/self.total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undocumented Methods - Methods moved below this line will intentionally be hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ConfusionMatrix.on_batch_end\"><code>on_batch_end</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/metrics.py#L116\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>on_batch_end</code>(<b>`last_output`</b>:`Tensor`, <b>`last_target`</b>:`Tensor`, <b>`kwargs`</b>)\n",
       "\n",
       "Called at the end of the batch.  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FBeta.on_batch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ConfusionMatrix.on_epoch_begin\"><code>on_epoch_begin</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/metrics.py#L113\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>on_epoch_begin</code>(<b>`kwargs`</b>)\n",
       "\n",
       "At the beginning of each epoch.  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FBeta.on_epoch_begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FBeta.on_epoch_end\"><code>on_epoch_end</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/metrics.py#L191\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>on_epoch_end</code>(<b>`kwargs`</b>)\n",
       "\n",
       "Called at the end of an epoch.  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FBeta.on_epoch_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Methods - Please document or move to the undocumented section"
   ]
  }
 ],
 "metadata": {
  "jekyll": {
   "keywords": "fastai",
   "summary": "Useful metrics for training",
   "title": "metrics"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
