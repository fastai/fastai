{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from fastai.core import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.imports.torch import *\n",
    "from fastai.tabular.data import *\n",
    "from fastai.tabular.transform import *\n",
    "from fastai.data import *\n",
    "from fastai.tabular.models import *\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype\n",
    "import seaborn as sns\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "#### Create input features - x1, x2, and x3 - and their effect on the reponse variable through the scale object. We'll later pass scale to numpy's `random.gamma()` method to generate a gamma-distributed response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "np.random.seed(90210)\n",
    "x1 = np.random.uniform(0, 5, n)\n",
    "x2 = np.random.normal(100, 20, n)\n",
    "x3 = np.random.choice((1,2,3,4,5), n, p = (0.35, 0.2, 0.05, 0.25, 0.15))\n",
    "scale = np.clip(3.2 * x1 + 0.05 * x2 + 2.15 * x3 - 0.12*x1*x3, a_min = 5, a_max = 1e15)\n",
    "sns.distplot(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation weights, `w`, will also be randomly generated. Each record in the dataset will be observed between 0.5 and 1.5 time units. This type of data often appears in modeling problems where observation periods vary between records in the dataset or when new records are created when something about the observational unit changes throughout the observation period. It is very common in insurance modeling, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.uniform(0.5, 1.5, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bundle the input features into both a numpy array and then a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.stack((x1, x2, x3), axis = 1)\n",
    "df = pd.DataFrame(x, columns = ['x1', 'x2', 'x3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, create a gamma-distributed response variable, `y`, which is assumed to have a scale parameter defined by the linear combination of the input features encoded into scale with a constant shape parameter of 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.gamma(shape = 1.2, scale = scale, size = n)\n",
    "y_range = [0., y.max()]\n",
    "sns.distplot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert the observations weights, `w`, and response, `y`,  into the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['w'] = w\n",
    "df['y'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and validation and specify categorical for continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idxs = np.random.permutation(len(y))[:3000]\n",
    "train_df = df[~df.index.isin(val_idxs)]\n",
    "valid_df = df[ df.index.isin(val_idxs)]\n",
    "\n",
    "cats = ['x3']\n",
    "conts = ['x1', 'x2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize fastai\n",
    "#### Create a subclass from `TabularDataset` called `TabularDataset_ow` that accepts observation weights. The `_ow` stands for **o**bservation  __w__eights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OptTabTfms = Optional[Collection[TabularTransform]]\n",
    "class TabularDataset_ow(TabularDataset):\n",
    "    \"Class for tabular data.\"\n",
    "    def __init__(self, wgt_var:str, df:DataFrame, *args, **kwargs):\n",
    "        super().__init__(df, *args, **kwargs)\n",
    "        if not is_numeric_dtype(df[wgt_var]): df[wgt_var] = df[wgt_var].cat.astype(np.int64)\n",
    "        self.w = np2model_tensor(df[wgt_var].values)\n",
    "        \n",
    "    def __getitem__(self, idx)->Tuple[Tuple[LongTensor,FloatTensor,FloatTensor], Tensor]:\n",
    "        return ((self.cats[idx], self.conts[idx]), [self.y[idx], self.w[idx]])\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df:DataFrame, dep_var:str, wgt_var:str, tfms:OptTabTfms=None, cat_names:OptStrList=None,\n",
    "                       cont_names:OptStrList=None, stats:OptStats=None, log_output:bool=False)->'TabularDataset':\n",
    "        \"Create a tabular dataframe from df after applying optional transforms.\"\n",
    "        if cat_names is None: cat_names = [n for n in df.columns if is_categorical_dtype(df[n])]\n",
    "        if cont_names is None: cont_names = [n for n in df.columns if is_numeric_dtype(df[n]) and not n==dep_var]\n",
    "        if tfms is None: tfms = []\n",
    "        for i,tfm in enumerate(tfms):\n",
    "            if isinstance(tfm, TabularTransform): tfm(df, test=True)\n",
    "            else:\n",
    "                tfm = tfm(cat_names, cont_names)\n",
    "                tfm(df)\n",
    "                tfms[i] = tfm\n",
    "                cat_names, cont_names = tfm.cat_names, tfm.cont_names\n",
    "        ds = cls(wgt_var, df, dep_var, cat_names, cont_names, stats, log_output)\n",
    "        ds.tfms,ds.cat_names,ds.cont_names = tfms,cat_names,cont_names\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customize the code to create DataLoaders. Again, we add an `_ow` suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_data_from_df_ow(path, train_df:DataFrame, valid_df:DataFrame, dep_var:str, wgt_var:str, test_df:OptDataFrame=None,\n",
    "                            tfms:OptTabTfms=None, cat_names:OptStrList=None, cont_names:OptStrList=None,\n",
    "                            stats:OptStats=None, log_output:bool=False, **kwargs)->DataBunch:\n",
    "    \"Create a `DataBunch` from train/valid/test dataframes.\"\n",
    "    cont_names = ifnone(cont_names, list(set(train_df)-set(cat_names)-{dep_var}))\n",
    "    train_ds = TabularDataset_ow.from_dataframe(train_df, dep_var, wgt_var, tfms, cat_names, cont_names, stats, log_output)\n",
    "    valid_ds = TabularDataset_ow.from_dataframe(valid_df, dep_var, wgt_var, train_ds.tfms, train_ds.cat_names,\n",
    "                                             train_ds.cont_names, train_ds.stats, log_output)\n",
    "    datasets = [train_ds, valid_ds]\n",
    "    if test_df is not None:\n",
    "        datasets.append(TabularDataset_ow.from_dataframe(test_df, dep_var, wgt_var, train_ds.tfms, train_ds.cat_names,\n",
    "                                                      train_ds.cont_names, train_ds.stats, log_output))\n",
    "    return DataBunch.create(*datasets, path=path, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = 'y'\n",
    "wgt_var = 'w'\n",
    "tfms = [Categorify]\n",
    "db = tabular_data_from_df_ow('.', train_df, valid_df, dep_var, wgt_var, tfms = tfms,\n",
    "                             cat_names = cats, cont_names = conts, bs = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_szs = [len(train_df[n].cat.categories)+1 for n in cats]\n",
    "emb_szs = [(c, min(10, (c+1)//2)) for c in cat_szs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TabularModel(emb_szs = emb_szs, n_cont = len(conts), out_sz = 1, layers = [5],\n",
    "                     ps = [0.], emb_drop = 0.02, y_range = y_range, use_bn = True)\n",
    "opt = optim.SGD(model.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have observation weights in our datasets and data loaders, we need to make sure our model's loss function and our evaluation metric that we'll be using to monitor learning can accept observational weights. Not doing so will cause an error. PyTorch doesn't supply weighted loss functions, so we'll need to create our own. Deviance, which is a general distance measure between prediction and response that accounts for non-nomal response distributions, is a good choice for our loss function. It is proportional to negative loglikelihood. See https://en.wikipedia.org/wiki/Deviance_(statistics). To monitor training we'll use weighted Mean Absolute Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wgtd_dev(prds, resp, wgts = None):\n",
    "    if wgts is None: wgts = torch.tensor(1.0).repeat(resp.shape)\n",
    "    prds.data.clamp_(min = 1e-5) # predictions must be positive\n",
    "    dev = -2.0 * wgts * (torch.log(resp/prds) - (resp - prds) / prds)\n",
    "    return dev.sum() / wgts.sum()\n",
    "\n",
    "def wgtd_mae(prds, resp, wgts = None):\n",
    "    if wgts is None: wgts = torch.tensor(1.0).repeat(resp.shape)\n",
    "    ae = torch.abs(prds - resp)\n",
    "    return ae.sum() / wgts.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "#### Write a custom `fit()` function just to show how the validation loss would need to be updated. The only thing that would need to change is how the `nums` are appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in db.train_dl:\n",
    "            pred = model(*xb)\n",
    "            if not is_listy(yb): yb = [yb]\n",
    "            loss = wgtd_dev(pred, *yb)\n",
    "    \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        model.eval()\n",
    "        losses,metrics,nums = [],[],[]\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in db.valid_dl:\n",
    "                pred = model(*xb)\n",
    "                if not is_listy(yb): yb = [yb]\n",
    "                losses.append(wgtd_dev(pred, *yb))\n",
    "                metrics.append(wgtd_mae(pred, *yb))\n",
    "                # next line represents changes to get weighted average over\n",
    "                # each batch\n",
    "                nums.append(yb[-1].sum() if len(yb) > 1 else yb[0].shape[0])\n",
    "            nums = np.array(nums, dtype=np.float32)\n",
    "            loss = (to_np(torch.stack(losses)) * nums).sum() / nums.sum()\n",
    "            metric = (to_np(torch.stack(metrics)) * nums).sum() / nums.sum()\n",
    "        print(epoch, loss, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure everything works without observation weights\n",
    "#### Recreate databunch object using fastai defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = tabular_data_from_df('.', train_df, valid_df, dep_var, tfms = tfms,\n",
    "                         cat_names = cats, cont_names = conts, bs = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
