{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory management utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions for memory management. Currently primarily for GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from fastai.gen_doc.nbdoc import *\n",
    "from fastai.utils.mem import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"gpu_mem_get\"><code>gpu_mem_get</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L26\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>gpu_mem_get</code>(**`id`**=***`None`***)\n",
       "\n",
       "get total, used and free memory (in MBs) for gpu `id`. if `id` is not passed, currently selected torch device is used  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(gpu_mem_get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`gpu_mem_get`](/utils.mem.html#gpu_mem_get)\n",
    "\n",
    "* for gpu returns `GPUMemory(total, used, free)`\n",
    "* for cpu returns `GPUMemory(0, 0, 0)`\n",
    "* for invalid gpu id returns `GPUMemory(0, 0, 0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"gpu_mem_get_all\"><code>gpu_mem_get_all</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L37\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>gpu_mem_get_all</code>()\n",
       "\n",
       "get total, used and free memory (in MBs) for each available gpu  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(gpu_mem_get_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`gpu_mem_get_all`](/utils.mem.html#gpu_mem_get_all)\n",
    "* for gpu returns `[ GPUMemory(total_0, used_0, free_0), GPUMemory(total_1, used_1, free_1), .... ]`\n",
    "* for cpu returns `[]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"gpu_mem_get_free_no_cache\"><code>gpu_mem_get_free_no_cache</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L42\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>gpu_mem_get_free_no_cache</code>()\n",
       "\n",
       "get free memory (in MBs) for the currently selected gpu id, after emptying the cache  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(gpu_mem_get_free_no_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`gpu_mem_get_free_no_cache`](/utils.mem.html#gpu_mem_get_free_no_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"gpu_mem_get_used_no_cache\"><code>gpu_mem_get_used_no_cache</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L47\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>gpu_mem_get_used_no_cache</code>()\n",
       "\n",
       "get used memory (in MBs) for the currently selected gpu id, after emptying the cache  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(gpu_mem_get_used_no_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`gpu_mem_get_used_no_cache`](/utils.mem.html#gpu_mem_get_used_no_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"gpu_mem_get_used_fast\"><code>gpu_mem_get_used_fast</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L52\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>gpu_mem_get_used_fast</code>(**`gpu_handle`**)\n",
       "\n",
       "get used memory (in MBs) for the currently selected gpu id, w/o emptying the cache, and needing the `gpu_handle` arg  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(gpu_mem_get_used_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`gpu_mem_get_used_fast`](/utils.mem.html#gpu_mem_get_used_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"gpu_with_max_free_mem\"><code>gpu_with_max_free_mem</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L57\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>gpu_with_max_free_mem</code>()\n",
       "\n",
       "get [gpu_id, its_free_ram] for the first gpu with highest available RAM  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(gpu_with_max_free_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`gpu_with_max_free_mem`](/utils.mem.html#gpu_with_max_free_mem):\n",
    "* for gpu returns: `gpu_with_max_free_ram_id, its_free_ram`\n",
    "* for cpu returns: `None, 0`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"preload_pytorch\"><code>preload_pytorch</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L19\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>preload_pytorch</code>()"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(preload_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`preload_pytorch`](/utils.mem.html#preload_pytorch) is helpful when GPU memory is being measured, since the first time any operation on `cuda` is performed by pytorch, usually about 0.5GB gets used by CUDA context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GPUMemory\"><code>class</code> <code>GPUMemory</code></h4>\n",
       "\n",
       "> <code>GPUMemory</code>(**`total`**, **`used`**, **`free`**) :: `tuple`\n",
       "\n",
       "GPUMemory(total, used, free)  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GPUMemory, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`GPUMemory`](/utils.mem.html#GPUMemory) is a namedtuple that is returned by functions like [`gpu_mem_get`](/utils.mem.html#gpu_mem_get) and [`gpu_mem_get_all`](/utils.mem.html#gpu_mem_get_all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"b2mb\"><code>b2mb</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L22\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>b2mb</code>(**`num`**)\n",
       "\n",
       "convert Bs to MBs and round down  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(b2mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`b2mb`](/utils.mem.html#b2mb) is a helper utility that just does `int(bytes/2**20)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Tracing Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GPUMemTrace\"><code>class</code> <code>GPUMemTrace</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L100\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>GPUMemTrace</code>(**`silent`**=***`False`***)\n",
       "\n",
       "Trace GPU allocated and peak memory usage  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GPUMemTrace, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage examples:\n",
    "```\n",
    "memtrace = GPUMemTrace()\n",
    "memtrace.start() start tracing\n",
    "\n",
    "some_code()\n",
    "memtrace.report() print intermediary cumulative report\n",
    "used, peak =  memtrace.data() same but as data\n",
    "\n",
    "some_code()\n",
    "memtrace.report('2nd run') print intermediary cumulative report\n",
    "used, peak =  memtrace.data()\n",
    "\n",
    "for i in range(10):\n",
    "    memtrace.reset()\n",
    "    code()\n",
    "    memtrace.report(f'i={i}') report for just the last code run since reset\n",
    "\n",
    "# combine report+reset\n",
    "memtrace.reset()\n",
    "for i in range(10):\n",
    "    code()\n",
    "    memtrace.report_n_reset(f'i={i}') report for just the last code run since reset\n",
    "\n",
    "memtrace.stop() # stop the monitor thread\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workarounds to the leaky ipython traceback on exception\n",
    "\n",
    "ipython has a feature where it stores tb with all the `locals()` tied in, which\n",
    "prevents `gc.collect()` from freeing those variables and leading to a leakage.\n",
    "\n",
    "Therefore we cleanse the tb before handing it over to ipython. The 2 ways of doing it are by either using the [`gpu_mem_restore`](/utils.mem.html#gpu_mem_restore) decorator or the [`gpu_mem_restore_ctx`](/utils.mem.html#gpu_mem_restore_ctx) context manager which are described next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"gpu_mem_restore\"><code>gpu_mem_restore</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L71\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>gpu_mem_restore</code>(**`func`**)\n",
       "\n",
       "Reclaim GPU RAM if CUDA out of memory happened, or execution was interrupted  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(gpu_mem_restore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`gpu_mem_restore`](/utils.mem.html#gpu_mem_restore) is a decorator to be used with any functions that interact with CUDA (top-level is fine)\n",
    "\n",
    "* under non-ipython environment it doesn't do anything.\n",
    "* under ipython currently it strips tb by default only for the \"CUDA out of memory\" exception.\n",
    "\n",
    "The env var `FASTAI_TB_CLEAR_FRAMES` changes this behavior when run under ipython,\n",
    "depending on its value: \n",
    "\n",
    "* \"0\": never  strip tb (makes it possible to always use `%debug` magic, but with leaks)\n",
    "* \"1\": always strip tb (never need to worry about leaks, but `%debug` won't work)\n",
    "\n",
    "e.g. `os.environ['FASTAI_TB_CLEAR_FRAMES']=\"0\"` will set it to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"gpu_mem_restore_ctx\"><code>class</code> <code>gpu_mem_restore_ctx</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L91\" class=\"source_link\">[source]</a></h4>\n",
       "\n",
       "> <code>gpu_mem_restore_ctx</code>()\n",
       "\n",
       "context manager to reclaim RAM if an exception happened under ipython  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(gpu_mem_restore_ctx, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if function decorator is not a good option, you can use a context manager instead. For example:\n",
    "```\n",
    "with gpu_mem_restore_ctx():\n",
    "   learn.fit_one_cycle(1,1e-2)\n",
    "```\n",
    "This particular one will clear tb on any exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undocumented Methods - Methods moved below this line will intentionally be hidden"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
