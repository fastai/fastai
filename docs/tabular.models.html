---

title: tabular.models
keywords: fastai
sidebar: home_sidebar

summary: "Model for training tabular/structured data"
---

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Simple-model-for-tabular-data">Simple model for tabular data<a class="anchor-link" href="#Simple-model-for-tabular-data">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TabularModel"><code>class</code> <code>TabularModel</code><a href="https://github.com/fastai/fastai/blob/master/fastai/tabular/models.py#L6" class="source_link">[source]</a></h2><blockquote><p><code>TabularModel</code>(<b>`emb_szs`</b>:<code>ListSizes</code>, <b>`n_cont`</b>:<code>int</code>, <b>`out_sz`</b>:<code>int</code>, <b>`layers`</b>:<code>Collection</code>[<code>int</code>], <b>`ps`</b>:<code>Collection</code>[<code>float</code>]=<b><i>`None`</i></b>, <b>`emb_drop`</b>:<code>float</code>=<b><i>`0.0`</i></b>, <b>`y_range`</b>:<code>OptRange</code>=<b><i>`None`</i></b>, <b>`use_bn`</b>:<code>bool</code>=<b><i>`True`</i></b>, <b>`bn_final`</b>:<code>bool</code>=<b><i>`False`</i></b>) :: <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a></p>
</blockquote>
<p>Basic model for tabular data.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>emb_szs</code> match each categorical variable size with an embedding size, <code>n_cont</code> is the number of continuous variables. The model consists of <code>Embedding</code> layers for the categorical variables, followed by a <code>Dropout</code> of <code>emb_drop</code>, and a <code>BatchNorm</code> for the continuous variables. The results are concatenated and followed by blocks of <code>BatchNorm</code>, <code>Dropout</code>, <code>Linear</code> and <code>ReLU</code> (the first block skips <code>BatchNorm</code> and <code>Dropout</code>, the last block skips the <code>ReLU</code>).</p>
<p>The sizes of the blocks are given in <a href="/layers.html#layers"><code>layers</code></a> and the probabilities of the <code>Dropout</code> in <code>ps</code>. The last size is <code>out_sz</code>, and we add a last activation that is a sigmoid rescaled to cover <code>y_range</code> (if it's not <code>None</code>). Lastly, if <code>use_bn</code> is set to False, all <code>BatchNorm</code> layers are skipped except the one applied to the continuous variables.</p>
<p>Generally it's easiest to just create a learner with <a href="/tabular.data.html#tabular_learner"><code>tabular_learner</code></a>, which will automatically create a <a href="/tabular.models.html#TabularModel"><code>TabularModel</code></a> for you.</p>

</div>
</div>
</div>
</div>
 

