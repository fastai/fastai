
= fastai.fp16

== Introduction and overview

```
...example...
```


== {{class FP16:Module}}

=== Arguments
{{arg module,,}}

=== Methods

{{method forward,}}

{{method load_state_dict,}}

{{method state_dict,}}

== Module Functions

{{method batchnorm_to_fp32,BatchNorm layers to have parameters in single precision.
Find all layers and convert them back to float. This can't
be done with built in .apply as that function will apply
fn to all modules, parameters, and buffers. Thus we wouldn't
be able to guard the float conversion based on the module type.}}

{{method copy_model_to_fp32,Creates a fp32 copy of model parameters and sets optimizer parameters
    }}

{{method copy_fp32_to_model,}}

{{method update_fp32_grads,}}

