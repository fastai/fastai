
= fastai.rnn_reg

== Introduction and overview

```
...example...
```


== {{class LockedDropout:Module}}

=== Arguments
{{arg p,,0.5}}

=== Methods

{{method forward,}}

== {{class WeightDrop:Module}}

=== Arguments
{{arg module,,}}

{{arg dropout,,}}

{{arg weights,,[]}}

=== Methods

{{method _setup,for each string defined in self.weights, the corresponding
attribute in the wrapped module is referenced, then deleted, and subsequently
registered as a new parameter with a slightly modified name.

Args:
    None

 Returns:
     None}}

{{method _setweights,Uses pytorch's built-in dropout function to apply dropout to the parameters of
the wrapped module.

Args:
    None
Returns:
    None}}

{{method forward,updates weights and delegates the propagation of the tensor to the wrapped module's
forward method

Args:
    *args: supplied arguments

Returns:
    tensor obtained by running the forward method on the wrapped module.}}

== {{class EmbeddingDropout:Module}}

=== Arguments
{{arg embed,,}}

=== Methods

{{method forward,}}

== Module Functions

{{method dropout_mask,Applies a dropout mask whose size is determined by passed argument 'sz'.
Args:
    x (nn.Variable): A torch Variable object
    sz (tuple(int, int, int)): The expected size of the new tensor
    dropout (float): The dropout fraction to apply

This method uses the bernoulli distribution to decide which activations to keep.
Additionally, the sampled activations is rescaled is using the factor 1/(1 - dropout).

In the example given below, one can see that approximately .8 fraction of the
returned tensors are zero. Rescaling with the factor 1/(1 - 0.8) returns a tensor
with 5's in the unit places.

The official link to the pytorch bernoulli function is here:
    http://pytorch.org/docs/master/torch.html#torch.bernoulli

Examples:
    >>> a_Var = torch.autograd.Variable(torch.Tensor(2, 3, 4).uniform_(0, 1), requires_grad=False)
    >>> a_Var
        Variable containing:
        (0 ,.,.) =
          0.6890  0.5412  0.4303  0.8918
          0.3871  0.7944  0.0791  0.5979
          0.4575  0.7036  0.6186  0.7217
        (1 ,.,.) =
          0.8354  0.1690  0.1734  0.8099
          0.6002  0.2602  0.7907  0.4446
          0.5877  0.7464  0.4257  0.3386
        [torch.FloatTensor of size 2x3x4]
    >>> a_mask = dropout_mask(a_Var.data, (1,a_Var.size(1),a_Var.size(2)), dropout=0.8)
    >>> a_mask
        (0 ,.,.) =
          0  5  0  0
          0  0  0  5
          5  0  5  0
        [torch.FloatTensor of size 1x3x4]}}

