= fastai.dataloader
Jeremy Howard and contributors
:toc:

== Introduction and overview


The fastai DataLoader is used to load a dataset into memory in batches. One example is the ImageData class, where it is used to load images, before is it passed into the model. Here is a usage in `ImageData.get_dl()`:

```
return DataLoader(ds, batch_size=self.bs, shuffle=shuffle,
            num_workers=self.num_workers, pin_memory=False)
```

Where `ds` is the datasets: train, validation, test, augmented, fixed, and test augmented.

[[DataLoader]]
== Class DataLoader [.small]#(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, pad_idx=0, num_workers=None, pin_memory=False, drop_last=False, pre_pad=True, half=False, transpose=False, transpose_y=False)#

Uses one of `SequentialSampler, RandomSampler, BatchSampler` from `torch.utils.data.sampler` to create tensors for your model to train on.

=== Arguments

dataset (type Dataset (from torch.utils.data.dataset))::
    The fastai library contains several classes that inherit from `Dataset` (such as `FilesArrayDataset`), which can be used (or a custom one can be written as needed).

batch_size (type int, default 1)::
    This parameter is passed into `torch.utils.data.sampler.BatchSampler` to set the size of the mini batches that the model will receive. If this is set too high, you may receive Out Of Memory (OOM) errors.

shuffle (type bool, default False)::
    This parameter is mutually exclusive with `sampler`. If `shuffle` is true, a `torch.utils.data.sampler.RandomSampler` will be used as a sampler instead of `torch.utils.data.sampler.SequentialSampler`.

sampler (type Sampler (from torch.utils.data.sampler) default `None`)::
    This parameter is mutually exclusive with `shuffle`. This allows a custom sampler object to be used, instead of the standard PyTorch samplers. This sampler is then passed into a `BatchSampler` object.

batch_sampler (type BatchSampler (from torch.utils.data.sampler) default `None`)::
    This parameter is mutually exclusive with `batch_size`, `shuffle`, `sampler`, and `drop_last`. This allows a custom BatchSampler object to be used, instead of the standard PyTorch BatchSampler.

pad_idx (type int, default 0)::
    A padding index representing how many zeros to add to each batch. See: `pre_pad`

num_workers (type int, default `None`)::
    Allows the user to manually set the number of workers. If left as `None`, it will default to the number of CPU cores the system has. If > 0, the dataloader will create `num_workers` number of jobs using `concurrent.futures.ThreadPoolExecutor`.

pin_memory (type bool, default `False`)::
    Determines whether or not to use the `pin_memory()` method, which can make data transfer to the GPU faster. This has possible disadvantages though, and is not recommended for a novice user.

drop_last (type bool, default `False`)::
    This parameter is mutually exclusive with `batch_sampler`. If `True`, the last batch will be dropped if the batch size would be less than `batch_size`.

pre_pad (type bool, default `True`)::
    If `pad_idx` is non-zero, this determines if the zeros should go at the beginning of the batch, or at the end. By default, the zeros are added at the beginning of the batch.

half (type bool, default `False`)::
    If `True`, `torch.cuda.HalfTensor()` will be used instead of `torch.FloatTensor()`.

transpose (type bool, default `False`)::
    If `True`, each batch will have its inputs transposed.

transpose_y (type bool, default `False`)::
    If `True`, each batch will have its outputs (labels) transposed.


=== Methods

jag_stack::
Helper method for `np_collate()`. Returns a np.array of the batch passed in, with zeros added for any shorter rows inside the batch, plus extra zeros if `self.pad_idx > 0`.

np_collate::
Helper method for `get_batch()`. Based on the input data type, it creates an appropriate np.array, list, or dict. If the method is passed a string or list of strings, it simply returns the parameter without modification. Batches must contain numbers, strings, dicts, or lists, and this method ensures this is the case.

get_batch::
Helper method for `__iter__()`. When an iterator of the dataloader is created, `get_batch()` is used to retrieve items from the dataset and apply transposes based on `self.transpose` and `self.transpose_y`.
