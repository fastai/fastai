---

title: Loss Functions


keywords: fastai
sidebar: home_sidebar

summary: "Custom fastai loss functions"
description: "Custom fastai loss functions"
nb_path: "nbs/01a_losses.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/01a_losses.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.9/site-packages/torch/cuda/__init__.py:145: UserWarning: 
NVIDIA GeForce RTX 3070 Laptop GPU with CUDA capability sm_86 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.
If you want to use the NVIDIA GeForce RTX 3070 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/

  warnings.warn(incompatible_device_warn.format(device_name, capability, &#34; &#34;.join(arch_list), device_name))
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="BaseLoss" class="doc_header"><code>class</code> <code>BaseLoss</code><a href="https://github.com/fastai/fastai/tree/master/fastai/losses.py#L13" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>BaseLoss</code>(<strong><code>loss_cls</code></strong>, <strong>*<code>args</code></strong>, <strong><code>axis</code></strong>:<code>int</code>=<em><code>-1</code></em>, <strong><code>flatten</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>floatify</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>is_2d</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Same as <code>loss_cls</code>, but flattens input and target.</p>
<table>
<thead><tr>
<th></th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>loss_cls</code></strong></td>
<td></td>
<td></td>
<td>Uninitialized PyTorch-compatible loss</td>
</tr>
<tr>
<td><strong><code>args</code></strong></td>
<td></td>
<td></td>
<td><em>No Content</em></td>
</tr>
<tr>
<td><strong><code>axis</code></strong></td>
<td><code>int</code></td>
<td><code>-1</code></td>
<td><em>No Content</em></td>
</tr>
<tr>
<td><strong><code>flatten</code></strong></td>
<td><code>bool</code></td>
<td><code>True</code></td>
<td><em>No Content</em></td>
</tr>
<tr>
<td><strong><code>floatify</code></strong></td>
<td><code>bool</code></td>
<td><code>False</code></td>
<td><em>No Content</em></td>
</tr>
<tr>
<td><strong><code>is_2d</code></strong></td>
<td><code>bool</code></td>
<td><code>True</code></td>
<td><em>No Content</em></td>
</tr>
<tr>
<td><strong><code>kwargs</code></strong></td>
<td></td>
<td></td>
<td><em>No Content</em></td>
</tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Wrapping a general loss function inside of <a href="/losses.html#BaseLoss"><code>BaseLoss</code></a> provides extra functionalities to your loss functions:</p>
<ul>
<li>flattens the tensors before trying to take the losses since it's more convenient (with a potential tranpose to put <code>axis</code> at the end)</li>
<li>a potential <code>activation</code> method that tells the library if there is an activation fused in the loss (useful for inference and methods such as <a href="/learner.html#Learner.get_preds"><code>Learner.get_preds</code></a> or <a href="/learner.html#Learner.predict"><code>Learner.predict</code></a>)</li>
<li>a potential <code>decodes</code> method that is used on predictions in inference (for instance, an argmax in classification)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>args</code> and <code>kwargs</code> will be passed to <code>loss_cls</code> during the initialization to instantiate a loss function. <code>axis</code> is put at the end for losses like softmax that are often performed on the last axis. If <code>floatify=True</code>, the <code>targs</code> will be converted to floats (useful for losses that only accept float targets like <code>BCEWithLogitsLoss</code>), and <code>is_2d</code> determines if we flatten while keeping the first dimension (batch size) or completely flatten the input. We want the first for losses like Cross Entropy, and the second for pretty much anything else.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="CrossEntropyLossFlat" class="doc_header"><code>class</code> <code>CrossEntropyLossFlat</code><a href="https://github.com/fastai/fastai/tree/master/fastai/losses.py#L59" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>CrossEntropyLossFlat</code>(<strong>*<code>args</code></strong>, <strong><code>axis</code></strong>:<code>int</code>=<em><code>-1</code></em>, <strong><code>weight</code></strong>=<em><code>None</code></em>, <strong><code>ignore_index</code></strong>=<em><code>-100</code></em>, <strong><code>reduction</code></strong>=<em><code>'mean'</code></em>, <strong><code>flatten</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>floatify</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>is_2d</code></strong>:<code>bool</code>=<em><code>True</code></em>) :: <a href="/losses.html#BaseLoss"><code>BaseLoss</code></a></p>
</blockquote>
<p>Same as <code>nn.CrossEntropyLoss</code>, but flattens input and target.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tst</span> <span class="o">=</span> <span class="n">CrossEntropyLossFlat</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="c1">#nn.CrossEntropy would fail with those two tensors, but not our flattened version.</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">tst</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="n">test_fail</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()(</span><span class="n">output</span><span class="p">,</span><span class="n">target</span><span class="p">))</span>

<span class="c1">#Associated activation is softmax</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">tst</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
<span class="c1">#This loss function has a decodes which is argmax</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">tst</span><span class="o">.</span><span class="n">decodes</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tst</span> <span class="o">=</span> <span class="n">CrossEntropyLossFlat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">tst</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">tst</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">tst</span><span class="o">.</span><span class="n">decodes</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://arxiv.org/pdf/1708.02002.pdf">Focal Loss</a> is the same as cross entropy except easy-to-classify observations are down-weighted in the loss calculation. The strength of down-weighting is proportional to the size of the <code>gamma</code> parameter. Put another way, the larger <code>gamma</code> the less the easy-to-classify observations contribute to the loss.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="FocalLoss" class="doc_header"><code>class</code> <code>FocalLoss</code><a href="https://github.com/fastai/fastai/tree/master/fastai/losses.py#L80" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>FocalLoss</code>(<strong><code>gamma</code></strong>:<code>float</code>=<em><code>2.0</code></em>, <strong><code>weight</code></strong>:<code>Tensor</code>=<em><code>None</code></em>, <strong><code>reduction</code></strong>:<code>str</code>=<em><code>'mean'</code></em>) :: <a href="/torch_core.html#Module"><code>Module</code></a></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>
<table>
<thead><tr>
<th></th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>gamma</code></strong></td>
<td><code>float</code></td>
<td><code>2.0</code></td>
<td>Focusing parameter. Higher values down-weight easy examples' contribution to loss</td>
</tr>
<tr>
<td><strong><code>weight</code></strong></td>
<td><code>Tensor</code></td>
<td><code>None</code></td>
<td>Manual rescaling weight given to each class</td>
</tr>
<tr>
<td><strong><code>reduction</code></strong></td>
<td><code>str</code></td>
<td><code>mean</code></td>
<td>PyTorch reduction to apply to the output</td>
</tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="FocalLossFlat" class="doc_header"><code>class</code> <code>FocalLossFlat</code><a href="https://github.com/fastai/fastai/tree/master/fastai/losses.py#L102" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>FocalLossFlat</code>(<strong>*<code>args</code></strong>, <strong><code>gamma</code></strong>:<code>float</code>=<em><code>2.0</code></em>, <strong><code>axis</code></strong>:<code>int</code>=<em><code>-1</code></em>, <strong><code>weight</code></strong>=<em><code>None</code></em>, <strong><code>reduction</code></strong>=<em><code>'mean'</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/losses.html#BaseLoss"><code>BaseLoss</code></a></p>
</blockquote>
<p>Same as CrossEntropyLossFlat but with focal paramter, <code>gamma</code>. Focal loss is introduced by Lin et al.
<a href="https://arxiv.org/pdf/1708.02002.pdf">https://arxiv.org/pdf/1708.02002.pdf</a>. Note the class weighting factor in the paper, alpha, can be
implemented through pytorch <code>weight</code> argument passed through to F.cross_entropy.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fl</span> <span class="o">=</span> <span class="n">FocalLossFlat</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ce</span> <span class="o">=</span> <span class="n">CrossEntropyLossFlat</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">fl</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="n">ce</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span>
<span class="c1">#Test focal loss with gamma &gt; 0 is different than cross entropy</span>
<span class="n">fl</span> <span class="o">=</span> <span class="n">FocalLossFlat</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">test_ne</span><span class="p">(</span><span class="n">fl</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="n">ce</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fl</span> <span class="o">=</span> <span class="n">FocalLossFlat</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ce</span> <span class="o">=</span> <span class="n">CrossEntropyLossFlat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">fl</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="n">ce</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">fl</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">fl</span><span class="o">.</span><span class="n">decodes</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="BCEWithLogitsLossFlat" class="doc_header"><code>class</code> <code>BCEWithLogitsLossFlat</code><a href="https://github.com/fastai/fastai/tree/master/fastai/losses.py#L127" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>BCEWithLogitsLossFlat</code>(<strong>*<code>args</code></strong>, <strong><code>axis</code></strong>:<code>int</code>=<em><code>-1</code></em>, <strong><code>floatify</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>thresh</code></strong>:<code>float</code>=<em><code>0.5</code></em>, <strong><code>weight</code></strong>=<em><code>None</code></em>, <strong><code>reduction</code></strong>=<em><code>'mean'</code></em>, <strong><code>pos_weight</code></strong>=<em><code>None</code></em>, <strong><code>flatten</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>is_2d</code></strong>:<code>bool</code>=<em><code>True</code></em>) :: <a href="/losses.html#BaseLoss"><code>BaseLoss</code></a></p>
</blockquote>
<p>Same as <code>nn.BCEWithLogitsLoss</code>, but flattens input and target.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tst</span> <span class="o">=</span> <span class="n">BCEWithLogitsLossFlat</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="c1">#nn.BCEWithLogitsLoss would fail with those two tensors, but not our flattened version.</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">tst</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">test_fail</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()(</span><span class="n">output</span><span class="p">,</span><span class="n">target</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="c1">#nn.BCEWithLogitsLoss would fail with int targets but not our flattened version.</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">tst</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">test_fail</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()(</span><span class="n">output</span><span class="p">,</span><span class="n">target</span><span class="p">))</span>

<span class="n">tst</span> <span class="o">=</span> <span class="n">BCEWithLogitsLossFlat</span><span class="p">(</span><span class="n">pos_weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">tst</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">test_fail</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()(</span><span class="n">output</span><span class="p">,</span><span class="n">target</span><span class="p">))</span>

<span class="c1">#Associated activation is sigmoid</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">tst</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="BCELossFlat" class="doc_header"><code>BCELossFlat</code><a href="https://github.com/fastai/fastai/tree/master/fastai/losses.py#L153" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>BCELossFlat</code>(<strong>*<code>args</code></strong>, <strong><code>axis</code></strong>:<code>int</code>=<em><code>-1</code></em>, <strong><code>floatify</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>weight</code></strong>=<em><code>None</code></em>, <strong><code>reduction</code></strong>=<em><code>'mean'</code></em>)</p>
</blockquote>
<p>Same as <code>nn.BCELoss</code>, but flattens input and target.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tst</span> <span class="o">=</span> <span class="n">BCELossFlat</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">tst</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">test_fail</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()(</span><span class="n">output</span><span class="p">,</span><span class="n">target</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MSELossFlat" class="doc_header"><code>MSELossFlat</code><a href="https://github.com/fastai/fastai/tree/master/fastai/losses.py#L164" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MSELossFlat</code>(<strong>*<code>args</code></strong>, <strong><code>axis</code></strong>:<code>int</code>=<em><code>-1</code></em>, <strong><code>floatify</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>reduction</code></strong>=<em><code>'mean'</code></em>)</p>
</blockquote>
<p>Same as <code>nn.MSELoss</code>, but flattens input and target.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tst</span> <span class="o">=</span> <span class="n">MSELossFlat</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">tst</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">test_fail</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()(</span><span class="n">output</span><span class="p">,</span><span class="n">target</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="L1LossFlat" class="doc_header"><code>L1LossFlat</code><a href="https://github.com/fastai/fastai/tree/master/fastai/losses.py#L175" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>L1LossFlat</code>(<strong>*<code>args</code></strong>, <strong><code>axis</code></strong>=<em><code>-1</code></em>, <strong><code>floatify</code></strong>=<em><code>True</code></em>, <strong><code>reduction</code></strong>=<em><code>'mean'</code></em>)</p>
</blockquote>
<p>Same as <code>nn.L1Loss</code>, but flattens input and target.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="LabelSmoothingCrossEntropy" class="doc_header"><code>class</code> <code>LabelSmoothingCrossEntropy</code><a href="https://github.com/fastai/fastai/tree/master/fastai/losses.py#L186" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>LabelSmoothingCrossEntropy</code>(<strong><code>eps</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>weight</code></strong>:<code>Tensor</code>=<em><code>None</code></em>, <strong><code>reduction</code></strong>:<code>str</code>=<em><code>'mean'</code></em>) :: <a href="/torch_core.html#Module"><code>Module</code></a></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>
<table>
<thead><tr>
<th></th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>eps</code></strong></td>
<td><code>float</code></td>
<td><code>0.1</code></td>
<td>The weight for the interpolation formula</td>
</tr>
<tr>
<td><strong><code>weight</code></strong></td>
<td><code>Tensor</code></td>
<td><code>None</code></td>
<td>Manual rescaling weight given to each class passed to <code>F.nll_loss</code></td>
</tr>
<tr>
<td><strong><code>reduction</code></strong></td>
<td><code>str</code></td>
<td><code>mean</code></td>
<td>PyTorch reduction to apply to the output</td>
</tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lmce</span> <span class="o">=</span> <span class="n">LabelSmoothingCrossEntropy</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">lmce</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">flatten</span><span class="p">()),</span> <span class="n">lmce</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">target</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>On top of the formula we define:</p>
<ul>
<li>a <code>reduction</code> attribute, that will be used when we call <a href="/learner.html#Learner.get_preds"><code>Learner.get_preds</code></a></li>
<li><code>weight</code> attribute to pass to BCE.</li>
<li>an <code>activation</code> function that represents the activation fused in the loss (since we use cross entropy behind the scenes). It will be applied to the output of the model when calling <a href="/learner.html#Learner.get_preds"><code>Learner.get_preds</code></a> or <a href="/learner.html#Learner.predict"><code>Learner.predict</code></a></li>
<li>a <code>decodes</code> function that converts the output of the model to a format similar to the target (here indices). This is used in <a href="/learner.html#Learner.predict"><code>Learner.predict</code></a> and <a href="/learner.html#Learner.show_results"><code>Learner.show_results</code></a> to decode the predictions </li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="LabelSmoothingCrossEntropyFlat" class="doc_header"><code>class</code> <code>LabelSmoothingCrossEntropyFlat</code><a href="https://github.com/fastai/fastai/tree/master/fastai/losses.py#L214" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>LabelSmoothingCrossEntropyFlat</code>(<strong>*<code>args</code></strong>, <strong><code>axis</code></strong>:<code>int</code>=<em><code>-1</code></em>, <strong><code>eps</code></strong>=<em><code>0.1</code></em>, <strong><code>reduction</code></strong>=<em><code>'mean'</code></em>, <strong><code>flatten</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>floatify</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>is_2d</code></strong>:<code>bool</code>=<em><code>True</code></em>) :: <a href="/losses.html#BaseLoss"><code>BaseLoss</code></a></p>
</blockquote>
<p>Same as <a href="/losses.html#LabelSmoothingCrossEntropy"><code>LabelSmoothingCrossEntropy</code></a>, but flattens input and target.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lmce</span> <span class="o">=</span> <span class="n">LabelSmoothingCrossEntropy</span><span class="p">()</span>
<span class="n">lmce_flat</span> <span class="o">=</span> <span class="n">LabelSmoothingCrossEntropyFlat</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">lmce</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">target</span><span class="p">),</span> <span class="n">lmce_flat</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="n">target</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We present a general <a href="/metrics.html#Dice"><code>Dice</code></a> loss for segmentation tasks. It is commonly used together with <code>CrossEntropyLoss</code> or <a href="/losses.html#FocalLoss"><code>FocalLoss</code></a> in kaggle competitions. This is very similar to the <a href="/metrics.html#DiceMulti"><code>DiceMulti</code></a> metric, but to be able to derivate through, we replace the <code>argmax</code> activation by a <code>softmax</code> and compare this with a one-hot encoded target mask. This function also adds a <code>smooth</code> parameter to help numerical stabilities in the intersection over union division. If your network has problem learning with this DiceLoss, try to set the <code>square_in_union</code> parameter in the DiceLoss constructor to <code>True</code>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="DiceLoss" class="doc_header"><code>class</code> <code>DiceLoss</code><a href="https://github.com/fastai/fastai/tree/master/fastai/losses.py#L234" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>DiceLoss</code>(<strong><code>axis</code></strong>:<code>int</code>=<em><code>1</code></em>, <strong><code>smooth</code></strong>:<code>float</code>=<em><code>1e-06</code></em>, <strong><code>reduction</code></strong>:<code>str</code>=<em><code>'sum'</code></em>, <strong><code>square_in_union</code></strong>:<code>bool</code>=<em><code>False</code></em>)</p>
</blockquote>
<p>Dice loss for segmentation</p>
<table>
<thead><tr>
<th></th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>axis</code></strong></td>
<td><code>int</code></td>
<td><code>1</code></td>
<td>Class axis</td>
</tr>
<tr>
<td><strong><code>smooth</code></strong></td>
<td><code>float</code></td>
<td><code>1e-06</code></td>
<td>Helps with numerical stabilities in the IoU division</td>
</tr>
<tr>
<td><strong><code>reduction</code></strong></td>
<td><code>str</code></td>
<td><code>sum</code></td>
<td>PyTorch reduction to apply to the output</td>
</tr>
<tr>
<td><strong><code>square_in_union</code></strong></td>
<td><code>bool</code></td>
<td><code>False</code></td>
<td>Squares predictions to increase slope of gradients</td>
</tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dl</span> <span class="o">=</span> <span class="n">DiceLoss</span><span class="p">()</span>
<span class="n">_x</span>         <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span> <span class="p">[[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                       <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]])</span>
<span class="n">_one_hot_x</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([[[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
                      <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span>
                      <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]]])</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">dl</span><span class="o">.</span><span class="n">_one_hot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">_one_hot_x</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dl</span> <span class="o">=</span> <span class="n">DiceLoss</span><span class="p">()</span>
<span class="n">model_output</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([[[[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
                         <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">]],</span>
                        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">],</span>
                         <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]],</span>
                        <span class="p">[[</span><span class="mf">3.</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                         <span class="p">[</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]]]])</span>
<span class="n">target</span>       <span class="o">=</span>  <span class="n">tensor</span><span class="p">([[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]])</span>
<span class="n">dl_out</span> <span class="o">=</span> <span class="n">dl</span><span class="p">(</span><span class="n">model_output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">dl</span><span class="o">.</span><span class="n">decodes</span><span class="p">(</span><span class="n">model_output</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dl</span> <span class="o">=</span> <span class="n">DiceLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
<span class="c1">#identical masks</span>
<span class="n">model_output</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([[[</span><span class="mf">.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">100.</span><span class="p">]]])</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">]])</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">dl</span><span class="p">(</span><span class="n">model_output</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1">#50% intersection</span>
<span class="n">model_output</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([[[</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">100.</span><span class="p">],</span> <span class="p">[</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">100.</span><span class="p">,</span> <span class="mf">.1</span><span class="p">]]])</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">dl</span><span class="p">(</span><span class="n">model_output</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="mf">.66</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As a test case for the dice loss consider satellite image segmentation. Let us say we have three classes: Background (0), River (1) and Road (2). Let us look at a specific target</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">target</span><span class="p">[:,</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">target</span><span class="p">[:,</span><span class="mi">50</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">target</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Nearly everything is background in this example, and we have a thin river at the left of the image as well as a thin road in the middle of the image. If all our data looks similar to this, we say that there is a class imbalance, meaning that some classes (like river and road) appear relatively infrequently. If our model just predicted "background" (i.e. the value 0) for all pixels, it would be correct for most pixels. But this would be a bad model and the diceloss should reflect that</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model_output_all_background</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="c1"># assign probability 1 to class 0 everywhere</span>
<span class="c1"># to get probability 1, we just need a high model output before softmax gets applied</span>
<span class="n">model_output_all_background</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,:]</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model_output_all_background</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">model_output_all_background</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">target</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our dice score should be around 1/3 here, because the "background" class is predicted correctly (and that for nearly every pixel), but the other two clases are never predicted correctly. Dice score of 1/3 means dice loss of 1 - 1/3 = 2/3:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_close</span><span class="p">(</span><span class="n">dl</span><span class="p">(</span><span class="n">model_output_all_background</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="mf">0.67</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If the model would predict everything correctly, the dice loss should be zero:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">correct_model_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">correct_model_output</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,:]</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">correct_model_output</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">correct_model_output</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,</span><span class="mi">50</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">correct_model_output</span><span class="p">[</span><span class="mi">1</span><span class="p">,:,</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">correct_model_output</span><span class="p">[</span><span class="mi">2</span><span class="p">,:,</span><span class="mi">50</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">correct_model_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">correct_model_output</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_close</span><span class="p">(</span><span class="n">dl</span><span class="p">(</span><span class="n">correct_model_output</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You could easily combine this loss with <a href="/losses.html#FocalLoss"><code>FocalLoss</code></a> defining a <code>CombinedLoss</code>, to balance between global (Dice) and local (Focal) features on the target mask.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">CombinedLoss</span><span class="p">:</span>
    <span class="s2">&quot;Dice and Focal combined&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">smooth</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
        <span class="n">store_attr</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">focal_loss</span> <span class="o">=</span> <span class="n">FocalLossFlat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dice_loss</span> <span class="o">=</span>  <span class="n">DiceLoss</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">smooth</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">targ</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">focal_loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">targ</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dice_loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">targ</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">decodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">cl</span> <span class="o">=</span> <span class="n">CombinedLoss</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">cl</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

