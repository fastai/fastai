---

title: Distributed and parallel training

keywords: fastai
sidebar: home_sidebar

summary: "Callbacks and helper functions to train in parallel or use distributed training"
description: "Callbacks and helper functions to train in parallel or use distributed training"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/20a_distributed.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Parallel">Parallel<a class="anchor-link" href="#Parallel"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Patch the parallel models so they work with RNNs</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="DataParallel.reset" class="doc_header"><code>DataParallel.reset</code><a href="https://github.com/fastai/fastai2/tree/master/fastai2/distributed.py#L12" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>DataParallel.reset</code>()</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ParallelTrainer" class="doc_header"><code>class</code> <code>ParallelTrainer</code><a href="https://github.com/fastai/fastai2/tree/master/fastai2/distributed.py#L18" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ParallelTrainer</code>(<strong><code>device_ids</code></strong>) :: <a href="/callback.core#Callback"><code>Callback</code></a></p>
</blockquote>
<p>Basic class handling tweaks of the training loop by changing a <a href="/13a_learner#Learner"><code>Learner</code></a> in various events</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Learner.to_parallel" class="doc_header"><code>Learner.to_parallel</code><a href="https://github.com/fastai/fastai2/tree/master/fastai2/distributed.py#L25" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Learner.to_parallel</code>(<strong><code>device_ids</code></strong>=<em><code>None</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Learner.detach_parallel" class="doc_header"><code>Learner.detach_parallel</code><a href="https://github.com/fastai/fastai2/tree/master/fastai2/distributed.py#L31" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Learner.detach_parallel</code>()</p>
</blockquote>
<p>Remove ParallelTrainer callback from Learner.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Learner.parallel_ctx" class="doc_header"><code>Learner.parallel_ctx</code><a href="https://github.com/fastai/fastai2/tree/master/fastai2/distributed.py#L38" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Learner.parallel_ctx</code>(<strong><code>device_ids</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>A context manager to adapt a learner to train in data parallel mode.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Distributed">Distributed<a class="anchor-link" href="#Distributed"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Patch the parallel models so they work with RNNs</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="DistributedDataParallel.reset" class="doc_header"><code>DistributedDataParallel.reset</code><a href="https://github.com/fastai/fastai2/tree/master/fastai2/distributed.py#L50" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>DistributedDataParallel.reset</code>()</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Convenience functions to set up/tear down torch distributed data parallel mode.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="setup_distrib" class="doc_header"><code>setup_distrib</code><a href="https://github.com/fastai/fastai2/tree/master/fastai2/distributed.py#L55" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>setup_distrib</code>(<strong><code>gpu</code></strong>=<em><code>None</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="teardown_distrib" class="doc_header"><code>teardown_distrib</code><a href="https://github.com/fastai/fastai2/tree/master/fastai2/distributed.py#L64" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>teardown_distrib</code>()</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="DataLoader">DataLoader<a class="anchor-link" href="#DataLoader"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We need to change the dataloaders so that they only get one part of the batch each (otherwise there is no point in using distributed training).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DistributedDL" class="doc_header"><code>class</code> <code>DistributedDL</code><a href="https://github.com/fastai/fastai2/tree/master/fastai2/distributed.py#L70" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DistributedDL</code>(<strong><code>dataset</code></strong>, <strong><code>rank</code></strong>, <strong><code>world_size</code></strong>, <strong><code>bs</code></strong>=<em><code>64</code></em>, <strong><code>shuffle</code></strong>=<em><code>False</code></em>, <strong><code>num_workers</code></strong>=<em><code>None</code></em>, <strong><code>verbose</code></strong>=<em><code>False</code></em>, <strong><code>do_setup</code></strong>=<em><code>True</code></em>, <strong><code>pin_memory</code></strong>=<em><code>False</code></em>, <strong><code>timeout</code></strong>=<em><code>0</code></em>, <strong><code>batch_size</code></strong>=<em><code>None</code></em>, <strong><code>drop_last</code></strong>=<em><code>False</code></em>, <strong><code>indexed</code></strong>=<em><code>None</code></em>, <strong><code>n</code></strong>=<em><code>None</code></em>, <strong><code>device</code></strong>=<em><code>None</code></em>, <strong><code>wif</code></strong>=<em><code>None</code></em>, <strong><code>before_iter</code></strong>=<em><code>None</code></em>, <strong><code>after_item</code></strong>=<em><code>None</code></em>, <strong><code>before_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_iter</code></strong>=<em><code>None</code></em>, <strong><code>create_batches</code></strong>=<em><code>None</code></em>, <strong><code>create_item</code></strong>=<em><code>None</code></em>, <strong><code>create_batch</code></strong>=<em><code>None</code></em>, <strong><code>retain</code></strong>=<em><code>None</code></em>, <strong><code>get_idxs</code></strong>=<em><code>None</code></em>, <strong><code>sample</code></strong>=<em><code>None</code></em>, <strong><code>shuffle_fn</code></strong>=<em><code>None</code></em>, <strong><code>do_batch</code></strong>=<em><code>None</code></em>) :: <a href="/data.core#TfmdDL"><code>TfmdDL</code></a></p>
</blockquote>
<p>Transformed <a href="/data.load#DataLoader"><code>DataLoader</code></a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dl</span> <span class="o">=</span> <span class="n">TfmdDL</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">)),</span> <span class="n">bs</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">dl1</span> <span class="o">=</span> <span class="n">DistributedDL</span><span class="o">.</span><span class="n">from_dl</span><span class="p">(</span><span class="n">dl</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">test_eq</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">dl1</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">52</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">%</span><span class="k">50</span>)
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dl</span> <span class="o">=</span> <span class="n">TfmdDL</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">)),</span> <span class="n">bs</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">dl1</span> <span class="o">=</span> <span class="n">DistributedDL</span><span class="o">.</span><span class="n">from_dl</span><span class="p">(</span><span class="n">dl</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">dl1</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="n">dl1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1">#All items should only be accessed once (except 0 and 1 for final cycle) with seeded shuffle</span>
<span class="n">test_eq</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">res</span><span class="p">),</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DistributedTrainer" class="doc_header"><code>class</code> <code>DistributedTrainer</code><a href="https://github.com/fastai/fastai2/tree/master/fastai2/distributed.py#L112" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DistributedTrainer</code>(<strong><code>cuda_id</code></strong>=<em><code>0</code></em>) :: <a href="/callback.core#Callback"><code>Callback</code></a></p>
</blockquote>
<p>Basic class handling tweaks of the training loop by changing a <a href="/13a_learner#Learner"><code>Learner</code></a> in various events</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Attach, remove a callback which adapts the model to use DistributedDL to train in distributed data parallel mode.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Learner.to_distributed" class="doc_header"><code>Learner.to_distributed</code><a href="https://github.com/fastai/fastai2/tree/master/fastai2/distributed.py#L138" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Learner.to_distributed</code>(<strong><code>cuda_id</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Learner.detach_distributed" class="doc_header"><code>Learner.detach_distributed</code><a href="https://github.com/fastai/fastai2/tree/master/fastai2/distributed.py#L145" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Learner.detach_distributed</code>()</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Learner.distrib_ctx" class="doc_header"><code>Learner.distrib_ctx</code><a href="https://github.com/fastai/fastai2/tree/master/fastai2/distributed.py#L153" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Learner.distrib_ctx</code>(<strong><code>cuda_id</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>A context manager to adapt a learner to train in distributed data parallel mode.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="distrib_ctx-context-manager"><code>distrib_ctx</code> context manager<a class="anchor-link" href="#distrib_ctx-context-manager"> </a></h3><p><strong><code>distrib_ctx(cuda_id)</code></strong> prepares a learner to train in distributed data parallel mode.  It assumes these <a href="https://pytorch.org/tutorials/intermediate/dist_tuto.html#initialization-methods">environment variables</a> have all been setup properly, such as those launched by <code>python -m fastai2.launch</code>.</p>
<h4 id="Typical-usage:">Typical usage:<a class="anchor-link" href="#Typical-usage:"> </a></h4>
<pre><code>with learn.distrib_ctx(): learn.fit(.....)</code></pre>
<p>It attaches a <a href="/20a_distributed#DistributedTrainer"><code>DistributedTrainer</code></a> callback and <a href="/20a_distributed#DistributedDL"><code>DistributedDL</code></a> data loader to  the learner, then executes <code>learn.fit(.....)</code>.  Upon exiting the context, it removes the <a href="/20a_distributed#DistributedTrainer"><code>DistributedTrainer</code></a> and <a href="/20a_distributed#DistributedDL"><code>DistributedDL</code></a>, and destroys any locally created distributed process group.  The process is still attached to the GPU though.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="rank0_first" class="doc_header"><code>rank0_first</code><a href="https://github.com/fastai/fastai2/tree/master/fastai2/distributed.py#L173" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>rank0_first</code>(<strong><code>func</code></strong>)</p>
</blockquote>
<p>Execute <code>func</code> in the Rank-0 process first, then in other ranks in parallel.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong><code>rank0_first(f)</code></strong> calls <code>f()</code> in rank-0 process first, then in parallel on the rest, in distributed training mode. In single process, non-distributed training mode, <code>f()</code> is called only once as expected.</p>
<p>One application of <code>rank0_first()</code> is to make fresh downloads via <code>untar_data()</code> safe in distributed training scripts launched by <code>python -m fastai2.launch &lt;script&gt;</code>:</p>
<blockquote><p><code>path = untar_data(URLs.IMDB)</code></p>
</blockquote>
<p>becomes:&gt; <code>path = <b>rank0_first(lambda:</b> untar_data(URLs.IMDB))</code></p>
<p>Some learner factory methods may use <code>untar_data()</code> to <strong>download pretrained models</strong> by default:</p>
<blockquote><p><code>learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)</code></p>
</blockquote>
<p>becomes:&gt; <code>learn = <b>rank0_first(lambda:</b> text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy))</code>
Otherwise, multiple processes will download at the same time and corrupt the data.</p>

</div>
</div>
</div>
</div>
 

