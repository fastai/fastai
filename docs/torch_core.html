---

title: torch_core
keywords: fastai
sidebar: home_sidebar

summary: "Basic functions using pytorch"
---

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Torch-Core">Torch Core<a class="anchor-link" href="#Torch-Core">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This module contains all the basic functions we need in other modules of the fastai library (split with <a href="/core.html#core"><code>core</code></a> that contains the ones not requiring pytorch). Its documentation can easily be skipped at a first read, unless you want to know what a given function does.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Global-constants">Global constants<a class="anchor-link" href="#Global-constants">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>AdamW = partial(optim.Adam, betas=(0.9,0.99))</code> <div style="text-align: right"><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L43">[source]</a></div></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)</code> <div style="text-align: right"><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L41">[source]</a></div></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>defaults.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')</code> <div style="text-align: right"><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L62">[source]</a></div></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you are trying to make fastai run on the CPU, simply change the default device: <code>defaults.device = 'cpu'</code>.</p>
<p>Alternatively, if not using wildcard imports: <code>fastai.torch_core.defaults.device = 'cpu'</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Functions-that-operate-conversions">Functions that operate conversions<a class="anchor-link" href="#Functions-that-operate-conversions">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="batch_to_half"><code>batch_to_half</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L203" class="source_link">[source]</a></h4><blockquote><p><code>batch_to_half</code>(<strong><code>b</code></strong>:<code>Collection</code>[<code>Tensor</code>]) → <code>Collection</code>[<code>Tensor</code>]</p>
</blockquote>
<p>Set the input of batch <code>b</code> to half precision.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="flatten_model"><code>flatten_model</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L157" class="source_link">[source]</a></h4><blockquote><p><code>flatten_model</code>(<strong><code>m</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Flattens all the layers of <code>m</code> into an array. This allows for easy access to the layers of the model and allows you to manipulate the model as if it was an array.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">simple_cnn</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">12</span><span class="p">])</span>
<span class="n">m</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Sequential(
  (0): Sequential(
    (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ReLU(inplace)
  )
  (1): Sequential(
    (0): Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ReLU(inplace)
  )
  (2): Sequential(
    (0): AdaptiveAvgPool2d(output_size=1)
    (1): Flatten()
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">flatten_model</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),
 ReLU(inplace),
 Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),
 ReLU(inplace),
 AdaptiveAvgPool2d(output_size=1),
 Flatten()]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="model2half"><code>model2half</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L213" class="source_link">[source]</a></h4><blockquote><p><code>model2half</code>(<strong><code>model</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>) → <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a></p>
</blockquote>
<p>Convert <code>model</code> to half precision except the batchnorm layers.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Converting model parameters to half precision allows us to leverage fast <code>FP16</code> arithmetic which can speed up the computations by 2-8 times. It also reduces memory consumption allowing us to train deeper models.</p>
<p><strong>Note</strong>: Batchnorm layers are not converted to half precision as that may lead to instability in training.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">simple_cnn</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">12</span><span class="p">],</span> <span class="n">bn</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">show_params_dtype</span><span class="p">(</span><span class="n">state_dict</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Simple function to pretty print the dtype of the model params&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">wt_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:&lt;30}</span><span class="s2">: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">wt_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">()</span>    

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dtypes of model parameters before model2half: &quot;</span><span class="p">)</span>
<span class="n">show_params_dtype</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

<span class="c1"># Converting model to half precision</span>
<span class="n">m_half</span> <span class="o">=</span> <span class="n">model2half</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dtypes of model parameters after model2half: &quot;</span><span class="p">)</span>
<span class="n">show_params_dtype</span><span class="p">(</span><span class="n">m_half</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>dtypes of model parameters before model2half: 
0.0.weight                    : torch.float32
0.2.weight                    : torch.float32
0.2.bias                      : torch.float32
0.2.running_mean              : torch.float32
0.2.running_var               : torch.float32
0.2.num_batches_tracked       : torch.int64
1.0.weight                    : torch.float32
1.0.bias                      : torch.float32

dtypes of model parameters after model2half: 
0.0.weight                    : torch.float16
0.2.weight                    : torch.float32
0.2.bias                      : torch.float32
0.2.running_mean              : torch.float32
0.2.running_var               : torch.float32
0.2.num_batches_tracked       : torch.int64
1.0.weight                    : torch.float16
1.0.bias                      : torch.float16

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="np2model_tensor"><code>np2model_tensor</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L260" class="source_link">[source]</a></h4><blockquote><p><code>np2model_tensor</code>(<strong><code>a</code></strong>)</p>
</blockquote>
<p>Tranform numpy array <code>a</code> to a tensor of the same type.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is a wrapper on top of Pytorch's <code>torch.as_tensor</code> which converts numpy array to torch tensor, and additionally attempts to map all floats to <code>torch.float32</code> and all integers to <code>torch.int64</code> for consistencies in model data. Below is an example demonstrating it's functionality for floating number, similar functionality applies to integer as well.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">a2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">a3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="n">b1</span> <span class="o">=</span> <span class="n">np2model_tensor</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span> <span class="c1"># Maps to torch.float32</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">np2model_tensor</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span> <span class="c1"># Maps to torch.float32</span>
<span class="n">b3</span> <span class="o">=</span> <span class="n">np2model_tensor</span><span class="p">(</span><span class="n">a3</span><span class="p">)</span> <span class="c1"># Maps to torch.float32</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;Datatype of as&#39;: </span><span class="si">{a1.dtype}</span><span class="s2">, </span><span class="si">{a2.dtype}</span><span class="s2">, </span><span class="si">{a3.dtype}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;Datatype of bs&#39;: </span><span class="si">{b1.dtype}</span><span class="s2">, </span><span class="si">{b2.dtype}</span><span class="s2">, </span><span class="si">{b3.dtype}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Datatype of as&#39;: float16, float32, float64
Datatype of bs&#39;: torch.float32, torch.float32, torch.float32
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="requires_grad"><code>requires_grad</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L117" class="source_link">[source]</a></h4><blockquote><p><code>requires_grad</code>(<strong><code>m</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>, <strong><code>b</code></strong>:<code>Optional</code>[<code>bool</code>]=<strong><em><code>None</code></em></strong>) → <code>Optional</code>[<code>bool</code>]</p>
</blockquote>
<p>If <code>b</code> is not set return <a href="/torch_core.html#requires_grad"><code>requires_grad</code></a> of first param, else set <a href="/torch_core.html#requires_grad"><code>requires_grad</code></a> on all params as <code>b</code></p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Performs both getting and setting of <a href="/torch_core.html#requires_grad"><code>requires_grad</code></a> parameter of the tensors, which decided whether to accumulate gradients or not.</p>
<ul>
<li><p>If <code>b</code> is <code>None</code>: The function <strong>gets</strong> the <a href="/torch_core.html#requires_grad"><code>requires_grad</code></a> for the model parameter, to be more specific it returns the <a href="/torch_core.html#requires_grad"><code>requires_grad</code></a> of the first element in the model.</p>
</li>
<li><p>Else if <code>b</code> is passed (a boolean value), <a href="/torch_core.html#requires_grad"><code>requires_grad</code></a> of all parameters of the model is <strong>set</strong> to <code>b</code>.</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Any Pytorch model</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">simple_cnn</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span> <span class="n">bn</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Get the requires_grad of model</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;requires_grad of model: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">(</span><span class="n">m</span><span class="p">)))</span>

<span class="c1"># Set requires_grad of all params in model to false</span>
<span class="n">requires_grad</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

<span class="c1"># Get the requires_grad of model</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;requires_grad of model: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">(</span><span class="n">m</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>requires_grad of model: True
requires_grad of model: False
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="tensor"><code>tensor</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L65" class="source_link">[source]</a></h4><blockquote><p><code>tensor</code>(<strong><code>x</code></strong>:<code>Any</code>, <strong>*<code>rest</code></strong>) → <code>Tensor</code></p>
</blockquote>
<p>Like <code>torch.as_tensor</code>, but handle lists too, and can pass multiple vector elements directly.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Handy function when you want to convert any list type object to tensor, initialize your weights manually, and other similar cases.</p>
<p><strong>NB</strong>: When passing multiple vectors, all vectors must be of same dimensions. (Obvious but can be forgotten sometimes)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Conversion from any numpy array</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>

<span class="c1"># Passing as multiple parameters</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>

<span class="c1"># Passing a single list</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>

<span class="c1"># Can work with multiple vectors / lists</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([1, 2, 3]) &lt;class &#39;torch.Tensor&#39;&gt;
tensor([1, 2, 3]) &lt;class &#39;torch.Tensor&#39;&gt;
tensor([1, 2, 3]) &lt;class &#39;torch.Tensor&#39;&gt;
tensor([[1, 2],
        [3, 4]]) &lt;class &#39;torch.Tensor&#39;&gt;
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="to_cpu"><code>to_cpu</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L92" class="source_link">[source]</a></h4><blockquote><p><code>to_cpu</code>(<strong><code>b</code></strong>:<code>ItemsList</code>)</p>
</blockquote>
<p>Recursively map lists of tensors in <code>b</code> to the cpu.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A wrapper on top of Pytorch's <code>torch.Tensor.cpu()</code> function, which creates and returns a copy of a tensor or even a <strong>list</strong> of tensors in the CPU. As described in Pytorch's docs, if the tensor or list of tensor is already on the CPU, the exact data is returned and no copy is made.</p>
<p>Useful to convert all the list of parameters of the model to CPU in a single call.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Id of tensors in a: &quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">a</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    
    <span class="c1"># Getting a CPU version of the tensors in GPU</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">to_cpu</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Id of tensors in b:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">b</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    
    <span class="c1"># Trying to perform to_cpu on a list of tensor already in CPU</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">to_cpu</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
    <span class="c1"># The tensors in c has exact id as that of b. No copy performed.</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Id of tensors in c:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">c</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[tensor([[-0.5932]], device=&#39;cuda:0&#39;), tensor([[-0.2867]], device=&#39;cuda:0&#39;), tensor([[-1.0616]], device=&#39;cuda:0&#39;)]
Id of tensors in a: 
139974954993416
139977016149120
139974955521008
[tensor([[-0.5932]]), tensor([[-0.2867]]), tensor([[-1.0616]])]
Id of tensors in b:
139974954963016
139974955458280
139974955521152
[tensor([[-0.5932]]), tensor([[-0.2867]]), tensor([[-1.0616]])]
Id of tensors in c:
139974954963016
139974955458280
139974955521152
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="to_data"><code>to_data</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L87" class="source_link">[source]</a></h4><blockquote><p><code>to_data</code>(<strong><code>b</code></strong>:<code>ItemsList</code>)</p>
</blockquote>
<p>Recursively map lists of items in <code>b</code> to their wrapped data.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Returns the data attribute from the object or collection of objects that inherits from <a href="/core.html#ItemBase"><code>ItemBase</code></a> class. Useful to examine the exact values of the data, could be used to work with the data outside of <code>fastai</code> classes.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Default example examined</span>

<span class="kn">from</span> <span class="nn">fastai</span> <span class="k">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">fastai.vision</span> <span class="k">import</span> <span class="o">*</span>

<span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">MNIST_SAMPLE</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">ImageDataBunch</span><span class="o">.</span><span class="n">from_folder</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="c1"># Examin the labels</span>
<span class="n">ys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Category display names: &quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">ys</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ys</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Unique classes internally represented as: &quot;</span><span class="p">,</span> <span class="n">to_data</span><span class="p">([</span><span class="n">ys</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ys</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Category display names:  [Category 3, Category 7]
Unique classes internally represented as:  [0, 1]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="to_detach"><code>to_detach</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L80" class="source_link">[source]</a></h4><blockquote><p><code>to_detach</code>(<strong><code>b</code></strong>:<code>Tensors</code>, <strong><code>cpu</code></strong>:<code>bool</code>=<strong><em><code>True</code></em></strong>)</p>
</blockquote>
<p>Recursively detach lists of tensors in <code>b</code>; put them on the CPU if <code>cpu=True</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="to_device"><code>to_device</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L107" class="source_link">[source]</a></h4><blockquote><p><code>to_device</code>(<strong><code>b</code></strong>:<code>Tensors</code>, <strong><code>device</code></strong>:<a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch-device"><code>device</code></a>)</p>
</blockquote>
<p>Recursively put <code>b</code> on <code>device</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="to_half"><code>to_half</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L97" class="source_link">[source]</a></h4><blockquote><p><code>to_half</code>(<strong><code>b</code></strong>:<code>Collection</code>[<code>Tensor</code>]) → <code>Collection</code>[<code>Tensor</code>]</p>
</blockquote>
<p>Recursively map lists of tensors in <code>b</code> to FP16.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Converts the tensor or list of <code>FP16</code>, resulting in less memory consumption and faster computations with the tensor. It does not convert <code>torch.int</code> types to half precision.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="n">a2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">a3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
<span class="n">a4</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">a5</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">a6</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dtype of as: &quot;</span><span class="p">,</span> <span class="n">a1</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">a2</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">a3</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">a4</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">a5</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">a6</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">b3</span><span class="p">,</span> <span class="n">b4</span><span class="p">,</span> <span class="n">b5</span><span class="p">,</span> <span class="n">b6</span> <span class="o">=</span> <span class="n">to_half</span><span class="p">([</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">,</span> <span class="n">a3</span><span class="p">,</span> <span class="n">a4</span><span class="p">,</span> <span class="n">a5</span><span class="p">,</span> <span class="n">a6</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dtype of bs: &quot;</span><span class="p">,</span> <span class="n">b1</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">b2</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">b3</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">b4</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">b5</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">b6</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>dtype of as: 	torch.int64	torch.int32	torch.int16	torch.float64	torch.float32	torch.float16
dtype of bs: 	torch.int64	torch.int32	torch.int16	torch.float16	torch.float16	torch.float16
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="to_np"><code>to_np</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L278" class="source_link">[source]</a></h4><blockquote><p><code>to_np</code>(<strong><code>x</code></strong>)</p>
</blockquote>
<p>Convert a tensor to a numpy array.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Internally puts the data to CPU, and converts to <code>numpy.ndarray</code> equivalent of <code>torch.tensor</code> by calling <code>torch.Tensor.numpy()</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">to_np</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([1., 2.], dtype=torch.float64) &lt;class &#39;torch.Tensor&#39;&gt; cpu
[1. 2.] &lt;class &#39;numpy.ndarray&#39;&gt;
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="try_int"><code>try_int</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L327" class="source_link">[source]</a></h4><blockquote><p><code>try_int</code>(<strong><code>o</code></strong>:<code>Any</code>) → <code>Any</code></p>
</blockquote>
<p>Try to convert <code>o</code> to int, default to <code>o</code> if not possible.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Converts floating point numbers to integer</span>
<span class="nb">print</span><span class="p">(</span><span class="n">try_int</span><span class="p">(</span><span class="mf">12.5</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">try_int</span><span class="p">(</span><span class="mf">12.5</span><span class="p">)))</span>

<span class="c1"># This is a Rank-1 ndarray, which ideally should not be converted to int </span>
<span class="nb">print</span><span class="p">(</span><span class="n">try_int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.5</span><span class="p">])),</span> <span class="n">try_int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.5</span><span class="p">]))</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Numpy array with a single elements are converted to int</span>
<span class="nb">print</span><span class="p">(</span><span class="n">try_int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)),</span> <span class="nb">type</span><span class="p">(</span><span class="n">try_int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">1.5</span><span class="p">))))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">try_int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.5</span><span class="p">)),</span> <span class="nb">type</span><span class="p">(</span><span class="n">try_int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.5</span><span class="p">))))</span>

<span class="c1"># Strings are not converted to int (of course)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">try_int</span><span class="p">(</span><span class="s2">&quot;12.5&quot;</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">try_int</span><span class="p">(</span><span class="s2">&quot;12.5&quot;</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>12 &lt;class &#39;int&#39;&gt;
[1.5] float64
1 &lt;class &#39;int&#39;&gt;
2 &lt;class &#39;int&#39;&gt;
12.5 &lt;class &#39;str&#39;&gt;
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Functions-to-deal-with-model-initialization">Functions to deal with model initialization<a class="anchor-link" href="#Functions-to-deal-with-model-initialization">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="apply_init"><code>apply_init</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L234" class="source_link">[source]</a></h4><blockquote><p><code>apply_init</code>(<strong><code>m</code></strong>, <strong><code>init_func</code></strong>:<code>LayerFunc</code>)</p>
</blockquote>
<p>Initialize all non-batchnorm layers of <code>m</code> with <code>init_func</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="apply_leaf"><code>apply_leaf</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L228" class="source_link">[source]</a></h4><blockquote><p><code>apply_leaf</code>(<strong><code>m</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>, <strong><code>f</code></strong>:<code>LayerFunc</code>)</p>
</blockquote>
<p>Apply <code>f</code> to children of <code>m</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="cond_init"><code>cond_init</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L224" class="source_link">[source]</a></h4><blockquote><p><code>cond_init</code>(<strong><code>m</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>, <strong><code>init_func</code></strong>:<code>LayerFunc</code>)</p>
</blockquote>
<p>Initialize the non-batchnorm layers of <code>m</code> with <code>init_func</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="in_channels"><code>in_channels</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L238" class="source_link">[source]</a></h4><blockquote><p><code>in_channels</code>(<strong><code>m</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>) → <code>List</code>[<code>int</code>]</p>
</blockquote>
<p>Return the shape of the first weight layer in <code>m</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="init_default"><code>init_default</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L217" class="source_link">[source]</a></h4><blockquote><p><code>init_default</code>(<strong><code>m</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>, <strong><code>func</code></strong>:<code>LayerFunc</code>=<strong><em><code>'kaiming_normal_'</code></em></strong>)</p>
</blockquote>
<p>Initialize <code>m</code> weights with <code>func</code> and set <code>bias</code> to 0.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Functions-to-get-information-of-a-model">Functions to get information of a model<a class="anchor-link" href="#Functions-to-get-information-of-a-model">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="children"><code>children</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L129" class="source_link">[source]</a></h4><blockquote><p><code>children</code>(<strong><code>m</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>) → <code>ModuleList</code></p>
</blockquote>
<p>Get children of <code>m</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="children_and_parameters"><code>children_and_parameters</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L149" class="source_link">[source]</a></h4><blockquote><p><code>children_and_parameters</code>(<strong><code>m</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>)</p>
</blockquote>
<p>Return the children of <code>m</code> and its direct parameters not registered in modules.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="first_layer"><code>first_layer</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L159" class="source_link">[source]</a></h4><blockquote><p><code>first_layer</code>(<strong><code>m</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>) → <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a></p>
</blockquote>
<p>Retrieve first layer in a module <code>m</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="last_layer"><code>last_layer</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L163" class="source_link">[source]</a></h4><blockquote><p><code>last_layer</code>(<strong><code>m</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>) → <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a></p>
</blockquote>
<p>Retrieve last layer in a module <code>m</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="num_children"><code>num_children</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L133" class="source_link">[source]</a></h4><blockquote><p><code>num_children</code>(<strong><code>m</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>) → <code>int</code></p>
</blockquote>
<p>Get number of children modules in <code>m</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="one_param"><code>one_param</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L323" class="source_link">[source]</a></h4><blockquote><p><code>one_param</code>(<strong><code>m</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>) → <code>Tensor</code></p>
</blockquote>
<p>Return the first parameter of <code>m</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="range_children"><code>range_children</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L137" class="source_link">[source]</a></h4><blockquote><p><code>range_children</code>(<strong><code>m</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>) → <code>Iterator</code>[<code>int</code>]</p>
</blockquote>
<p>Return iterator of len of children of <code>m</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="trainable_params"><code>trainable_params</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L124" class="source_link">[source]</a></h4><blockquote><p><code>trainable_params</code>(<strong><code>m</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>) → <code>ParamList</code></p>
</blockquote>
<p>Return list of trainable params in <code>m</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Functions-to-deal-with-BatchNorm-layers">Functions to deal with BatchNorm layers<a class="anchor-link" href="#Functions-to-deal-with-BatchNorm-layers">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="bn2float"><code>bn2float</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L207" class="source_link">[source]</a></h4><blockquote><p><code>bn2float</code>(<strong><code>module</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>) → <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a></p>
</blockquote>
<p>If <code>module</code> is batchnorm don't use half precision.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="set_bn_eval"><code>set_bn_eval</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L196" class="source_link">[source]</a></h4><blockquote><p><code>set_bn_eval</code>(<strong><code>m</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>)</p>
</blockquote>
<p>Set bn layers in eval mode for all recursive children of <code>m</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="split_bn_bias"><code>split_bn_bias</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L185" class="source_link">[source]</a></h4><blockquote><p><code>split_bn_bias</code>(<strong><code>layer_groups</code></strong>:<code>ModuleList</code>) → <code>ModuleList</code></p>
</blockquote>
<p>Split the layers in <code>layer_groups</code> into batchnorm (<code>bn_types</code>) and non-batchnorm groups.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Functions-to-get-random-tensors">Functions to get random tensors<a class="anchor-link" href="#Functions-to-get-random-tensors">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="log_uniform"><code>log_uniform</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L310" class="source_link">[source]</a></h4><blockquote><p><code>log_uniform</code>(<strong><code>low</code></strong>, <strong><code>high</code></strong>, <strong><code>size</code></strong>:<code>Optional</code>[<code>List</code>[<code>int</code>]]=<strong><em><code>None</code></em></strong>) → <code>FloatOrTensor</code></p>
</blockquote>
<p>Draw 1 or shape=<code>size</code> random floats from uniform dist: min=log(<code>low</code>), max=log(<code>high</code>).</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">log_uniform</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">2</span><span class="p">,(</span><span class="mi">8</span><span class="p">,))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([0.5775, 0.7902, 0.6087, 0.5730, 0.8057, 0.8845, 0.8975, 0.5585])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="rand_bool"><code>rand_bool</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L315" class="source_link">[source]</a></h4><blockquote><p><code>rand_bool</code>(<strong><code>p</code></strong>:<code>float</code>, <strong><code>size</code></strong>:<code>Optional</code>[<code>List</code>[<code>int</code>]]=<strong><em><code>None</code></em></strong>) → <code>BoolOrTensor</code></p>
</blockquote>
<p>Draw 1 or shape=<code>size</code> random booleans (<code>True</code> occuring with probability <code>p</code>).</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rand_bool</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([1, 1, 0, 1, 0, 0, 1, 0], dtype=torch.uint8)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="uniform"><code>uniform</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L305" class="source_link">[source]</a></h4><blockquote><p><code>uniform</code>(<strong><code>low</code></strong>:<code>Number</code>, <strong><code>high</code></strong>:<code>Number</code>=<strong><em><code>None</code></em></strong>, <strong><code>size</code></strong>:<code>Optional</code>[<code>List</code>[<code>int</code>]]=<strong><em><code>None</code></em></strong>) → <code>FloatOrTensor</code></p>
</blockquote>
<p>Draw 1 or shape=<code>size</code> random floats from uniform dist: min=<code>low</code>, max=<code>high</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">8</span><span class="p">,))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([0.6432, 0.3110, 0.7588, 0.7058, 0.7121, 0.8552, 0.3352, 0.2620])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="uniform_int"><code>uniform_int</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L319" class="source_link">[source]</a></h4><blockquote><p><code>uniform_int</code>(<strong><code>low</code></strong>:<code>int</code>, <strong><code>high</code></strong>:<code>int</code>, <strong><code>size</code></strong>:<code>Optional</code>[<code>List</code>[<code>int</code>]]=<strong><em><code>None</code></em></strong>) → <code>IntOrTensor</code></p>
</blockquote>
<p>Generate int or tensor <code>size</code> of ints between <code>low</code> and <code>high</code> (included).</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">uniform_int</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,(</span><span class="mi">8</span><span class="p">,))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([0, 1, 1, 2, 1, 1, 1, 2])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Other-functions">Other functions<a class="anchor-link" href="#Other-functions">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="ParameterModule"><code>class</code> <code>ParameterModule</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L141" class="source_link">[source]</a></h3><blockquote><p><code>ParameterModule</code>(<strong><code>p</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter"><code>Parameter</code></a>) :: <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a></p>
</blockquote>
<p>Register a lone parameter <code>p</code> in a module.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="calc_loss"><code>calc_loss</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L244" class="source_link">[source]</a></h4><blockquote><p><code>calc_loss</code>(<strong><code>y_pred</code></strong>:<code>Tensor</code>, <strong><code>y_true</code></strong>:<code>Tensor</code>, <strong><code>loss_func</code></strong>:<code>LossFunction</code>)</p>
</blockquote>
<p>Calculate loss between <code>y_pred</code> and <code>y_true</code> using <code>loss_func</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="data_collate"><code>data_collate</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L113" class="source_link">[source]</a></h4><blockquote><p><code>data_collate</code>(<strong><code>batch</code></strong>:<code>ItemsList</code>) → <code>Tensor</code></p>
</blockquote>
<p>Convert <code>batch</code> items to tensor data.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_model"><code>get_model</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L335" class="source_link">[source]</a></h4><blockquote><p><code>get_model</code>(<strong><code>model</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>)</p>
</blockquote>
<p>Return the model maybe wrapped inside <code>model</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="grab_idx"><code>grab_idx</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L290" class="source_link">[source]</a></h4><blockquote><p><code>grab_idx</code>(<strong><code>x</code></strong>, <strong><code>i</code></strong>, <strong><code>batch_first</code></strong>:<code>bool</code>=<strong><em><code>True</code></em></strong>)</p>
</blockquote>
<p>Grab the <code>i</code>-th batch in <code>x</code>, <code>batch_first</code> stating the batch dimension.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="logit"><code>logit</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L295" class="source_link">[source]</a></h4><blockquote><p><code>logit</code>(<strong><code>x</code></strong>:<code>Tensor</code>) → <code>Tensor</code></p>
</blockquote>
<p>Logit of <code>x</code>, clamped to avoid inf.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="logit_"><code>logit_</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L300" class="source_link">[source]</a></h4><blockquote><p><code>logit_</code>(<strong><code>x</code></strong>:<code>Tensor</code>) → <code>Tensor</code></p>
</blockquote>
<p>Inplace logit of <code>x</code>, clamped to avoid inf</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="model_type"><code>model_type</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L254" class="source_link">[source]</a></h4><blockquote><p><code>model_type</code>(<strong><code>dtype</code></strong>)</p>
</blockquote>
<p>Return the torch type corresponding to <code>dtype</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="np_address"><code>np_address</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L76" class="source_link">[source]</a></h4><blockquote><p><code>np_address</code>(<strong><code>x</code></strong>:<code>ndarray</code>) → <code>int</code></p>
</blockquote>
<p>Address of <code>x</code> in memory.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="split_model"><code>split_model</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L174" class="source_link">[source]</a></h4><blockquote><p><code>split_model</code>(<strong><code>model</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>, <strong><code>splits</code></strong>:<code>Collection</code>[<code>Union</code>[<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>, <code>ModuleList</code>]], <strong><code>want_idxs</code></strong>:<code>bool</code>=<strong><em><code>False</code></em></strong>)</p>
</blockquote>
<p>Split <code>model</code> according to the layers in <code>splits</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If <code>splits</code> are layers, the model is split at those (not included) sequentially. If <code>want_idxs</code> is True, the corresponding indexes are returned. If <code>splits</code> are lists of layers, the model is split according to those.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="split_model_idx"><code>split_model_idx</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L167" class="source_link">[source]</a></h4><blockquote><p><code>split_model_idx</code>(<strong><code>model</code></strong>:<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>, <strong><code>idxs</code></strong>:<code>Collection</code>[<code>int</code>]) → <code>ModuleList</code></p>
</blockquote>
<p>Split <code>model</code> according to the indexes in <code>idxs</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="trange_of"><code>trange_of</code><a href="https://github.com/fastai/fastai/blob/master/fastai/torch_core.py#L274" class="source_link">[source]</a></h4><blockquote><p><code>trange_of</code>(<strong><code>x</code></strong>)</p>
</blockquote>
<p>Create a tensor from <code>range_of(x)</code>.</p>

</div>

</div>

</div>
</div>

</div>
</div>
 

