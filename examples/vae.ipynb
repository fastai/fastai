{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variation Autoencoders in fastai\n",
    "### Adaption of https://github.com/pytorch/examples/tree/master/vae to work with fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from fastai.vision import * \n",
    "from fastai import *\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab the MNIST sample dataset\n",
    "\n",
    "**Note:** the fastai library currently only supports image classification, so our y values will be the classes of the \n",
    "MNIST images. However, we'll just ignore the y values set by fastai here and instead include the original image as part of the output of our model. This has the added advantage that we can make use of fastai's data augmentation techniques as well!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "data = ImageDataBunch.from_folder(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define out model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, zdims):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.zdims = 20\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, zdims)\n",
    "        self.fc22 = nn.Linear(400, zdims)\n",
    "        self.fc3 = nn.Linear(zdims, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        outputs the reconstructed image, the image we trained on, mu, and log(variance)\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), x, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "See https://github.com/pytorch/examples/blob/master/vae/main.py  for a deeper explanation of the loss function.\n",
    "\n",
    "fastai will take all of our outputs from our model's `forward` method and pass it as a tuple into the first argument of our loss function.  As noted above, we'll ignore the y values, as we don't care about the labels for our images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(out, _) -> Variable:\n",
    "    recon_x, x, mu, logvar = out\n",
    "    batch_size = x.shape[0]\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784))\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    KLD /= batch_size * 784\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, vae, loss_func=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 00:27\n",
      "epoch  train_loss  valid_loss\n",
      "1      0.196566    0.186746    (00:02)\n",
      "2      0.179038    0.173951    (00:02)\n",
      "3      0.171522    0.169113    (00:02)\n",
      "4      0.169041    0.166268    (00:02)\n",
      "5      0.166219    0.165584    (00:02)\n",
      "6      0.165616    0.164444    (00:02)\n",
      "7      0.164781    0.163765    (00:02)\n",
      "8      0.163487    0.162520    (00:02)\n",
      "9      0.162315    0.161844    (00:02)\n",
      "10     0.162938    0.161012    (00:02)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.fit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itera = iter(data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batch, _ = next(itera)\n",
    "valid_batch = valid_batch[0].unsqueeze(0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_img = Image(valid_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    d = Variable(valid_batch).cuda()\n",
    "    recon_batch, mu, logvar, original = vae(d)\n",
    "    recon_batch = recon_batch.view(-1,3,28,28).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_img = Image(recon_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6d10346898>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(orig_img)\n",
    "show_image(recon_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
